{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ea5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import parallel\n",
    "import torch\n",
    "import movable_wall_parallel\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "import wandb\n",
    "RANDOM = 42\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46a3cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using WALLS\n",
      "DONT FORGET TO ADD CODE TO SAVE CHECKPOINTS IF YOU WANT TO DO THAT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############################ HIGHLY IMPORTANT VARIABLES TO SET######################################\n",
    "GRID_SIZE = 7\n",
    "NUM_THINGS = 5 # number of things in the grid wall, pred1, pred2, h1, h2, movablewall\n",
    "\n",
    "\n",
    "INITIALIZATIONS = [\n",
    "    #'./models/PRED_GROUP_POLICY_BOTH_INDIV_RWD_2700.ckpt', # pred_policy\n",
    "    #'./models/PRED_GROUP_POLICY_BOTH_INDIV_RWD_2700.ckpt',    # hider_policy\n",
    "    #'./models/HIDER_GROUP_POLICY_BOTH_INDIV_RWD_1800.ckpt',\n",
    "    #'./models/HIDER_GROUP_POLICY_BOTH_INDIV_RWD_1800.ckpt',\n",
    "    RANDOM,\n",
    "    RANDOM,\n",
    "    RANDOM,\n",
    "    RANDOM,\n",
    "    ]    # hider_2\n",
    "#should be either RANDOM ; or a path to a pretrained checkpoint (a String)\n",
    "\n",
    "IS_POLICY_TRAINING = {\"preds\":True, \"hiders\":True} #preds, hiders\n",
    "\n",
    "IS_TRAINING =   [\n",
    "    IS_POLICY_TRAINING[\"preds\"], #pred1\n",
    "    IS_POLICY_TRAINING[\"preds\"], #pred2\n",
    "    IS_POLICY_TRAINING[\"hiders\"], #hider1\n",
    "    IS_POLICY_TRAINING[\"hiders\"] #hider2\n",
    "]\n",
    "#either True or False, if False, weights are frozen (or if random it will stay random)\n",
    "\n",
    "\n",
    "envname = 'mparallel-walls' #just for wandb logging\n",
    "CUSTOMENV = movable_wall_parallel.parallel_env(grid_size=GRID_SIZE,walls=True)\n",
    "# change architecture if needed\n",
    "\n",
    "ent_coef = 0.1\n",
    "vf_coef = 0.2\n",
    "clip_coef = 0.1\n",
    "gamma = 0.975\n",
    "batch_size = 64\n",
    "max_cycles = 200\n",
    "total_episodes = 10000\n",
    "PPO_STEPS = 3\n",
    "\n",
    "reminder = '''DONT FORGET TO ADD CODE TO SAVE CHECKPOINTS IF YOU WANT TO DO THAT'''\n",
    "ckpt_name= \"GROUP_POLICY_BOTH_INDIVRWD2\" #\"GROUP_POLICY_BOTH_INDIV_RWD_ROUND2bis\"\n",
    "SAVE_PRED_POL = True\n",
    "SAVE_HIDER_POL = True\n",
    "print(reminder)\n",
    "\n",
    "##################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5747b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Super_Agent(nn.Module):\n",
    "    #Common agent class for all hiders/seekers\n",
    "    \n",
    "    def __init__(self, num_actions, num_agents):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN architecture inspired by DQN for Atari\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(NUM_THINGS, 32, kernel_size=3, stride=1, padding=1),  # Output: 32 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Output: 64 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Output: 64 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),  # Output: 64 * 7 * 7 = 3136\n",
    "        )\n",
    "        self.actor = self._layer_init(nn.Linear(64*GRID_SIZE**2, num_actions), std=0.01) #TODO depends on GRID_SIZE\n",
    "        self.critic = self._layer_init(nn.Linear(64*GRID_SIZE**2, 1))\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.network(x / 1.0))  # Normalize input to [0, 1]\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x / 1.0)  # Normalize input to [0, 1]\n",
    "        \n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
    "\n",
    "    \n",
    "\n",
    "def batchify_obs(obs, device):\n",
    "    \"\"\"Converts PZ style observations to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    obs = np.stack([obs[a] for a in obs], axis=0)\n",
    "    # convert to torch\n",
    "    obs = torch.tensor(obs).to(device)\n",
    "\n",
    "    return obs\n",
    "\n",
    "\n",
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unbatchify(x, env):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {a: x[i] for i, a in enumerate(env.possible_agents)}\n",
    "\n",
    "    return x\n",
    "\n",
    "def reshape_obs(observations, env):\n",
    "    modified_observations = {}\n",
    "    for self_name, obs in observations.items():\n",
    "        self_layer = env.agent_layers[self_name]\n",
    "        enemy_layers = []\n",
    "        for name, layer_idx in env.agent_layers.items():\n",
    "            \n",
    "            if name == self_name:\n",
    "                self_layer = obs[layer_idx]\n",
    "            elif name.startswith(self_name[:4]): #starts with the same 4 letters: pred or hide\n",
    "                friend_layer = obs[layer_idx]\n",
    "            else:\n",
    "                enemy_layers.append(obs[layer_idx])\n",
    "         \n",
    "        new_obs = [obs[0]] #walls\n",
    "        new_obs.append(self_layer) #self\n",
    "        new_obs.append(friend_layer) #friend\n",
    "        new_obs.append(sum(enemy_layers)) #enemies\n",
    "        new_obs.append(obs[-1]) #movable walls\n",
    "\n",
    "        modified_observations[self_name] = np.stack(new_obs, axis = 0)\n",
    "        \n",
    "    return modified_observations\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ded135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33milias-baali\u001b[0m (\u001b[33milias-baali-georgia-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hice1/ibaali3/2d_RL_hide_seek/PARALLEL/wandb/run-20241208_121924-qs0ssinm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo/runs/qs0ssinm' target=\"_blank\">GROUP_POLICY_BOTH_INDIVRWD2</a></strong> to <a href='https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo' target=\"_blank\">https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo/runs/qs0ssinm' target=\"_blank\">https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo/runs/qs0ssinm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 10\n",
      "Episodic Return: [-878.182     554.4734     25.126799  130.9869  ]\n",
      "Smoothed Returns for agent_0 (Training): 308.3064880371094\n",
      "Smoothed Returns for agent_1 (Training): 541.6873779296875\n",
      "Smoothed Returns for agent_2 (Training): -488.5980529785156\n",
      "Smoothed Returns for agent_3 (Training): -506.37091064453125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 8835.30078125\n",
      "Policy Loss: -20.268306732177734\n",
      "Old Approx KL: -0.003532171482220292\n",
      "Approx KL: 0.004386565648019314\n",
      "Clip Fraction: 0.10703656503132411\n",
      "Explained Variance: 4.8041343688964844e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 20\n",
      "Episodic Return: [ 1459.268       12.085456  -253.60524  -1403.8383  ]\n",
      "Smoothed Returns for agent_0 (Training): 402.94927978515625\n",
      "Smoothed Returns for agent_1 (Training): 682.6124877929688\n",
      "Smoothed Returns for agent_2 (Training): -617.9024658203125\n",
      "Smoothed Returns for agent_3 (Training): -593.1023559570312\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 8182.20068359375\n",
      "Policy Loss: 75.01053619384766\n",
      "Old Approx KL: 0.008774016983807087\n",
      "Approx KL: 0.005975544918328524\n",
      "Clip Fraction: 0.1596513624702181\n",
      "Explained Variance: 0.00018411874771118164\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 30\n",
      "Episodic Return: [-739.57074  -38.92692  283.14685  427.20737]\n",
      "Smoothed Returns for agent_0 (Training): 179.5203857421875\n",
      "Smoothed Returns for agent_1 (Training): 590.1956787109375\n",
      "Smoothed Returns for agent_2 (Training): -460.8583068847656\n",
      "Smoothed Returns for agent_3 (Training): -384.4823913574219\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2313.133544921875\n",
      "Policy Loss: -46.19320297241211\n",
      "Old Approx KL: -0.005126280710101128\n",
      "Approx KL: 0.0014184501487761736\n",
      "Clip Fraction: 0.00977891186873118\n",
      "Explained Variance: 0.00037413835525512695\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 40\n",
      "Episodic Return: [  666.58997   923.9498  -1216.6503   -265.57846]\n",
      "Smoothed Returns for agent_0 (Training): 168.53048706054688\n",
      "Smoothed Returns for agent_1 (Training): 690.7551879882812\n",
      "Smoothed Returns for agent_2 (Training): -498.952392578125\n",
      "Smoothed Returns for agent_3 (Training): -417.5220642089844\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 5511.9267578125\n",
      "Policy Loss: 2.982267141342163\n",
      "Old Approx KL: 0.03152051195502281\n",
      "Approx KL: 0.0032114090863615274\n",
      "Clip Fraction: 0.2452168379511152\n",
      "Explained Variance: 3.2186508178710938e-06\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 50\n",
      "Episodic Return: [ 1619.5216  1523.8219 -2089.2368 -1217.1046]\n",
      "Smoothed Returns for agent_0 (Training): 630.1835327148438\n",
      "Smoothed Returns for agent_1 (Training): 901.005859375\n",
      "Smoothed Returns for agent_2 (Training): -907.5802001953125\n",
      "Smoothed Returns for agent_3 (Training): -683.78759765625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 30695.40234375\n",
      "Policy Loss: 187.26475524902344\n",
      "Old Approx KL: 0.03853432089090347\n",
      "Approx KL: 0.003953691106289625\n",
      "Clip Fraction: 0.05697279033206758\n",
      "Explained Variance: -5.543231964111328e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 60\n",
      "Episodic Return: [ -985.0725 -1071.3713   933.9016  1024.1696]\n",
      "Smoothed Returns for agent_0 (Training): 363.9317321777344\n",
      "Smoothed Returns for agent_1 (Training): 372.2384338378906\n",
      "Smoothed Returns for agent_2 (Training): -516.4178466796875\n",
      "Smoothed Returns for agent_3 (Training): -265.24151611328125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 5585.29931640625\n",
      "Policy Loss: -107.4494400024414\n",
      "Old Approx KL: -0.02960771508514881\n",
      "Approx KL: 0.003285795683041215\n",
      "Clip Fraction: 0.1366921773269063\n",
      "Explained Variance: 0.00017905235290527344\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 70\n",
      "Episodic Return: [ 1606.9789  2015.3043 -4023.6484   953.0174]\n",
      "Smoothed Returns for agent_0 (Training): -158.8450469970703\n",
      "Smoothed Returns for agent_1 (Training): -125.35942077636719\n",
      "Smoothed Returns for agent_2 (Training): -150.7737274169922\n",
      "Smoothed Returns for agent_3 (Training): 405.9544982910156\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 46933.7421875\n",
      "Policy Loss: 180.21005249023438\n",
      "Old Approx KL: -0.028084680438041687\n",
      "Approx KL: 0.015236161649227142\n",
      "Clip Fraction: 0.055272109451748076\n",
      "Explained Variance: 0.00012552738189697266\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 80\n",
      "Episodic Return: [  864.7614  1074.9456  -643.108  -1503.7555]\n",
      "Smoothed Returns for agent_0 (Training): 187.6842498779297\n",
      "Smoothed Returns for agent_1 (Training): 198.56236267089844\n",
      "Smoothed Returns for agent_2 (Training): -688.5664672851562\n",
      "Smoothed Returns for agent_3 (Training): 39.563934326171875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 15177.4775390625\n",
      "Policy Loss: 77.65093994140625\n",
      "Old Approx KL: -0.006023765075951815\n",
      "Approx KL: 0.007457197178155184\n",
      "Clip Fraction: 0.3031462615444547\n",
      "Explained Variance: 3.594160079956055e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 90\n",
      "Episodic Return: [ -369.67618 -1050.3759    536.9829    526.7221 ]\n",
      "Smoothed Returns for agent_0 (Training): 18.549339294433594\n",
      "Smoothed Returns for agent_1 (Training): 23.465789794921875\n",
      "Smoothed Returns for agent_2 (Training): -310.08538818359375\n",
      "Smoothed Returns for agent_3 (Training): -38.691349029541016\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 493.7073059082031\n",
      "Policy Loss: -20.935504913330078\n",
      "Old Approx KL: -5.9975049225613475e-05\n",
      "Approx KL: 0.0010648880852386355\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.0004956722259521484\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 100\n",
      "Episodic Return: [-841.4603  -773.3207   772.7349   919.74225]\n",
      "Smoothed Returns for agent_0 (Training): 202.0260772705078\n",
      "Smoothed Returns for agent_1 (Training): 132.0439453125\n",
      "Smoothed Returns for agent_2 (Training): -379.46099853515625\n",
      "Smoothed Returns for agent_3 (Training): -351.2749938964844\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 5095.552734375\n",
      "Policy Loss: -100.45320129394531\n",
      "Old Approx KL: -3.701448440551758e-05\n",
      "Approx KL: 0.0007798970327712595\n",
      "Clip Fraction: 0.05357142857142857\n",
      "Explained Variance: 0.00018131732940673828\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 110\n",
      "Episodic Return: [-1031.893  -1134.5397  1058.5797  1193.1627]\n",
      "Smoothed Returns for agent_0 (Training): 592.241455078125\n",
      "Smoothed Returns for agent_1 (Training): 641.8594360351562\n",
      "Smoothed Returns for agent_2 (Training): -714.8367919921875\n",
      "Smoothed Returns for agent_3 (Training): -841.8114013671875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 7614.3544921875\n",
      "Policy Loss: -117.47689056396484\n",
      "Old Approx KL: -0.003871011082082987\n",
      "Approx KL: 0.0024637794122099876\n",
      "Clip Fraction: 0.21099064747492471\n",
      "Explained Variance: 0.00015777349472045898\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 120\n",
      "Episodic Return: [ 1021.5998  1517.5375 -2994.0776   749.9583]\n",
      "Smoothed Returns for agent_0 (Training): 67.40937805175781\n",
      "Smoothed Returns for agent_1 (Training): 104.6808090209961\n",
      "Smoothed Returns for agent_2 (Training): -261.54632568359375\n",
      "Smoothed Returns for agent_3 (Training): 3.474383592605591\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 11714.26171875\n",
      "Policy Loss: 30.325517654418945\n",
      "Old Approx KL: -0.009245812892913818\n",
      "Approx KL: 5.681174297933467e-05\n",
      "Clip Fraction: 0.13594813006264822\n",
      "Explained Variance: 1.1265277862548828e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 130\n",
      "Episodic Return: [-840.634  -802.1641  836.9249  938.6391]\n",
      "Smoothed Returns for agent_0 (Training): 1069.5372314453125\n",
      "Smoothed Returns for agent_1 (Training): 960.55029296875\n",
      "Smoothed Returns for agent_2 (Training): -1164.92431640625\n",
      "Smoothed Returns for agent_3 (Training): -1209.794189453125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 8468.0927734375\n",
      "Policy Loss: -132.19924926757812\n",
      "Old Approx KL: -0.028293117880821228\n",
      "Approx KL: 0.0034061800688505173\n",
      "Clip Fraction: 0.1409438791729155\n",
      "Explained Variance: 2.8908252716064453e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 140\n",
      "Episodic Return: [-681.62726 -682.09344  543.1695   835.5214 ]\n",
      "Smoothed Returns for agent_0 (Training): 920.0303955078125\n",
      "Smoothed Returns for agent_1 (Training): 1180.177978515625\n",
      "Smoothed Returns for agent_2 (Training): -928.4781494140625\n",
      "Smoothed Returns for agent_3 (Training): -1482.294677734375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 4678.70849609375\n",
      "Policy Loss: -87.78301239013672\n",
      "Old Approx KL: 0.0049187797121703625\n",
      "Approx KL: 0.0026224853936582804\n",
      "Clip Fraction: 0.013180272919791085\n",
      "Explained Variance: 7.677078247070312e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 150\n",
      "Episodic Return: [-733.16125 -691.0584   454.72742  853.8125 ]\n",
      "Smoothed Returns for agent_0 (Training): -597.7681274414062\n",
      "Smoothed Returns for agent_1 (Training): -298.1903381347656\n",
      "Smoothed Returns for agent_2 (Training): 479.87066650390625\n",
      "Smoothed Returns for agent_3 (Training): 347.1196594238281\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 187.94845581054688\n",
      "Policy Loss: -5.734297752380371\n",
      "Old Approx KL: 0.01329728588461876\n",
      "Approx KL: 0.004951204638928175\n",
      "Clip Fraction: 0.08886054477521352\n",
      "Explained Variance: -0.00042510032653808594\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 160\n",
      "Episodic Return: [ 1230.9425   1300.207     721.43665 -2934.6401 ]\n",
      "Smoothed Returns for agent_0 (Training): -96.98444366455078\n",
      "Smoothed Returns for agent_1 (Training): 344.1874694824219\n",
      "Smoothed Returns for agent_2 (Training): 174.1871337890625\n",
      "Smoothed Returns for agent_3 (Training): -557.3978271484375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 9972.5546875\n",
      "Policy Loss: 56.598976135253906\n",
      "Old Approx KL: 0.00912865623831749\n",
      "Approx KL: 0.005083450581878424\n",
      "Clip Fraction: 0.12893282515662058\n",
      "Explained Variance: 2.682209014892578e-06\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 170\n",
      "Episodic Return: [ 4433.3022  -898.1315 -3289.0662 -2172.562 ]\n",
      "Smoothed Returns for agent_0 (Training): 705.3759765625\n",
      "Smoothed Returns for agent_1 (Training): 1405.2674560546875\n",
      "Smoothed Returns for agent_2 (Training): -1051.879150390625\n",
      "Smoothed Returns for agent_3 (Training): -1496.13818359375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 30396.41015625\n",
      "Policy Loss: 191.02264404296875\n",
      "Old Approx KL: -0.036923982203006744\n",
      "Approx KL: 0.005874463822692633\n",
      "Clip Fraction: 0.07249149822053455\n",
      "Explained Variance: -3.0875205993652344e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 180\n",
      "Episodic Return: [  171.91138  -1142.5117     611.25995    124.459946]\n",
      "Smoothed Returns for agent_0 (Training): 498.180419921875\n",
      "Smoothed Returns for agent_1 (Training): 671.0303955078125\n",
      "Smoothed Returns for agent_2 (Training): -830.6349487304688\n",
      "Smoothed Returns for agent_3 (Training): -703.99072265625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2852.070068359375\n",
      "Policy Loss: -67.58381652832031\n",
      "Old Approx KL: -0.0420713871717453\n",
      "Approx KL: 0.005778657738119364\n",
      "Clip Fraction: 0.18090986637842088\n",
      "Explained Variance: 0.00013911724090576172\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 190\n",
      "Episodic Return: [-1073.507    -516.0722    732.811     589.41156]\n",
      "Smoothed Returns for agent_0 (Training): -310.4974365234375\n",
      "Smoothed Returns for agent_1 (Training): -345.3846740722656\n",
      "Smoothed Returns for agent_2 (Training): 355.2167663574219\n",
      "Smoothed Returns for agent_3 (Training): 207.54867553710938\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2062.08056640625\n",
      "Policy Loss: -50.75827407836914\n",
      "Old Approx KL: -0.03444068878889084\n",
      "Approx KL: 0.004657750017940998\n",
      "Clip Fraction: 0.07823129494984944\n",
      "Explained Variance: 0.0011731386184692383\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 200\n",
      "Episodic Return: [ -284.4552  4390.83   -3051.5535 -2820.3157]\n",
      "Smoothed Returns for agent_0 (Training): -296.2366027832031\n",
      "Smoothed Returns for agent_1 (Training): 274.68585205078125\n",
      "Smoothed Returns for agent_2 (Training): -105.1213150024414\n",
      "Smoothed Returns for agent_3 (Training): -233.0242462158203\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 54258.390625\n",
      "Policy Loss: 285.9483947753906\n",
      "Old Approx KL: 0.0846061035990715\n",
      "Approx KL: 0.009115164168179035\n",
      "Clip Fraction: 0.1494472800266175\n",
      "Explained Variance: -4.398822784423828e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 210\n",
      "Episodic Return: [  76.32601 -222.98352 -353.16943  539.88806]\n",
      "Smoothed Returns for agent_0 (Training): 419.4410705566406\n",
      "Smoothed Returns for agent_1 (Training): 785.1096801757812\n",
      "Smoothed Returns for agent_2 (Training): -626.23193359375\n",
      "Smoothed Returns for agent_3 (Training): -854.7296142578125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2863.945556640625\n",
      "Policy Loss: -25.570613861083984\n",
      "Old Approx KL: 0.012394757010042667\n",
      "Approx KL: 0.004132317844778299\n",
      "Clip Fraction: 0.3030399665946052\n",
      "Explained Variance: 7.95125961303711e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 220\n",
      "Episodic Return: [  99.75934 -298.3358  -173.11627  155.41554]\n",
      "Smoothed Returns for agent_0 (Training): 2.205911159515381\n",
      "Smoothed Returns for agent_1 (Training): -194.23448181152344\n",
      "Smoothed Returns for agent_2 (Training): 152.6183319091797\n",
      "Smoothed Returns for agent_3 (Training): 49.89800262451172\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2302.010498046875\n",
      "Policy Loss: 47.74831771850586\n",
      "Old Approx KL: 0.009460185654461384\n",
      "Approx KL: 0.001046065823175013\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: 9.870529174804688e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 230\n",
      "Episodic Return: [ 1992.4675  1624.7639 -2360.9502 -1465.8862]\n",
      "Smoothed Returns for agent_0 (Training): 456.91900634765625\n",
      "Smoothed Returns for agent_1 (Training): 392.00469970703125\n",
      "Smoothed Returns for agent_2 (Training): -862.5054931640625\n",
      "Smoothed Returns for agent_3 (Training): 6.2271728515625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 15733.9716796875\n",
      "Policy Loss: 131.43606567382812\n",
      "Old Approx KL: 0.0170279573649168\n",
      "Approx KL: 0.007862231694161892\n",
      "Clip Fraction: 0.20471938912357604\n",
      "Explained Variance: 6.16312026977539e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 240\n",
      "Episodic Return: [-781.1002  -727.0652   591.66895  707.35364]\n",
      "Smoothed Returns for agent_0 (Training): 451.72235107421875\n",
      "Smoothed Returns for agent_1 (Training): 483.177978515625\n",
      "Smoothed Returns for agent_2 (Training): -847.5950927734375\n",
      "Smoothed Returns for agent_3 (Training): -41.716949462890625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 940.8408813476562\n",
      "Policy Loss: -32.067665100097656\n",
      "Old Approx KL: -0.018244003877043724\n",
      "Approx KL: 0.0017415882321074605\n",
      "Clip Fraction: 0.1332908176950046\n",
      "Explained Variance: 0.0003877878189086914\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 250\n",
      "Episodic Return: [ 1373.295   2481.9905 -2607.8533  -980.1735]\n",
      "Smoothed Returns for agent_0 (Training): -93.14558410644531\n",
      "Smoothed Returns for agent_1 (Training): 79.1146469116211\n",
      "Smoothed Returns for agent_2 (Training): 26.633533477783203\n",
      "Smoothed Returns for agent_3 (Training): -87.43345642089844\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 50039.73828125\n",
      "Policy Loss: 195.6426544189453\n",
      "Old Approx KL: -0.023603560402989388\n",
      "Approx KL: 0.0032552906777709723\n",
      "Clip Fraction: 0.08450255294640859\n",
      "Explained Variance: 4.214048385620117e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 260\n",
      "Episodic Return: [-977.92035 -182.3323   338.7531   359.8613 ]\n",
      "Smoothed Returns for agent_0 (Training): 793.92822265625\n",
      "Smoothed Returns for agent_1 (Training): 433.93731689453125\n",
      "Smoothed Returns for agent_2 (Training): -832.4912109375\n",
      "Smoothed Returns for agent_3 (Training): -669.1011962890625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3143.577392578125\n",
      "Policy Loss: -41.201873779296875\n",
      "Old Approx KL: -0.002201569965109229\n",
      "Approx KL: 0.005198823753744364\n",
      "Clip Fraction: 0.13828656645048232\n",
      "Explained Variance: 0.00017625093460083008\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 270\n",
      "Episodic Return: [ 1223.9264  2045.2731 -1839.8691 -1788.836 ]\n",
      "Smoothed Returns for agent_0 (Training): 717.6709594726562\n",
      "Smoothed Returns for agent_1 (Training): 434.7119140625\n",
      "Smoothed Returns for agent_2 (Training): -846.0462646484375\n",
      "Smoothed Returns for agent_3 (Training): -537.7398681640625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 14156.666015625\n",
      "Policy Loss: 152.87091064453125\n",
      "Old Approx KL: -0.014256546273827553\n",
      "Approx KL: 0.001621063333004713\n",
      "Clip Fraction: 0.03050595238095238\n",
      "Explained Variance: 2.09808349609375e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 280\n",
      "Episodic Return: [-363.22003   -379.72433    724.2452       2.2027178]\n",
      "Smoothed Returns for agent_0 (Training): 43.81206512451172\n",
      "Smoothed Returns for agent_1 (Training): 548.5299072265625\n",
      "Smoothed Returns for agent_2 (Training): -397.44061279296875\n",
      "Smoothed Returns for agent_3 (Training): -294.3931579589844\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2217.145751953125\n",
      "Policy Loss: -41.12276840209961\n",
      "Old Approx KL: -0.0036600488238036633\n",
      "Approx KL: 0.000652700720820576\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: 6.401538848876953e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 290\n",
      "Episodic Return: [ 2902.1333   -159.19861 -1838.9062  -1686.6459 ]\n",
      "Smoothed Returns for agent_0 (Training): 637.4855346679688\n",
      "Smoothed Returns for agent_1 (Training): 774.750732421875\n",
      "Smoothed Returns for agent_2 (Training): -853.0501708984375\n",
      "Smoothed Returns for agent_3 (Training): -700.4671630859375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 11862.5126953125\n",
      "Policy Loss: 113.05152130126953\n",
      "Old Approx KL: 0.035493459552526474\n",
      "Approx KL: 0.01095591764897108\n",
      "Clip Fraction: 0.3762755138533456\n",
      "Explained Variance: 9.161233901977539e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 300\n",
      "Episodic Return: [-645.8543    38.79742  378.83905  258.9985 ]\n",
      "Smoothed Returns for agent_0 (Training): 499.0188903808594\n",
      "Smoothed Returns for agent_1 (Training): 553.7645263671875\n",
      "Smoothed Returns for agent_2 (Training): -683.1571044921875\n",
      "Smoothed Returns for agent_3 (Training): -466.70751953125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3546.173828125\n",
      "Policy Loss: -43.01857376098633\n",
      "Old Approx KL: -0.0134358499199152\n",
      "Approx KL: 0.0002626138157211244\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: 3.0219554901123047e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 310\n",
      "Episodic Return: [ -41.332466  422.63156    50.64756  -472.80606 ]\n",
      "Smoothed Returns for agent_0 (Training): 385.5537414550781\n",
      "Smoothed Returns for agent_1 (Training): 319.1279602050781\n",
      "Smoothed Returns for agent_2 (Training): -460.6339416503906\n",
      "Smoothed Returns for agent_3 (Training): -543.2110595703125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 6762.6044921875\n",
      "Policy Loss: -57.214141845703125\n",
      "Old Approx KL: -0.00496233394369483\n",
      "Approx KL: 0.005150603596121073\n",
      "Clip Fraction: 0.09959608955042702\n",
      "Explained Variance: -1.9431114196777344e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 320\n",
      "Episodic Return: [-769.8828    340.27234   322.19794  -106.959175]\n",
      "Smoothed Returns for agent_0 (Training): 398.2818298339844\n",
      "Smoothed Returns for agent_1 (Training): 170.5044708251953\n",
      "Smoothed Returns for agent_2 (Training): -410.399169921875\n",
      "Smoothed Returns for agent_3 (Training): -439.7876892089844\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2645.293701171875\n",
      "Policy Loss: -55.550357818603516\n",
      "Old Approx KL: -0.0020277672447264194\n",
      "Approx KL: 0.00048678688472136855\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: 0.0003383159637451172\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 330\n",
      "Episodic Return: [ 2024.1244  2666.774  -2979.1672 -2469.0208]\n",
      "Smoothed Returns for agent_0 (Training): 146.44815063476562\n",
      "Smoothed Returns for agent_1 (Training): 212.86001586914062\n",
      "Smoothed Returns for agent_2 (Training): -403.39642333984375\n",
      "Smoothed Returns for agent_3 (Training): -217.31759643554688\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 26646.291015625\n",
      "Policy Loss: 173.28086853027344\n",
      "Old Approx KL: 0.029122039675712585\n",
      "Approx KL: 0.00477030873298645\n",
      "Clip Fraction: 0.32472364391599384\n",
      "Explained Variance: 0.0001493692398071289\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 340\n",
      "Episodic Return: [ 509.76437  644.62775 -690.2704  -428.1227 ]\n",
      "Smoothed Returns for agent_0 (Training): 484.45184326171875\n",
      "Smoothed Returns for agent_1 (Training): 702.4100341796875\n",
      "Smoothed Returns for agent_2 (Training): -757.0619506835938\n",
      "Smoothed Returns for agent_3 (Training): -745.73486328125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1647.4422607421875\n",
      "Policy Loss: -6.4796528816223145\n",
      "Old Approx KL: -0.0023173349909484386\n",
      "Approx KL: 0.0043204426765441895\n",
      "Clip Fraction: 0.2418154790287926\n",
      "Explained Variance: 0.0003504157066345215\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 350\n",
      "Episodic Return: [-801.001   -253.11877  495.7948   512.8245 ]\n",
      "Smoothed Returns for agent_0 (Training): 236.4550323486328\n",
      "Smoothed Returns for agent_1 (Training): 429.09893798828125\n",
      "Smoothed Returns for agent_2 (Training): -389.0718688964844\n",
      "Smoothed Returns for agent_3 (Training): -355.4382019042969\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 4069.59716796875\n",
      "Policy Loss: -83.44219207763672\n",
      "Old Approx KL: 0.0004587940056808293\n",
      "Approx KL: 0.0003174032608512789\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: 0.0006148815155029297\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 360\n",
      "Episodic Return: [  846.084     850.35583   545.4808  -1995.4446 ]\n",
      "Smoothed Returns for agent_0 (Training): 254.9260711669922\n",
      "Smoothed Returns for agent_1 (Training): 308.75927734375\n",
      "Smoothed Returns for agent_2 (Training): -195.74945068359375\n",
      "Smoothed Returns for agent_3 (Training): -410.66259765625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 12868.0283203125\n",
      "Policy Loss: 19.435794830322266\n",
      "Old Approx KL: 0.004341151099652052\n",
      "Approx KL: 0.0032396786846220493\n",
      "Clip Fraction: 0.002976190476190476\n",
      "Explained Variance: 0.000750124454498291\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 370\n",
      "Episodic Return: [ 654.36206 -717.4265   216.17526 -188.90831]\n",
      "Smoothed Returns for agent_0 (Training): 283.1021423339844\n",
      "Smoothed Returns for agent_1 (Training): 276.3851623535156\n",
      "Smoothed Returns for agent_2 (Training): -73.15339660644531\n",
      "Smoothed Returns for agent_3 (Training): -505.4404296875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2581.973876953125\n",
      "Policy Loss: -36.805213928222656\n",
      "Old Approx KL: 0.007117663510143757\n",
      "Approx KL: 0.004269714932888746\n",
      "Clip Fraction: 0.009672619047619048\n",
      "Explained Variance: 0.0017628669738769531\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 380\n",
      "Episodic Return: [1145.7855    95.98818 -920.83624 -680.544  ]\n",
      "Smoothed Returns for agent_0 (Training): 364.9694519042969\n",
      "Smoothed Returns for agent_1 (Training): 266.2783508300781\n",
      "Smoothed Returns for agent_2 (Training): -217.7433319091797\n",
      "Smoothed Returns for agent_3 (Training): -487.65582275390625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 5999.26123046875\n",
      "Policy Loss: 45.642642974853516\n",
      "Old Approx KL: 0.01795578934252262\n",
      "Approx KL: 0.004273993894457817\n",
      "Clip Fraction: 0.29868197441101074\n",
      "Explained Variance: 0.0006654262542724609\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 390\n",
      "Episodic Return: [ 601.96454  840.6303  -617.10077 -791.74097]\n",
      "Smoothed Returns for agent_0 (Training): 394.8939514160156\n",
      "Smoothed Returns for agent_1 (Training): 262.4144592285156\n",
      "Smoothed Returns for agent_2 (Training): -179.76400756835938\n",
      "Smoothed Returns for agent_3 (Training): -542.2908935546875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 5407.8125\n",
      "Policy Loss: 43.828067779541016\n",
      "Old Approx KL: -0.005292790476232767\n",
      "Approx KL: 0.0019229225581511855\n",
      "Clip Fraction: 0.08886054513000306\n",
      "Explained Variance: 0.0014973282814025879\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 400\n",
      "Episodic Return: [ 290.92014  204.0922  -799.931    497.3761 ]\n",
      "Smoothed Returns for agent_0 (Training): 385.38311767578125\n",
      "Smoothed Returns for agent_1 (Training): 234.8238067626953\n",
      "Smoothed Returns for agent_2 (Training): -327.70648193359375\n",
      "Smoothed Returns for agent_3 (Training): -289.06951904296875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3225.54833984375\n",
      "Policy Loss: -22.608821868896484\n",
      "Old Approx KL: -0.012265291064977646\n",
      "Approx KL: 0.0032933654729276896\n",
      "Clip Fraction: 0.1244685383779662\n",
      "Explained Variance: 0.0050643086433410645\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 410\n",
      "Episodic Return: [-362.7046   545.70874 -429.02643  348.02716]\n",
      "Smoothed Returns for agent_0 (Training): 491.85919189453125\n",
      "Smoothed Returns for agent_1 (Training): 380.3340759277344\n",
      "Smoothed Returns for agent_2 (Training): -431.4146423339844\n",
      "Smoothed Returns for agent_3 (Training): -452.14697265625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2151.144287109375\n",
      "Policy Loss: -35.45155715942383\n",
      "Old Approx KL: -0.0016054341103881598\n",
      "Approx KL: 0.0011944685829803348\n",
      "Clip Fraction: 0.009353741648651305\n",
      "Explained Variance: 0.004987001419067383\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 420\n",
      "Episodic Return: [  447.23364  1912.4662  -1268.5573  -1247.8348 ]\n",
      "Smoothed Returns for agent_0 (Training): 543.1484985351562\n",
      "Smoothed Returns for agent_1 (Training): 557.3670043945312\n",
      "Smoothed Returns for agent_2 (Training): -354.6897888183594\n",
      "Smoothed Returns for agent_3 (Training): -805.6378784179688\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 10934.669921875\n",
      "Policy Loss: 97.64861297607422\n",
      "Old Approx KL: 0.047025036066770554\n",
      "Approx KL: 0.004997262265533209\n",
      "Clip Fraction: 0.36426446054662975\n",
      "Explained Variance: 0.00341188907623291\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 430\n",
      "Episodic Return: [-378.1839      12.8479805 -245.8464     754.551    ]\n",
      "Smoothed Returns for agent_0 (Training): 569.6438598632812\n",
      "Smoothed Returns for agent_1 (Training): 628.3074340820312\n",
      "Smoothed Returns for agent_2 (Training): -620.2066650390625\n",
      "Smoothed Returns for agent_3 (Training): -698.0789794921875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3427.434326171875\n",
      "Policy Loss: -34.1809196472168\n",
      "Old Approx KL: -0.006497988011687994\n",
      "Approx KL: 0.0014025953132659197\n",
      "Clip Fraction: 0.07748724520206451\n",
      "Explained Variance: 0.017570316791534424\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 440\n",
      "Episodic Return: [ 1259.7324   465.9873 -1236.3191  -937.0166]\n",
      "Smoothed Returns for agent_0 (Training): 590.3739624023438\n",
      "Smoothed Returns for agent_1 (Training): 666.2586059570312\n",
      "Smoothed Returns for agent_2 (Training): -804.8065185546875\n",
      "Smoothed Returns for agent_3 (Training): -566.3892822265625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 5151.2763671875\n",
      "Policy Loss: 62.360504150390625\n",
      "Old Approx KL: 0.008966395631432533\n",
      "Approx KL: 0.006291364319622517\n",
      "Clip Fraction: 0.2816751712844485\n",
      "Explained Variance: 0.006525516510009766\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 450\n",
      "Episodic Return: [   50.16481   342.96503   826.1235  -1055.8494 ]\n",
      "Smoothed Returns for agent_0 (Training): 382.25750732421875\n",
      "Smoothed Returns for agent_1 (Training): 657.0260620117188\n",
      "Smoothed Returns for agent_2 (Training): -567.1314697265625\n",
      "Smoothed Returns for agent_3 (Training): -502.9325256347656\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3020.922119140625\n",
      "Policy Loss: -39.040870666503906\n",
      "Old Approx KL: 0.04547537490725517\n",
      "Approx KL: 0.006556740961968899\n",
      "Clip Fraction: 0.1820790838627588\n",
      "Explained Variance: 0.05027484893798828\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 460\n",
      "Episodic Return: [ 442.80872  339.62802 -440.71484 -363.55774]\n",
      "Smoothed Returns for agent_0 (Training): 223.06179809570312\n",
      "Smoothed Returns for agent_1 (Training): 390.95672607421875\n",
      "Smoothed Returns for agent_2 (Training): -322.08905029296875\n",
      "Smoothed Returns for agent_3 (Training): -250.67367553710938\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1379.6527099609375\n",
      "Policy Loss: -19.137290954589844\n",
      "Old Approx KL: 0.007145550101995468\n",
      "Approx KL: 0.004666703287512064\n",
      "Clip Fraction: 0.10671768834193547\n",
      "Explained Variance: 0.01154637336730957\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 470\n",
      "Episodic Return: [-385.60843    20.388351  206.87305   135.9687  ]\n",
      "Smoothed Returns for agent_0 (Training): 253.3491668701172\n",
      "Smoothed Returns for agent_1 (Training): 214.2701416015625\n",
      "Smoothed Returns for agent_2 (Training): -361.5624694824219\n",
      "Smoothed Returns for agent_3 (Training): -91.3554916381836\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 689.437744140625\n",
      "Policy Loss: -35.0825309753418\n",
      "Old Approx KL: -0.0019965344108641148\n",
      "Approx KL: 0.001344195450656116\n",
      "Clip Fraction: 0.07472364072288785\n",
      "Explained Variance: 0.03965407609939575\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 480\n",
      "Episodic Return: [-182.63686   -42.285843   68.41335   128.26527 ]\n",
      "Smoothed Returns for agent_0 (Training): 196.17755126953125\n",
      "Smoothed Returns for agent_1 (Training): 206.87875366210938\n",
      "Smoothed Returns for agent_2 (Training): -275.5809326171875\n",
      "Smoothed Returns for agent_3 (Training): -174.769287109375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 543.4945068359375\n",
      "Policy Loss: -2.975863456726074\n",
      "Old Approx KL: 0.010425014421343803\n",
      "Approx KL: 0.0016327756457030773\n",
      "Clip Fraction: 0.07525510234492165\n",
      "Explained Variance: -0.010870933532714844\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 490\n",
      "Episodic Return: [ 358.94897  402.77258 -503.64087 -327.24567]\n",
      "Smoothed Returns for agent_0 (Training): 21.860881805419922\n",
      "Smoothed Returns for agent_1 (Training): 57.1058349609375\n",
      "Smoothed Returns for agent_2 (Training): -17.841073989868164\n",
      "Smoothed Returns for agent_3 (Training): -113.53670501708984\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 479.97601318359375\n",
      "Policy Loss: 14.040840148925781\n",
      "Old Approx KL: -0.013326866552233696\n",
      "Approx KL: 0.0024249255657196045\n",
      "Clip Fraction: 0.07801870859804608\n",
      "Explained Variance: 0.011919796466827393\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 500\n",
      "Episodic Return: [-69.46123  -28.944433 -12.834116 157.2843  ]\n",
      "Smoothed Returns for agent_0 (Training): -75.3880615234375\n",
      "Smoothed Returns for agent_1 (Training): -55.693260192871094\n",
      "Smoothed Returns for agent_2 (Training): 81.71666717529297\n",
      "Smoothed Returns for agent_3 (Training): 32.458709716796875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1123.4481201171875\n",
      "Policy Loss: 20.9591064453125\n",
      "Old Approx KL: 0.011755654588341713\n",
      "Approx KL: 0.0003428672207519412\n",
      "Clip Fraction: 0.06664540831531797\n",
      "Explained Variance: 0.002216160297393799\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 510\n",
      "Episodic Return: [  26.308342  205.25124   544.96344  -656.26794 ]\n",
      "Smoothed Returns for agent_0 (Training): 38.450660705566406\n",
      "Smoothed Returns for agent_1 (Training): 32.305572509765625\n",
      "Smoothed Returns for agent_2 (Training): -152.5151824951172\n",
      "Smoothed Returns for agent_3 (Training): 85.98509216308594\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 503.045166015625\n",
      "Policy Loss: 0.7374414801597595\n",
      "Old Approx KL: 0.015550222247838974\n",
      "Approx KL: 0.0009465728653594851\n",
      "Clip Fraction: 0.029868197583016894\n",
      "Explained Variance: 0.059857308864593506\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 520\n",
      "Episodic Return: [-579.63696   468.86276   -48.461132   81.759415]\n",
      "Smoothed Returns for agent_0 (Training): 54.39844512939453\n",
      "Smoothed Returns for agent_1 (Training): 26.1146183013916\n",
      "Smoothed Returns for agent_2 (Training): -178.3366241455078\n",
      "Smoothed Returns for agent_3 (Training): 9.627057075500488\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1272.67529296875\n",
      "Policy Loss: -34.909690856933594\n",
      "Old Approx KL: -0.0002536433166824281\n",
      "Approx KL: 0.0010421276092529297\n",
      "Clip Fraction: 0.07982568088031951\n",
      "Explained Variance: 0.014204859733581543\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 530\n",
      "Episodic Return: [ -35.8485   171.64874   37.58169 -112.71861]\n",
      "Smoothed Returns for agent_0 (Training): -75.41474914550781\n",
      "Smoothed Returns for agent_1 (Training): 159.72109985351562\n",
      "Smoothed Returns for agent_2 (Training): -85.70061492919922\n",
      "Smoothed Returns for agent_3 (Training): -127.16453552246094\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2539.629638671875\n",
      "Policy Loss: 27.740432739257812\n",
      "Old Approx KL: 0.054988276213407516\n",
      "Approx KL: 0.00339735415764153\n",
      "Clip Fraction: 0.0889668379511152\n",
      "Explained Variance: 0.021807610988616943\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 540\n",
      "Episodic Return: [-165.1678   638.3355  -210.4182  -250.03668]\n",
      "Smoothed Returns for agent_0 (Training): -49.97034454345703\n",
      "Smoothed Returns for agent_1 (Training): 285.9561767578125\n",
      "Smoothed Returns for agent_2 (Training): -154.64515686035156\n",
      "Smoothed Returns for agent_3 (Training): -107.05375671386719\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 929.41650390625\n",
      "Policy Loss: -30.705524444580078\n",
      "Old Approx KL: -0.013253766112029552\n",
      "Approx KL: 0.0023578235413879156\n",
      "Clip Fraction: 0.05144557853539785\n",
      "Explained Variance: 0.005381584167480469\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 550\n",
      "Episodic Return: [-107.41759   -44.63666   238.55484   -52.828888]\n",
      "Smoothed Returns for agent_0 (Training): -24.09200096130371\n",
      "Smoothed Returns for agent_1 (Training): 190.53848266601562\n",
      "Smoothed Returns for agent_2 (Training): -107.48814392089844\n",
      "Smoothed Returns for agent_3 (Training): -79.33289337158203\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 222.88180541992188\n",
      "Policy Loss: 4.0957183837890625\n",
      "Old Approx KL: 0.0050667524337768555\n",
      "Approx KL: 0.0005214257398620248\n",
      "Clip Fraction: 0.019026360696270353\n",
      "Explained Variance: -0.005391836166381836\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 560\n",
      "Episodic Return: [-410.9166  -503.72522  353.4263   628.67285]\n",
      "Smoothed Returns for agent_0 (Training): -83.21582794189453\n",
      "Smoothed Returns for agent_1 (Training): 51.621307373046875\n",
      "Smoothed Returns for agent_2 (Training): 9.778425216674805\n",
      "Smoothed Returns for agent_3 (Training): -22.684967041015625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 149.8149871826172\n",
      "Policy Loss: -6.479986190795898\n",
      "Old Approx KL: -0.0034231289755553007\n",
      "Approx KL: 0.00013395292626228184\n",
      "Clip Fraction: 0.053252551172460826\n",
      "Explained Variance: -0.03489720821380615\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 570\n",
      "Episodic Return: [  164.44243   593.5558  -1000.8304    323.75513]\n",
      "Smoothed Returns for agent_0 (Training): -129.14657592773438\n",
      "Smoothed Returns for agent_1 (Training): 37.70802688598633\n",
      "Smoothed Returns for agent_2 (Training): -26.270782470703125\n",
      "Smoothed Returns for agent_3 (Training): 47.7848014831543\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2535.08056640625\n",
      "Policy Loss: 27.439655303955078\n",
      "Old Approx KL: -0.006779041141271591\n",
      "Approx KL: 0.0002825430710799992\n",
      "Clip Fraction: 0.06558248329730261\n",
      "Explained Variance: 0.034393310546875\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 580\n",
      "Episodic Return: [-342.77408 -477.25046  398.9809   462.0124 ]\n",
      "Smoothed Returns for agent_0 (Training): -120.8310546875\n",
      "Smoothed Returns for agent_1 (Training): 80.84245300292969\n",
      "Smoothed Returns for agent_2 (Training): -62.37971115112305\n",
      "Smoothed Returns for agent_3 (Training): 29.789514541625977\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 764.4342651367188\n",
      "Policy Loss: 9.408615112304688\n",
      "Old Approx KL: 0.00024603947531431913\n",
      "Approx KL: 0.0008866957505233586\n",
      "Clip Fraction: 0.011160714285714286\n",
      "Explained Variance: -0.05651116371154785\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 590\n",
      "Episodic Return: [  77.71345  -388.71677   -12.583603  166.09221 ]\n",
      "Smoothed Returns for agent_0 (Training): -35.741127014160156\n",
      "Smoothed Returns for agent_1 (Training): 7.474296569824219\n",
      "Smoothed Returns for agent_2 (Training): -43.19832229614258\n",
      "Smoothed Returns for agent_3 (Training): 4.294347286224365\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1469.328857421875\n",
      "Policy Loss: 39.62827682495117\n",
      "Old Approx KL: 0.002892715623602271\n",
      "Approx KL: 6.252101593418047e-05\n",
      "Clip Fraction: 0.05442176901158832\n",
      "Explained Variance: 0.01723533868789673\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 600\n",
      "Episodic Return: [ 357.04755   -38.02279  -580.62225    96.004715]\n",
      "Smoothed Returns for agent_0 (Training): -32.68602752685547\n",
      "Smoothed Returns for agent_1 (Training): -88.98857116699219\n",
      "Smoothed Returns for agent_2 (Training): -10.831811904907227\n",
      "Smoothed Returns for agent_3 (Training): 86.70821380615234\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 949.160888671875\n",
      "Policy Loss: 1.7568060159683228\n",
      "Old Approx KL: -0.007652146741747856\n",
      "Approx KL: 0.0062884436920285225\n",
      "Clip Fraction: 0.06090561335995084\n",
      "Explained Variance: -0.005000948905944824\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 610\n",
      "Episodic Return: [  99.48836    50.138092   77.37296  -153.07794 ]\n",
      "Smoothed Returns for agent_0 (Training): -14.60510540008545\n",
      "Smoothed Returns for agent_1 (Training): -140.4909210205078\n",
      "Smoothed Returns for agent_2 (Training): 55.6500244140625\n",
      "Smoothed Returns for agent_3 (Training): 84.02650451660156\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1018.6007080078125\n",
      "Policy Loss: 8.820033073425293\n",
      "Old Approx KL: 0.011302974075078964\n",
      "Approx KL: 0.0018948104698210955\n",
      "Clip Fraction: 0.04187925230889093\n",
      "Explained Variance: 0.05214965343475342\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 620\n",
      "Episodic Return: [-231.49283   347.571     -67.34041   -24.887873]\n",
      "Smoothed Returns for agent_0 (Training): -30.883087158203125\n",
      "Smoothed Returns for agent_1 (Training): -73.95794677734375\n",
      "Smoothed Returns for agent_2 (Training): 54.01111602783203\n",
      "Smoothed Returns for agent_3 (Training): 14.013626098632812\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1442.9727783203125\n",
      "Policy Loss: 20.645627975463867\n",
      "Old Approx KL: -0.004457985050976276\n",
      "Approx KL: 3.4966640669154e-05\n",
      "Clip Fraction: 0.05899234754698617\n",
      "Explained Variance: 0.03469657897949219\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 630\n",
      "Episodic Return: [  15.329618 -205.40881   271.5776     49.065605]\n",
      "Smoothed Returns for agent_0 (Training): -102.66865539550781\n",
      "Smoothed Returns for agent_1 (Training): -128.4598846435547\n",
      "Smoothed Returns for agent_2 (Training): 80.04207611083984\n",
      "Smoothed Returns for agent_3 (Training): 118.78813171386719\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 201.78289794921875\n",
      "Policy Loss: 1.290404200553894\n",
      "Old Approx KL: -0.0037079881876707077\n",
      "Approx KL: 0.00042277577449567616\n",
      "Clip Fraction: 0.06781462615444547\n",
      "Explained Variance: -0.039684414863586426\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 640\n",
      "Episodic Return: [ 153.7364  -161.9074   317.87738 -388.08115]\n",
      "Smoothed Returns for agent_0 (Training): -152.8774871826172\n",
      "Smoothed Returns for agent_1 (Training): -224.0717010498047\n",
      "Smoothed Returns for agent_2 (Training): 199.1581573486328\n",
      "Smoothed Returns for agent_3 (Training): 180.68617248535156\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 817.201904296875\n",
      "Policy Loss: 14.83629322052002\n",
      "Old Approx KL: 0.0160032007843256\n",
      "Approx KL: 0.0034300857223570347\n",
      "Clip Fraction: 0.07397959310384024\n",
      "Explained Variance: 0.07107049226760864\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 650\n",
      "Episodic Return: [-196.30634 -266.85794    8.68602  491.10358]\n",
      "Smoothed Returns for agent_0 (Training): -63.268714904785156\n",
      "Smoothed Returns for agent_1 (Training): -170.65646362304688\n",
      "Smoothed Returns for agent_2 (Training): 67.27722930908203\n",
      "Smoothed Returns for agent_3 (Training): 127.1978988647461\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 514.9515380859375\n",
      "Policy Loss: -20.8558349609375\n",
      "Old Approx KL: 0.015203851275146008\n",
      "Approx KL: 0.0007464119698852301\n",
      "Clip Fraction: 0.05420918478852227\n",
      "Explained Variance: 0.013684093952178955\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 660\n",
      "Episodic Return: [ -20.96766 -473.35855  343.19037 -146.31679]\n",
      "Smoothed Returns for agent_0 (Training): 12.811059951782227\n",
      "Smoothed Returns for agent_1 (Training): -117.6116943359375\n",
      "Smoothed Returns for agent_2 (Training): -41.35491180419922\n",
      "Smoothed Returns for agent_3 (Training): 53.470550537109375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 366.87530517578125\n",
      "Policy Loss: -8.514442443847656\n",
      "Old Approx KL: 0.01917307823896408\n",
      "Approx KL: 0.0010411612456664443\n",
      "Clip Fraction: 0.06420068088031951\n",
      "Explained Variance: 0.012907683849334717\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 670\n",
      "Episodic Return: [ 340.79608 -237.25055  384.93985 -350.57104]\n",
      "Smoothed Returns for agent_0 (Training): -73.0212631225586\n",
      "Smoothed Returns for agent_1 (Training): -60.619407653808594\n",
      "Smoothed Returns for agent_2 (Training): 34.93988800048828\n",
      "Smoothed Returns for agent_3 (Training): 14.49462890625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1692.5079345703125\n",
      "Policy Loss: 35.307838439941406\n",
      "Old Approx KL: 0.020697440952062607\n",
      "Approx KL: 0.01001193281263113\n",
      "Clip Fraction: 0.10299745024669738\n",
      "Explained Variance: 0.08625096082687378\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 680\n",
      "Episodic Return: [  415.27557   312.3798    584.6007  -1105.4507 ]\n",
      "Smoothed Returns for agent_0 (Training): -46.627254486083984\n",
      "Smoothed Returns for agent_1 (Training): 11.804783821105957\n",
      "Smoothed Returns for agent_2 (Training): 7.0996551513671875\n",
      "Smoothed Returns for agent_3 (Training): -40.12278747558594\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 361.0137634277344\n",
      "Policy Loss: 22.78720474243164\n",
      "Old Approx KL: -0.002129980595782399\n",
      "Approx KL: 0.0035661461297422647\n",
      "Clip Fraction: 0.12680697405622118\n",
      "Explained Variance: 0.08434844017028809\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 690\n",
      "Episodic Return: [ 451.3457  -169.59978 -347.8268   131.56056]\n",
      "Smoothed Returns for agent_0 (Training): 167.59439086914062\n",
      "Smoothed Returns for agent_1 (Training): 133.35989379882812\n",
      "Smoothed Returns for agent_2 (Training): -196.5360107421875\n",
      "Smoothed Returns for agent_3 (Training): -184.8289031982422\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 874.0291748046875\n",
      "Policy Loss: 16.686429977416992\n",
      "Old Approx KL: 0.0003224611282348633\n",
      "Approx KL: 0.0006479791481979191\n",
      "Clip Fraction: 0.06717687135650999\n",
      "Explained Variance: 0.043398499488830566\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 700\n",
      "Episodic Return: [-296.03287 -374.47015  643.3514   153.46513]\n",
      "Smoothed Returns for agent_0 (Training): 262.95208740234375\n",
      "Smoothed Returns for agent_1 (Training): 164.63388061523438\n",
      "Smoothed Returns for agent_2 (Training): -83.24639892578125\n",
      "Smoothed Returns for agent_3 (Training): -369.96685791015625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 495.1868896484375\n",
      "Policy Loss: -27.44744110107422\n",
      "Old Approx KL: -0.018693873658776283\n",
      "Approx KL: 0.0012591865379363298\n",
      "Clip Fraction: 0.10926870824325652\n",
      "Explained Variance: -0.16042876243591309\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 710\n",
      "Episodic Return: [-294.99905 -565.3838   408.67636  451.38885]\n",
      "Smoothed Returns for agent_0 (Training): 134.17105102539062\n",
      "Smoothed Returns for agent_1 (Training): 56.211700439453125\n",
      "Smoothed Returns for agent_2 (Training): 129.42312622070312\n",
      "Smoothed Returns for agent_3 (Training): -300.5540771484375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1265.905517578125\n",
      "Policy Loss: 12.248254776000977\n",
      "Old Approx KL: -0.0038282531313598156\n",
      "Approx KL: 0.0020397689659148455\n",
      "Clip Fraction: 0.05335884399357296\n",
      "Explained Variance: 0.02439361810684204\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 720\n",
      "Episodic Return: [-187.43402  -81.16198  131.53249  206.97556]\n",
      "Smoothed Returns for agent_0 (Training): 37.622642517089844\n",
      "Smoothed Returns for agent_1 (Training): -86.86430358886719\n",
      "Smoothed Returns for agent_2 (Training): 13.059832572937012\n",
      "Smoothed Returns for agent_3 (Training): -3.7372207641601562\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 245.05532836914062\n",
      "Policy Loss: -9.790440559387207\n",
      "Old Approx KL: -0.007723127491772175\n",
      "Approx KL: 0.00015611734124831855\n",
      "Clip Fraction: 0.0417729594877788\n",
      "Explained Variance: -0.06753253936767578\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 730\n",
      "Episodic Return: [-571.9117  -654.0711   740.56415  306.9518 ]\n",
      "Smoothed Returns for agent_0 (Training): -27.21590232849121\n",
      "Smoothed Returns for agent_1 (Training): -159.42344665527344\n",
      "Smoothed Returns for agent_2 (Training): 54.100059509277344\n",
      "Smoothed Returns for agent_3 (Training): 109.384765625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 794.6356811523438\n",
      "Policy Loss: 18.054563522338867\n",
      "Old Approx KL: 0.02011459320783615\n",
      "Approx KL: 0.0008636883576400578\n",
      "Clip Fraction: 0.059311224945953915\n",
      "Explained Variance: 0.1161077618598938\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 740\n",
      "Episodic Return: [-167.7039  -221.50888  -81.9939   494.68933]\n",
      "Smoothed Returns for agent_0 (Training): -235.2438507080078\n",
      "Smoothed Returns for agent_1 (Training): -221.84225463867188\n",
      "Smoothed Returns for agent_2 (Training): 189.19847106933594\n",
      "Smoothed Returns for agent_3 (Training): 254.2200469970703\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2794.540771484375\n",
      "Policy Loss: 45.411006927490234\n",
      "Old Approx KL: 0.030169522389769554\n",
      "Approx KL: 0.004350006580352783\n",
      "Clip Fraction: 0.07153486424968356\n",
      "Explained Variance: 0.10333776473999023\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 750\n",
      "Episodic Return: [  722.38806   698.2539    401.06076 -1604.2738 ]\n",
      "Smoothed Returns for agent_0 (Training): -228.35977172851562\n",
      "Smoothed Returns for agent_1 (Training): -260.7771911621094\n",
      "Smoothed Returns for agent_2 (Training): 333.7782897949219\n",
      "Smoothed Returns for agent_3 (Training): 241.1719970703125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2038.2066650390625\n",
      "Policy Loss: 21.997468948364258\n",
      "Old Approx KL: 0.08284609764814377\n",
      "Approx KL: 0.008344939909875393\n",
      "Clip Fraction: 0.20556972991852535\n",
      "Explained Variance: 0.049767374992370605\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 760\n",
      "Episodic Return: [  566.437     262.68307  1017.97864 -1291.5927 ]\n",
      "Smoothed Returns for agent_0 (Training): 94.47103118896484\n",
      "Smoothed Returns for agent_1 (Training): -8.986262321472168\n",
      "Smoothed Returns for agent_2 (Training): 287.20318603515625\n",
      "Smoothed Returns for agent_3 (Training): -223.137451171875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1961.852294921875\n",
      "Policy Loss: 32.062530517578125\n",
      "Old Approx KL: 0.009246443398296833\n",
      "Approx KL: 0.012324001640081406\n",
      "Clip Fraction: 0.12308673560619354\n",
      "Explained Variance: -0.020141959190368652\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 770\n",
      "Episodic Return: [-729.9799  -860.5141  1200.2306   728.64685]\n",
      "Smoothed Returns for agent_0 (Training): -55.416297912597656\n",
      "Smoothed Returns for agent_1 (Training): -127.51033020019531\n",
      "Smoothed Returns for agent_2 (Training): 317.2398986816406\n",
      "Smoothed Returns for agent_3 (Training): 68.31849670410156\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 98.0326919555664\n",
      "Policy Loss: -10.902294158935547\n",
      "Old Approx KL: 0.0027206456288695335\n",
      "Approx KL: 1.9426857761573046e-05\n",
      "Clip Fraction: 0.002976190476190476\n",
      "Explained Variance: -0.0026025772094726562\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 780\n",
      "Episodic Return: [-866.2957 -867.0155  858.5327 1347.4264]\n",
      "Smoothed Returns for agent_0 (Training): -167.40428161621094\n",
      "Smoothed Returns for agent_1 (Training): -249.4425506591797\n",
      "Smoothed Returns for agent_2 (Training): 316.68218994140625\n",
      "Smoothed Returns for agent_3 (Training): 310.03802490234375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 95.27914428710938\n",
      "Policy Loss: -5.671576976776123\n",
      "Old Approx KL: -0.0003144060028716922\n",
      "Approx KL: 2.524682486182428e-06\n",
      "Clip Fraction: 0.002976190476190476\n",
      "Explained Variance: 0.07871878147125244\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 790\n",
      "Episodic Return: [-538.1408  -540.63495  574.97266  946.4102 ]\n",
      "Smoothed Returns for agent_0 (Training): 113.52641296386719\n",
      "Smoothed Returns for agent_1 (Training): 54.037010192871094\n",
      "Smoothed Returns for agent_2 (Training): -143.53895568847656\n",
      "Smoothed Returns for agent_3 (Training): 102.99754333496094\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 71.80194091796875\n",
      "Policy Loss: 4.676137924194336\n",
      "Old Approx KL: 0.0038310461677610874\n",
      "Approx KL: 1.5688794519519433e-05\n",
      "Clip Fraction: 0.005952380952380952\n",
      "Explained Variance: -0.18840861320495605\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 800\n",
      "Episodic Return: [-407.8641  -443.46033  428.72577  751.57355]\n",
      "Smoothed Returns for agent_0 (Training): 231.89340209960938\n",
      "Smoothed Returns for agent_1 (Training): 130.2695770263672\n",
      "Smoothed Returns for agent_2 (Training): -349.7083435058594\n",
      "Smoothed Returns for agent_3 (Training): 104.24066162109375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 64.82064819335938\n",
      "Policy Loss: 3.522361993789673\n",
      "Old Approx KL: 0.007632170803844929\n",
      "Approx KL: 0.00038426264654845\n",
      "Clip Fraction: 0.004464285714285714\n",
      "Explained Variance: 0.06959879398345947\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 810\n",
      "Episodic Return: [  509.1161   162.3074   930.6892 -1169.7323]\n",
      "Smoothed Returns for agent_0 (Training): 114.7765121459961\n",
      "Smoothed Returns for agent_1 (Training): -18.133333206176758\n",
      "Smoothed Returns for agent_2 (Training): 41.56642532348633\n",
      "Smoothed Returns for agent_3 (Training): 20.937602996826172\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 650.8349609375\n",
      "Policy Loss: 0.9230600595474243\n",
      "Old Approx KL: 0.018903963267803192\n",
      "Approx KL: 0.003464379580691457\n",
      "Clip Fraction: 0.08875425266368049\n",
      "Explained Variance: 0.125746488571167\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 820\n",
      "Episodic Return: [1247.1042   -65.64976 -565.73535 -439.47113]\n",
      "Smoothed Returns for agent_0 (Training): 151.86221313476562\n",
      "Smoothed Returns for agent_1 (Training): -67.41104125976562\n",
      "Smoothed Returns for agent_2 (Training): 112.7611312866211\n",
      "Smoothed Returns for agent_3 (Training): -38.81376266479492\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1977.5201416015625\n",
      "Policy Loss: 29.48495864868164\n",
      "Old Approx KL: 0.0028260948602110147\n",
      "Approx KL: 0.00210151937790215\n",
      "Clip Fraction: 0.17846513858863286\n",
      "Explained Variance: 0.022966384887695312\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 830\n",
      "Episodic Return: [  -6.5112247 -511.07855    839.5893    -311.9989   ]\n",
      "Smoothed Returns for agent_0 (Training): 250.89047241210938\n",
      "Smoothed Returns for agent_1 (Training): -22.591346740722656\n",
      "Smoothed Returns for agent_2 (Training): -79.39485168457031\n",
      "Smoothed Returns for agent_3 (Training): -82.32624816894531\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 306.5606994628906\n",
      "Policy Loss: 7.786969184875488\n",
      "Old Approx KL: 0.02434517815709114\n",
      "Approx KL: 0.0055017899721860886\n",
      "Clip Fraction: 0.11649660056545622\n",
      "Explained Variance: 0.16073912382125854\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 840\n",
      "Episodic Return: [ 248.95679  248.07715 -696.9907  -189.87202]\n",
      "Smoothed Returns for agent_0 (Training): 200.293212890625\n",
      "Smoothed Returns for agent_1 (Training): 53.85368728637695\n",
      "Smoothed Returns for agent_2 (Training): -69.3032455444336\n",
      "Smoothed Returns for agent_3 (Training): -53.46657180786133\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1769.05419921875\n",
      "Policy Loss: 42.30681610107422\n",
      "Old Approx KL: 0.035645078867673874\n",
      "Approx KL: 0.004345344845205545\n",
      "Clip Fraction: 0.08535289196741014\n",
      "Explained Variance: 0.11105567216873169\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 850\n",
      "Episodic Return: [  81.264786  693.947     116.88287  -651.3449  ]\n",
      "Smoothed Returns for agent_0 (Training): 321.73846435546875\n",
      "Smoothed Returns for agent_1 (Training): 216.3444366455078\n",
      "Smoothed Returns for agent_2 (Training): -66.2391357421875\n",
      "Smoothed Returns for agent_3 (Training): -257.94342041015625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1076.239013671875\n",
      "Policy Loss: 14.753822326660156\n",
      "Old Approx KL: 0.021806079894304276\n",
      "Approx KL: 0.0018337029032409191\n",
      "Clip Fraction: 0.09704506929431643\n",
      "Explained Variance: 0.051966845989227295\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 860\n",
      "Episodic Return: [  328.208     286.19858 -1114.7646    870.6068 ]\n",
      "Smoothed Returns for agent_0 (Training): 332.18011474609375\n",
      "Smoothed Returns for agent_1 (Training): 310.98175048828125\n",
      "Smoothed Returns for agent_2 (Training): -236.0411834716797\n",
      "Smoothed Returns for agent_3 (Training): -246.0800018310547\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 506.9251708984375\n",
      "Policy Loss: -11.091764450073242\n",
      "Old Approx KL: -0.002401249948889017\n",
      "Approx KL: 0.0010350304655730724\n",
      "Clip Fraction: 0.06335034044015975\n",
      "Explained Variance: 0.026916325092315674\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 870\n",
      "Episodic Return: [  36.91625  -750.46954   -41.959995  811.5875  ]\n",
      "Smoothed Returns for agent_0 (Training): 279.1658020019531\n",
      "Smoothed Returns for agent_1 (Training): 250.71728515625\n",
      "Smoothed Returns for agent_2 (Training): -286.087646484375\n",
      "Smoothed Returns for agent_3 (Training): -117.7569808959961\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 974.4905395507812\n",
      "Policy Loss: -9.240270614624023\n",
      "Old Approx KL: 0.03031865321099758\n",
      "Approx KL: 0.007066565100103617\n",
      "Clip Fraction: 0.022427721392540706\n",
      "Explained Variance: -0.1561565399169922\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 880\n",
      "Episodic Return: [ 245.76468  899.2677  -615.4878  -648.8649 ]\n",
      "Smoothed Returns for agent_0 (Training): 200.92047119140625\n",
      "Smoothed Returns for agent_1 (Training): 159.64633178710938\n",
      "Smoothed Returns for agent_2 (Training): -88.72290802001953\n",
      "Smoothed Returns for agent_3 (Training): -143.16017150878906\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 816.235595703125\n",
      "Policy Loss: 10.847147941589355\n",
      "Old Approx KL: 0.02100207284092903\n",
      "Approx KL: 0.0016183641273528337\n",
      "Clip Fraction: 0.05963010234492166\n",
      "Explained Variance: 0.04519134759902954\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 890\n",
      "Episodic Return: [-496.10605 -523.6379   684.2442   528.8529 ]\n",
      "Smoothed Returns for agent_0 (Training): 44.1376838684082\n",
      "Smoothed Returns for agent_1 (Training): 229.9084930419922\n",
      "Smoothed Returns for agent_2 (Training): -40.527244567871094\n",
      "Smoothed Returns for agent_3 (Training): -84.61624145507812\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1854.5296630859375\n",
      "Policy Loss: -34.33461380004883\n",
      "Old Approx KL: 0.004370825830847025\n",
      "Approx KL: 6.094149284763262e-05\n",
      "Clip Fraction: 0.018601190476190476\n",
      "Explained Variance: -0.11021912097930908\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 900\n",
      "Episodic Return: [ 283.88208  182.21483 -978.25214  976.48865]\n",
      "Smoothed Returns for agent_0 (Training): 129.67178344726562\n",
      "Smoothed Returns for agent_1 (Training): 332.81201171875\n",
      "Smoothed Returns for agent_2 (Training): -227.1547393798828\n",
      "Smoothed Returns for agent_3 (Training): -114.6777572631836\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 361.39007568359375\n",
      "Policy Loss: 0.7625896334648132\n",
      "Old Approx KL: -0.02098648063838482\n",
      "Approx KL: 0.006916102021932602\n",
      "Clip Fraction: 0.19791666808582487\n",
      "Explained Variance: -0.11462020874023438\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 910\n",
      "Episodic Return: [  289.3569    456.166     678.42645 -1089.849  ]\n",
      "Smoothed Returns for agent_0 (Training): 188.32186889648438\n",
      "Smoothed Returns for agent_1 (Training): 307.31524658203125\n",
      "Smoothed Returns for agent_2 (Training): 116.73451232910156\n",
      "Smoothed Returns for agent_3 (Training): -429.0497131347656\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 433.5982666015625\n",
      "Policy Loss: 6.3444905281066895\n",
      "Old Approx KL: 0.008849297650158405\n",
      "Approx KL: 0.0007263975567184389\n",
      "Clip Fraction: 0.09024234790177572\n",
      "Explained Variance: 0.10708904266357422\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 920\n",
      "Episodic Return: [-590.14874 -587.61615  573.11475 1322.8842 ]\n",
      "Smoothed Returns for agent_0 (Training): 57.75712966918945\n",
      "Smoothed Returns for agent_1 (Training): 93.58403015136719\n",
      "Smoothed Returns for agent_2 (Training): 448.9205017089844\n",
      "Smoothed Returns for agent_3 (Training): -310.95697021484375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1064.380615234375\n",
      "Policy Loss: 2.7698915004730225\n",
      "Old Approx KL: 0.004313877783715725\n",
      "Approx KL: 1.9448145394562744e-05\n",
      "Clip Fraction: 0.003720238095238095\n",
      "Explained Variance: -0.030327916145324707\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 930\n",
      "Episodic Return: [ 617.84937   -35.229313  167.43065  -790.8471  ]\n",
      "Smoothed Returns for agent_0 (Training): 103.9601821899414\n",
      "Smoothed Returns for agent_1 (Training): 3.7924530506134033\n",
      "Smoothed Returns for agent_2 (Training): 316.19268798828125\n",
      "Smoothed Returns for agent_3 (Training): -197.7330780029297\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1343.6705322265625\n",
      "Policy Loss: -0.39879992604255676\n",
      "Old Approx KL: -0.0145929716527462\n",
      "Approx KL: 0.000912747229449451\n",
      "Clip Fraction: 0.025403911868731182\n",
      "Explained Variance: 0.022660672664642334\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 940\n",
      "Episodic Return: [ 339.06946   382.18457  -634.59607   -22.822636]\n",
      "Smoothed Returns for agent_0 (Training): 234.702392578125\n",
      "Smoothed Returns for agent_1 (Training): 190.45657348632812\n",
      "Smoothed Returns for agent_2 (Training): -80.92958068847656\n",
      "Smoothed Returns for agent_3 (Training): -268.3777160644531\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 681.5406494140625\n",
      "Policy Loss: -28.532245635986328\n",
      "Old Approx KL: -0.011416393332183361\n",
      "Approx KL: 0.0008193510002456605\n",
      "Clip Fraction: 0.06887755117246083\n",
      "Explained Variance: -0.025263071060180664\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "        project=\"multi-agent-ppo\",  # Set your project name\n",
    "        name=ckpt_name,\n",
    "        config={\n",
    "            \"env\": envname,\n",
    "            \"GRID_SIZE\": GRID_SIZE,\n",
    "            \"NUM_THINGS\": NUM_THINGS,\n",
    "            \"INITIALIZATIONS\": INITIALIZATIONS,\n",
    "            \"IS_TRAINING\": IS_TRAINING,\n",
    "            \"ent_coef\": ent_coef,\n",
    "            \"vf_coef\": vf_coef,\n",
    "            \"clip_coef\": clip_coef,\n",
    "            \"gamma\": gamma,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"max_cycles\": max_cycles,\n",
    "            \"total_episodes\": total_episodes,\n",
    "            \"PPO_STEPS\": PPO_STEPS,\n",
    "        }\n",
    ")\n",
    "\"\"\"ALGO PARAMS\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 0.0001\n",
    "\n",
    "\n",
    "\"\"\" ENV SETUP \"\"\"\n",
    "env = CUSTOMENV\n",
    "\n",
    "num_agents = len(env.possible_agents)\n",
    "num_actions = env.action_space(env.possible_agents[0]).n\n",
    "observation_size = env.observation_space(env.possible_agents[0]).shape\n",
    "\n",
    "\"\"\" LEARNER SETUP \"\"\"\n",
    "# Create a list of agents, one for each training agent\n",
    "training_agents = []\n",
    "optimizers = []\n",
    "training_policy_names = [team for team, training in IS_POLICY_TRAINING.items() if training]\n",
    "training_agent_indices = [i for i, training in enumerate(IS_TRAINING) if training]\n",
    "frozen_agent_indices = [i for i, training in enumerate(IS_TRAINING) if not training and INITIALIZATIONS[i] != RANDOM]\n",
    "\n",
    "teams = {\"preds\":[0,1],\"hiders\":[2,3]}\n",
    "\n",
    "pred_policy = Super_Agent(num_actions, num_agents).to(device)\n",
    "pred_optimizer = optim.Adam(pred_policy.parameters(), lr=lr, eps=1e-5)\n",
    "if INITIALIZATIONS[0] != RANDOM:\n",
    "    pred_policy.load_state_dict(torch.load(INITIALIZATIONS[0]))\n",
    "    print(f'loaded from {INITIALIZATIONS[0]}')\n",
    "    \n",
    "hider_policy = Super_Agent(num_actions, num_agents).to(device)\n",
    "hider_optimizer = optim.Adam(hider_policy.parameters(), lr=lr, eps=1e-5)\n",
    "if INITIALIZATIONS[2] != RANDOM:\n",
    "    hider_policy.load_state_dict(torch.load(INITIALIZATIONS[1]))\n",
    "    print(f'loaded from {INITIALIZATIONS[1]}')\n",
    "\n",
    "team_policies = {\"preds\":pred_policy, \"hiders\":hider_policy}\n",
    "team_optimizers = {\"preds\":pred_optimizer, \"hiders\":hider_optimizer}\n",
    "\n",
    "# a frozen is one that is NOT TRAINING and NOT RANDOM\n",
    "for idx in training_agent_indices:\n",
    "    if idx in teams[\"hiders\"]:\n",
    "        agent = hider_policy\n",
    "        optimizer = hider_optimizer\n",
    "    elif idx in teams[\"preds\"]:\n",
    "        agent = pred_policy\n",
    "        optimizer = pred_optimizer\n",
    "\n",
    "    training_agents.append(agent)\n",
    "    optimizers.append(optimizer)\n",
    "\n",
    "frozen_agents = [] # These agents are not random, but are NOT TRAINING ; initialized with a checkpoint\n",
    "for idx, init in enumerate(INITIALIZATIONS):\n",
    "    if init != RANDOM and not IS_TRAINING[idx]:\n",
    "        agent = Super_Agent(num_actions=num_actions, num_agents=num_agents).to(device)\n",
    "        agent.load_state_dict(torch.load(init))\n",
    "        agent.eval()\n",
    "        #freeze weights\n",
    "        for param in agent.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(f' loaded from {init}')\n",
    "        frozen_agents.append(agent)\n",
    "\n",
    "\"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "end_step = 0\n",
    "total_episodic_return = 0\n",
    "rb_obs = torch.zeros((max_cycles, num_agents, NUM_THINGS,GRID_SIZE,GRID_SIZE)).to(device)\n",
    "rb_actions = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_logprobs = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_rewards = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_terms = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_values = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "\n",
    "\"\"\" TRAINING LOGIC \"\"\"\n",
    "# Track returns for all agents\n",
    "all_returns = [[] for _ in range(num_agents)]\n",
    "\n",
    "for episode in range(1,total_episodes+1):\n",
    "    # collect an episode\n",
    "    with torch.no_grad():\n",
    "        # collect observations and convert to batch of torch tensors\n",
    "        next_obs, info = env.reset(seed=None)\n",
    "        # reset the episodic return\n",
    "        total_episodic_return = 0\n",
    "\n",
    "        # each episode has num_steps\n",
    "        for step in range(0, max_cycles):\n",
    "            #modify observation to get self, friends, enemy position layers\n",
    "            obs = reshape_obs(next_obs, env)\n",
    "            \n",
    "            # rollover the observation\n",
    "            obs = batchify_obs(obs, device)\n",
    "\n",
    "            # get action for first agent from the trained agents\n",
    "            # get random actions for other agents\n",
    "            actions = torch.zeros(num_agents, dtype=torch.long).to(device)\n",
    "            logprobs = torch.zeros(num_agents).to(device)\n",
    "            values = torch.zeros(num_agents).to(device)\n",
    "\n",
    "            # Process each agent\n",
    "            for i in range(num_agents):\n",
    "                if IS_TRAINING[i]:\n",
    "                    # Find the index of this training agent among training agents\n",
    "                    train_idx = training_agent_indices.index(i)\n",
    "                    # Get action and value for training agent\n",
    "                    agent_obs = obs[i].unsqueeze(0)\n",
    "                    actions[i], logprobs[i], _, values[i] = training_agents[train_idx].get_action_and_value(agent_obs)\n",
    "                elif INITIALIZATIONS[i] != RANDOM:\n",
    "                    #this is a frozen agent (not training, but not random because it has a checkpoint)\n",
    "                    frozen_idx = frozen_agent_indices.index(i)\n",
    "                    agent_obs = obs[i].unsqueeze(0)\n",
    "                    actions[i], logprobs[i], _, values[i] = frozen_agents[frozen_idx].get_action_and_value(agent_obs)\n",
    "\n",
    "                    logprobs[i] = torch.log(torch.tensor(1.0/num_actions))\n",
    "                    values[i] = 0.0  # No value estimation for frozen agents\n",
    "                else:\n",
    "                    # Random action for random agents\n",
    "                    actions[i] = torch.randint(0, num_actions, (1,)).to(device)\n",
    "                    logprobs[i] = torch.log(torch.tensor(1.0/num_actions))\n",
    "                    values[i] = 0.0  # No value estimation for random agents\n",
    "\n",
    "            # execute the environment and log data\n",
    "            next_obs, rewards, terms, truncs, infos = env.step(\n",
    "                unbatchify(actions, env)\n",
    "            )\n",
    "\n",
    "            # add to episode storage\n",
    "            rb_obs[step] = obs\n",
    "            rb_rewards[step] = batchify(rewards, device)\n",
    "            rb_terms[step] = batchify(terms, device)\n",
    "            rb_actions[step] = actions\n",
    "            rb_logprobs[step] = logprobs\n",
    "            rb_values[step] = values\n",
    "\n",
    "            # compute episodic return\n",
    "            total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "\n",
    "            # if we reach termination or truncation, end\n",
    "            if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
    "                end_step = step\n",
    "                break\n",
    "\n",
    "    # Train only the specified policies\n",
    "    for team_train_idx,team_name in enumerate(training_policy_names):\n",
    "        b_obs = torch.zeros(len(teams[team_name])*end_step, *rb_obs.shape[2:]).to(device)\n",
    "        b_logprobs = torch.zeros(len(teams[team_name])*end_step, *rb_logprobs.shape[2:]).to(device)\n",
    "        b_actions = torch.zeros(len(teams[team_name])*end_step, *rb_actions.shape[2:]).to(device)\n",
    "        b_returns = torch.zeros(len(teams[team_name])*end_step, *rb_rewards.shape[2:]).to(device)\n",
    "        b_values = torch.zeros(len(teams[team_name])*end_step, *rb_values.shape[2:]).to(device)\n",
    "        b_advantages = torch.zeros(len(teams[team_name])*end_step, *rb_rewards.shape[2:]).to(device)\n",
    "        \n",
    "        #Create a big batch containing the data from all the agents in the team\n",
    "        for train_idx, agent_idx in enumerate(teams[team_name]):\n",
    "            # Bootstrap value and advantages only for the training agent\n",
    "            with torch.no_grad():\n",
    "                rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "                for t in reversed(range(end_step)):\n",
    "                    delta = (\n",
    "                        rb_rewards[t, agent_idx]  # Only specific agent's reward\n",
    "                        + gamma * rb_values[t + 1, agent_idx] * rb_terms[t + 1, agent_idx]\n",
    "                        - rb_values[t, agent_idx]\n",
    "                    )\n",
    "                    rb_advantages[t, agent_idx] = delta + gamma * gamma * rb_advantages[t + 1, agent_idx]\n",
    "                rb_returns = rb_advantages + rb_values\n",
    "\n",
    "            # convert our episodes to batch of individual transitions (only for specific agent)\n",
    "            b_obs[train_idx*end_step:(train_idx+1)*end_step] = rb_obs[:end_step, agent_idx]\n",
    "            b_logprobs[train_idx*end_step:(train_idx+1)*end_step] = rb_logprobs[:end_step, agent_idx]\n",
    "            b_actions[train_idx*end_step:(train_idx+1)*end_step] = rb_actions[:end_step, agent_idx]\n",
    "            b_returns[train_idx*end_step:(train_idx+1)*end_step] = rb_returns[:end_step, agent_idx]\n",
    "            b_values[train_idx*end_step:(train_idx+1)*end_step] = rb_values[:end_step, agent_idx]\n",
    "            b_advantages[train_idx*end_step:(train_idx+1)*end_step] = rb_advantages[:end_step, agent_idx]\n",
    "\n",
    "        # Optimizing the policy and value network\n",
    "        b_index = np.arange(len(b_obs))\n",
    "        clip_fracs = []\n",
    "        for repeat in range(PPO_STEPS):\n",
    "            # shuffle the indices we use to access the data\n",
    "            np.random.shuffle(b_index)\n",
    "            for start in range(0, len(b_obs), batch_size):\n",
    "                # select the indices we want to train on\n",
    "                end = start + batch_size\n",
    "                batch_index = b_index[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, value = team_policies[team_name].get_action_and_value(\n",
    "                    b_obs[batch_index], b_actions.long()[batch_index]\n",
    "                )\n",
    "                logratio = newlogprob - b_logprobs[batch_index]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clip_fracs += [\n",
    "                        ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                    ]\n",
    "\n",
    "                # normalize advantages\n",
    "                advantages = b_advantages[batch_index]\n",
    "                advantages = (advantages - advantages.mean()) / (\n",
    "                    advantages.std() + 1e-8\n",
    "                )\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "                pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                    ratio, 1 - clip_coef, 1 + clip_coef\n",
    "                )\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                value = value.flatten()\n",
    "                v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "                v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                    value - b_values[batch_index],\n",
    "                    -clip_coef,\n",
    "                    clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "                team_optimizers[team_name].zero_grad()\n",
    "                loss.backward()\n",
    "                team_optimizers[team_name].step()\n",
    "\n",
    "        # Store returns for the training agents\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # Accumulate returns for all agents\n",
    "    for i in range(num_agents):\n",
    "        all_returns[i].append(total_episodic_return[i])\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Training episode {episode}\")\n",
    "        print(f\"Episodic Return: {(total_episodic_return)}\")\n",
    "\n",
    "        # Print smoothed returns for each agent\n",
    "        for i in range(num_agents):\n",
    "            status = None\n",
    "\n",
    "            if IS_TRAINING[i]:\n",
    "                status = \"Training\"\n",
    "            elif INITIALIZATIONS[i] == RANDOM:\n",
    "                status = \"Random\"\n",
    "            else:\n",
    "                status = \"Frozen\"\n",
    "                \n",
    "            print(f\"Smoothed Returns for agent_{i} ({status}): {np.mean(all_returns[i][-20:])}\")\n",
    "\n",
    "        print(f\"Episode Length: {end_step}\")\n",
    "        print(\"\")\n",
    "        print(f\"Value Loss: {v_loss.item()}\")\n",
    "        print(f\"Policy Loss: {pg_loss.item()}\")\n",
    "        print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
    "        print(f\"Approx KL: {approx_kl.item()}\")\n",
    "        print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "        print(f\"Explained Variance: {explained_var.item()}\")\n",
    "        print(\"\\n-------------------------------------------\\n\")\n",
    "\n",
    "    #log all with wandb\n",
    "    wandb.log({\n",
    "        \"Ep return pred1\": total_episodic_return[0],\n",
    "        \"Ep return pred2\": total_episodic_return[1],\n",
    "        \"Ep return hider1\": total_episodic_return[2],\n",
    "        \"Ep return hider2\": total_episodic_return[3],\n",
    "        \"Episode Length\": end_step,\n",
    "        \"Value Loss\": v_loss.item(),\n",
    "        \"Policy Loss\": pg_loss.item(),\n",
    "        \"Old Approx KL\": old_approx_kl.item(),\n",
    "        \"Approx KL\": approx_kl.item(),\n",
    "        \"Clip Fraction\": np.mean(clip_fracs),\n",
    "        \"Explained Variance\": explained_var.item()\n",
    "    })\n",
    "\n",
    "    #if for pred_1 (index 0) episode return and smoothed are greater than -200, save the model\n",
    "\n",
    "    # if total_episodic_return[0] > -210 and np.mean(all_returns[0][-20:]) > -210:\n",
    "    #     #create dir\n",
    "    #     import os\n",
    "    #     if not os.path.exists('./models'):\n",
    "    #         os.makedirs('./models')f\n",
    "    #     #save just state dict for 0\n",
    "    #     torch.save(agents[0].state_dict(), f'./models/agentwalls_{episode}.ckpt')\n",
    "    #     exit(1)\n",
    "\n",
    "\n",
    "\n",
    "    #if reward greater than 600 for hider_1 both for last and smoothed for last 5\n",
    "    #every 100 epochs save the 2 models\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        if SAVE_PRED_POL:\n",
    "            torch.save(pred_policy.state_dict(), f'./models/PRED_{ckpt_name}_{episode}.ckpt')\n",
    "        if SAVE_HIDER_POL:\n",
    "            torch.save(hider_policy.state_dict(), f'./models/HIDER_{ckpt_name}_{episode}.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9811a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c8ea2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672304be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3410733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL project env",
   "language": "python",
   "name": "drl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
