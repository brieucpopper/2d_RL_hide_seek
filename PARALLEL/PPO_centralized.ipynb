{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31ea5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import parallel\n",
    "import torch\n",
    "import movable_wall_parallel\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "import wandb\n",
    "RANDOM = 42\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c46a3cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using WALLS\n",
      "DONT FORGET TO ADD CODE TO SAVE CHECKPOINTS IF YOU WANT TO DO THAT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############################ HIGHLY IMPORTANT VARIABLES TO SET######################################\n",
    "GRID_SIZE = 7\n",
    "NUM_THINGS = 5 # number of things in the grid wall, pred1, pred2, h1, h2, movablewall\n",
    "\n",
    "\n",
    "INITIALIZATIONS = [\n",
    "    #'./models/PRED_GROUP_POLICY_BOTH_INDIV_RWD_2700.ckpt', # pred_policy\n",
    "    #'./models/PRED_GROUP_POLICY_BOTH_INDIV_RWD_2700.ckpt',    # hider_policy\n",
    "    #'./models/HIDER_GROUP_POLICY_BOTH_INDIV_RWD_1800.ckpt',\n",
    "    #'./models/HIDER_GROUP_POLICY_BOTH_INDIV_RWD_1800.ckpt',\n",
    "    RANDOM,\n",
    "    RANDOM,\n",
    "    RANDOM,\n",
    "    RANDOM,\n",
    "    ]    # hider_2\n",
    "#should be either RANDOM ; or a path to a pretrained checkpoint (a String)\n",
    "\n",
    "IS_POLICY_TRAINING = {\"preds\":True, \"hiders\":True} #preds, hiders\n",
    "\n",
    "IS_TRAINING =   [\n",
    "    IS_POLICY_TRAINING[\"preds\"], #pred1\n",
    "    IS_POLICY_TRAINING[\"preds\"], #pred2\n",
    "    IS_POLICY_TRAINING[\"hiders\"], #hider1\n",
    "    IS_POLICY_TRAINING[\"hiders\"] #hider2\n",
    "]\n",
    "#either True or False, if False, weights are frozen (or if random it will stay random)\n",
    "\n",
    "\n",
    "envname = 'mparallel-walls' #just for wandb logging\n",
    "CUSTOMENV = movable_wall_parallel.parallel_env(grid_size=GRID_SIZE,walls=True)\n",
    "# change architecture if needed\n",
    "\n",
    "ent_coef = 0.4\n",
    "vf_coef = 0.2\n",
    "clip_coef = 0.1\n",
    "gamma = 0.975\n",
    "batch_size = 64\n",
    "max_cycles = 200\n",
    "total_episodes = 4000\n",
    "PPO_STEPS = 3\n",
    "\n",
    "reminder = '''DONT FORGET TO ADD CODE TO SAVE CHECKPOINTS IF YOU WANT TO DO THAT'''\n",
    "ckpt_name= \"GROUP_POLICY_BOTH_INDIVRWD\" #\"GROUP_POLICY_BOTH_INDIV_RWD_ROUND2bis\"\n",
    "SAVE_PRED_POL = True\n",
    "SAVE_HIDER_POL = True\n",
    "print(reminder)\n",
    "\n",
    "##################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5747b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Super_Agent(nn.Module):\n",
    "    #Common agent class for all hiders/seekers\n",
    "    \n",
    "    def __init__(self, num_actions, num_agents):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN architecture inspired by DQN for Atari\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(NUM_THINGS, 32, kernel_size=3, stride=1, padding=1),  # Output: 32 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Output: 64 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Output: 64 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),  # Output: 64 * 7 * 7 = 3136\n",
    "        )\n",
    "        self.actor = self._layer_init(nn.Linear(64*GRID_SIZE**2, num_actions), std=0.01) #TODO depends on GRID_SIZE\n",
    "        self.critic = self._layer_init(nn.Linear(64*GRID_SIZE**2, 1))\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.network(x / 1.0))  # Normalize input to [0, 1]\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x / 1.0)  # Normalize input to [0, 1]\n",
    "        \n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
    "\n",
    "    \n",
    "\n",
    "def batchify_obs(obs, device):\n",
    "    \"\"\"Converts PZ style observations to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    obs = np.stack([obs[a] for a in obs], axis=0)\n",
    "    # convert to torch\n",
    "    obs = torch.tensor(obs).to(device)\n",
    "\n",
    "    return obs\n",
    "\n",
    "\n",
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unbatchify(x, env):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {a: x[i] for i, a in enumerate(env.possible_agents)}\n",
    "\n",
    "    return x\n",
    "\n",
    "def reshape_obs(observations, env):\n",
    "    modified_observations = {}\n",
    "    for self_name, obs in observations.items():\n",
    "        self_layer = env.agent_layers[self_name]\n",
    "        enemy_layers = []\n",
    "        for name, layer_idx in env.agent_layers.items():\n",
    "            \n",
    "            if name == self_name:\n",
    "                self_layer = obs[layer_idx]\n",
    "            elif name.startswith(self_name[:4]): #starts with the same 4 letters: pred or hide\n",
    "                friend_layer = obs[layer_idx]\n",
    "            else:\n",
    "                enemy_layers.append(obs[layer_idx])\n",
    "         \n",
    "        new_obs = [obs[0]] #walls\n",
    "        new_obs.append(self_layer) #self\n",
    "        new_obs.append(friend_layer) #friend\n",
    "        new_obs.append(sum(enemy_layers)) #enemies\n",
    "        new_obs.append(obs[-1]) #movable walls\n",
    "\n",
    "        modified_observations[self_name] = np.stack(new_obs, axis = 0)\n",
    "        \n",
    "    return modified_observations\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ded135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:qpdelqdc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Approx KL</td><td>▂▁▂▆▂▂▄▁▄▂▁▁▂▃▂▂▃▃▄▁▁▁▂▁▃▄▂▁▃▄▁▇▂▃▂█▂▄▄▁</td></tr><tr><td>Clip Fraction</td><td>▂▃▃▄▂▃▃▄▄▁▅▁▂▃▃▁▃▄█▂▃▂▂▂▂▂▃▄▃▄▂▂▃▂▃▂▃▄▃▃</td></tr><tr><td>Ep return hider1</td><td>█▇██▆▇▆▆▄█▇█▅▄▆▅▆▅▄▅▄▃▃▅▄▁▂▄▃▄▄▄▂▃▃▃▅▃▄▃</td></tr><tr><td>Ep return hider2</td><td>▄▅▆▆▇▇█▅▇▅▅██▆▇▇▇▇▅▅▃▃▅▄▅▁▆▂▃▁▂▅▃▄▃▂▂▄▄▂</td></tr><tr><td>Ep return pred1</td><td>▃▂▃▃▃▃▁▂▄▄▂▃▃▂▃▄▃▄▇▄▅▇▆▅▅▅▆▆▆▅▆█▅▇▇██▇▇▆</td></tr><tr><td>Ep return pred2</td><td>▂▄▂▅▃▄▄▅▃▂▁▂▅▂▂▄▅▄▇▄▅▄▆▇▇▆▅█▆▇▇▆█▇▇▇▇▆▆▇</td></tr><tr><td>Episode Length</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Explained Variance</td><td>▆▇▆▆▅█▁▃▆▇▇▆▅▆▆▇▄▅▆▅▅▅▅▅█▆▆▇▆▆▆▅▇▇▅▇█▆▇█</td></tr><tr><td>Old Approx KL</td><td>▃▄▄▄▄▂▂▃▅▅▁▄▃▃▄▁▃▃▄▃▃▃▅▂▆▂▃▅▆▂▁▄▄▃▄▆▅▃█▄</td></tr><tr><td>Policy Loss</td><td>▇▁▃▆▂▇▇▃▆▇▃▅▄▄▄▅▄▃▇▂▇▅▅▃▄▄▅▆▅▄▅▆▅▅▅▄▂▃█▄</td></tr><tr><td>Value Loss</td><td>█▂▁▃▃▂▁▅▇▃▃▁▁▁▁▅▁▆▃▃▂▂▄▃▂▃▃▃▃▂▂▂▃▂▃▂▁▂▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Approx KL</td><td>0.00174</td></tr><tr><td>Clip Fraction</td><td>0.10714</td></tr><tr><td>Ep return hider1</td><td>-3281.87842</td></tr><tr><td>Ep return hider2</td><td>-3813.58203</td></tr><tr><td>Ep return pred1</td><td>3464.45557</td></tr><tr><td>Ep return pred2</td><td>3347.54517</td></tr><tr><td>Episode Length</td><td>199</td></tr><tr><td>Explained Variance</td><td>0.03267</td></tr><tr><td>Old Approx KL</td><td>-0.0214</td></tr><tr><td>Policy Loss</td><td>-33.28627</td></tr><tr><td>Value Loss</td><td>3557.31641</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TEST</strong> at: <a href='https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo/runs/qpdelqdc' target=\"_blank\">https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo/runs/qpdelqdc</a><br/> View project at: <a href='https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo' target=\"_blank\">https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241207_164032-qpdelqdc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:qpdelqdc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hice1/ibaali3/2d_RL_hide_seek/PARALLEL/wandb/run-20241207_165510-k6vr281j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo/runs/k6vr281j' target=\"_blank\">GROUP_POLICY_BOTH_INDIVRWD</a></strong> to <a href='https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo' target=\"_blank\">https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo/runs/k6vr281j' target=\"_blank\">https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo/runs/k6vr281j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 10\n",
      "Episodic Return: [ 1254.8323   867.0374 -1296.9248 -1010.4286]\n",
      "Smoothed Returns for agent_0 (Training): 105.19563293457031\n",
      "Smoothed Returns for agent_1 (Training): 431.97479248046875\n",
      "Smoothed Returns for agent_2 (Training): -303.53289794921875\n",
      "Smoothed Returns for agent_3 (Training): -374.6353454589844\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 16306.029296875\n",
      "Policy Loss: 97.17435455322266\n",
      "Old Approx KL: -0.020823180675506592\n",
      "Approx KL: 0.0020114779472351074\n",
      "Clip Fraction: 0.03178146339598156\n",
      "Explained Variance: 5.1081180572509766e-05\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 20\n",
      "Episodic Return: [ 1633.4447   -177.20023  -385.97424 -1264.9896 ]\n",
      "Smoothed Returns for agent_0 (Training): 156.82447814941406\n",
      "Smoothed Returns for agent_1 (Training): 620.79345703125\n",
      "Smoothed Returns for agent_2 (Training): -472.3787536621094\n",
      "Smoothed Returns for agent_3 (Training): -500.73358154296875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3351.7294921875\n",
      "Policy Loss: 24.105815887451172\n",
      "Old Approx KL: -0.07276691496372223\n",
      "Approx KL: 0.00867057777941227\n",
      "Clip Fraction: 0.06590136176063902\n",
      "Explained Variance: 0.00045806169509887695\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 30\n",
      "Episodic Return: [-103.784035 -228.7135   -253.47647   -74.24246 ]\n",
      "Smoothed Returns for agent_0 (Training): -129.88021850585938\n",
      "Smoothed Returns for agent_1 (Training): 166.7864532470703\n",
      "Smoothed Returns for agent_2 (Training): -49.37586212158203\n",
      "Smoothed Returns for agent_3 (Training): -119.66717529296875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2828.408935546875\n",
      "Policy Loss: 18.801977157592773\n",
      "Old Approx KL: 0.011171030811965466\n",
      "Approx KL: 0.003556443378329277\n",
      "Clip Fraction: 0.033694728499367124\n",
      "Explained Variance: 0.0007457137107849121\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "        project=\"multi-agent-ppo\",  # Set your project name\n",
    "        name=ckpt_name,\n",
    "        config={\n",
    "            \"env\": envname,\n",
    "            \"GRID_SIZE\": GRID_SIZE,\n",
    "            \"NUM_THINGS\": NUM_THINGS,\n",
    "            \"INITIALIZATIONS\": INITIALIZATIONS,\n",
    "            \"IS_TRAINING\": IS_TRAINING,\n",
    "            \"ent_coef\": ent_coef,\n",
    "            \"vf_coef\": vf_coef,\n",
    "            \"clip_coef\": clip_coef,\n",
    "            \"gamma\": gamma,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"max_cycles\": max_cycles,\n",
    "            \"total_episodes\": total_episodes,\n",
    "            \"PPO_STEPS\": PPO_STEPS,\n",
    "        }\n",
    ")\n",
    "\"\"\"ALGO PARAMS\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 0.0001\n",
    "\n",
    "\n",
    "\"\"\" ENV SETUP \"\"\"\n",
    "env = CUSTOMENV\n",
    "\n",
    "num_agents = len(env.possible_agents)\n",
    "num_actions = env.action_space(env.possible_agents[0]).n\n",
    "observation_size = env.observation_space(env.possible_agents[0]).shape\n",
    "\n",
    "\"\"\" LEARNER SETUP \"\"\"\n",
    "# Create a list of agents, one for each training agent\n",
    "training_agents = []\n",
    "optimizers = []\n",
    "training_policy_names = [team for team, training in IS_POLICY_TRAINING.items() if training]\n",
    "training_agent_indices = [i for i, training in enumerate(IS_TRAINING) if training]\n",
    "frozen_agent_indices = [i for i, training in enumerate(IS_TRAINING) if not training and INITIALIZATIONS[i] != RANDOM]\n",
    "\n",
    "teams = {\"preds\":[0,1],\"hiders\":[2,3]}\n",
    "\n",
    "pred_policy = Super_Agent(num_actions, num_agents).to(device)\n",
    "pred_optimizer = optim.Adam(pred_policy.parameters(), lr=lr, eps=1e-5)\n",
    "if INITIALIZATIONS[0] != RANDOM:\n",
    "    pred_policy.load_state_dict(torch.load(INITIALIZATIONS[0]))\n",
    "    print(f'loaded from {INITIALIZATIONS[0]}')\n",
    "    \n",
    "hider_policy = Super_Agent(num_actions, num_agents).to(device)\n",
    "hider_optimizer = optim.Adam(hider_policy.parameters(), lr=lr, eps=1e-5)\n",
    "if INITIALIZATIONS[2] != RANDOM:\n",
    "    hider_policy.load_state_dict(torch.load(INITIALIZATIONS[1]))\n",
    "    print(f'loaded from {INITIALIZATIONS[1]}')\n",
    "\n",
    "team_policies = {\"preds\":pred_policy, \"hiders\":hider_policy}\n",
    "team_optimizers = {\"preds\":pred_optimizer, \"hiders\":hider_optimizer}\n",
    "\n",
    "# a frozen is one that is NOT TRAINING and NOT RANDOM\n",
    "for idx in training_agent_indices:\n",
    "    if idx in teams[\"hiders\"]:\n",
    "        agent = hider_policy\n",
    "        optimizer = hider_optimizer\n",
    "    elif idx in teams[\"preds\"]:\n",
    "        agent = pred_policy\n",
    "        optimizer = pred_optimizer\n",
    "\n",
    "    training_agents.append(agent)\n",
    "    optimizers.append(optimizer)\n",
    "\n",
    "frozen_agents = [] # These agents are not random, but are NOT TRAINING ; initialized with a checkpoint\n",
    "for idx, init in enumerate(INITIALIZATIONS):\n",
    "    if init != RANDOM and not IS_TRAINING[idx]:\n",
    "        agent = Super_Agent(num_actions=num_actions, num_agents=num_agents).to(device)\n",
    "        agent.load_state_dict(torch.load(init))\n",
    "        agent.eval()\n",
    "        #freeze weights\n",
    "        for param in agent.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(f' loaded from {init}')\n",
    "        frozen_agents.append(agent)\n",
    "\n",
    "\"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "end_step = 0\n",
    "total_episodic_return = 0\n",
    "rb_obs = torch.zeros((max_cycles, num_agents, NUM_THINGS,GRID_SIZE,GRID_SIZE)).to(device)\n",
    "rb_actions = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_logprobs = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_rewards = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_terms = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_values = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "\n",
    "\"\"\" TRAINING LOGIC \"\"\"\n",
    "# Track returns for all agents\n",
    "all_returns = [[] for _ in range(num_agents)]\n",
    "\n",
    "for episode in range(1,total_episodes+1):\n",
    "    # collect an episode\n",
    "    with torch.no_grad():\n",
    "        # collect observations and convert to batch of torch tensors\n",
    "        next_obs, info = env.reset(seed=None)\n",
    "        # reset the episodic return\n",
    "        total_episodic_return = 0\n",
    "\n",
    "        # each episode has num_steps\n",
    "        for step in range(0, max_cycles):\n",
    "            #modify observation to get self, friends, enemy position layers\n",
    "            obs = reshape_obs(next_obs, env)\n",
    "            \n",
    "            # rollover the observation\n",
    "            obs = batchify_obs(obs, device)\n",
    "\n",
    "            # get action for first agent from the trained agents\n",
    "            # get random actions for other agents\n",
    "            actions = torch.zeros(num_agents, dtype=torch.long).to(device)\n",
    "            logprobs = torch.zeros(num_agents).to(device)\n",
    "            values = torch.zeros(num_agents).to(device)\n",
    "\n",
    "            # Process each agent\n",
    "            for i in range(num_agents):\n",
    "                if IS_TRAINING[i]:\n",
    "                    # Find the index of this training agent among training agents\n",
    "                    train_idx = training_agent_indices.index(i)\n",
    "                    # Get action and value for training agent\n",
    "                    agent_obs = obs[i].unsqueeze(0)\n",
    "                    actions[i], logprobs[i], _, values[i] = training_agents[train_idx].get_action_and_value(agent_obs)\n",
    "                elif INITIALIZATIONS[i] != RANDOM:\n",
    "                    #this is a frozen agent (not training, but not random because it has a checkpoint)\n",
    "                    frozen_idx = frozen_agent_indices.index(i)\n",
    "                    agent_obs = obs[i].unsqueeze(0)\n",
    "                    actions[i], logprobs[i], _, values[i] = frozen_agents[frozen_idx].get_action_and_value(agent_obs)\n",
    "\n",
    "                    logprobs[i] = torch.log(torch.tensor(1.0/num_actions))\n",
    "                    values[i] = 0.0  # No value estimation for frozen agents\n",
    "                else:\n",
    "                    # Random action for random agents\n",
    "                    actions[i] = torch.randint(0, num_actions, (1,)).to(device)\n",
    "                    logprobs[i] = torch.log(torch.tensor(1.0/num_actions))\n",
    "                    values[i] = 0.0  # No value estimation for random agents\n",
    "\n",
    "            # execute the environment and log data\n",
    "            next_obs, rewards, terms, truncs, infos = env.step(\n",
    "                unbatchify(actions, env)\n",
    "            )\n",
    "\n",
    "            # add to episode storage\n",
    "            rb_obs[step] = obs\n",
    "            rb_rewards[step] = batchify(rewards, device)\n",
    "            rb_terms[step] = batchify(terms, device)\n",
    "            rb_actions[step] = actions\n",
    "            rb_logprobs[step] = logprobs\n",
    "            rb_values[step] = values\n",
    "\n",
    "            # compute episodic return\n",
    "            total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "\n",
    "            # if we reach termination or truncation, end\n",
    "            if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
    "                end_step = step\n",
    "                break\n",
    "\n",
    "    # Train only the specified policies\n",
    "    for team_train_idx,team_name in enumerate(training_policy_names):\n",
    "        b_obs = torch.zeros(len(teams[team_name])*end_step, *rb_obs.shape[2:]).to(device)\n",
    "        b_logprobs = torch.zeros(len(teams[team_name])*end_step, *rb_logprobs.shape[2:]).to(device)\n",
    "        b_actions = torch.zeros(len(teams[team_name])*end_step, *rb_actions.shape[2:]).to(device)\n",
    "        b_returns = torch.zeros(len(teams[team_name])*end_step, *rb_returns.shape[2:]).to(device)\n",
    "        b_values = torch.zeros(len(teams[team_name])*end_step, *rb_values.shape[2:]).to(device)\n",
    "        b_advantages = torch.zeros(len(teams[team_name])*end_step, *rb_advantages.shape[2:]).to(device)\n",
    "        \n",
    "        #Create a big batch containing the data from all the agents in the team\n",
    "        for train_idx, agent_idx in enumerate(teams[team_name]):\n",
    "            # Bootstrap value and advantages only for the training agent\n",
    "            with torch.no_grad():\n",
    "                rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "                for t in reversed(range(end_step)):\n",
    "                    delta = (\n",
    "                        rb_rewards[t, agent_idx]  # Only specific agent's reward\n",
    "                        + gamma * rb_values[t + 1, agent_idx] * rb_terms[t + 1, agent_idx]\n",
    "                        - rb_values[t, agent_idx]\n",
    "                    )\n",
    "                    rb_advantages[t, agent_idx] = delta + gamma * gamma * rb_advantages[t + 1, agent_idx]\n",
    "                rb_returns = rb_advantages + rb_values\n",
    "\n",
    "            # convert our episodes to batch of individual transitions (only for specific agent)\n",
    "            b_obs[train_idx*end_step:(train_idx+1)*end_step] = rb_obs[:end_step, agent_idx]\n",
    "            b_logprobs[train_idx*end_step:(train_idx+1)*end_step] = rb_logprobs[:end_step, agent_idx]\n",
    "            b_actions[train_idx*end_step:(train_idx+1)*end_step] = rb_actions[:end_step, agent_idx]\n",
    "            b_returns[train_idx*end_step:(train_idx+1)*end_step] = rb_returns[:end_step, agent_idx]\n",
    "            b_values[train_idx*end_step:(train_idx+1)*end_step] = rb_values[:end_step, agent_idx]\n",
    "            b_advantages[train_idx*end_step:(train_idx+1)*end_step] = rb_advantages[:end_step, agent_idx]\n",
    "\n",
    "        # Optimizing the policy and value network\n",
    "        b_index = np.arange(len(b_obs))\n",
    "        clip_fracs = []\n",
    "        for repeat in range(PPO_STEPS):\n",
    "            # shuffle the indices we use to access the data\n",
    "            np.random.shuffle(b_index)\n",
    "            for start in range(0, len(b_obs), batch_size):\n",
    "                # select the indices we want to train on\n",
    "                end = start + batch_size\n",
    "                batch_index = b_index[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, value = team_policies[team_name].get_action_and_value(\n",
    "                    b_obs[batch_index], b_actions.long()[batch_index]\n",
    "                )\n",
    "                logratio = newlogprob - b_logprobs[batch_index]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clip_fracs += [\n",
    "                        ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                    ]\n",
    "\n",
    "                # normalize advantages\n",
    "                advantages = b_advantages[batch_index]\n",
    "                advantages = (advantages - advantages.mean()) / (\n",
    "                    advantages.std() + 1e-8\n",
    "                )\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "                pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                    ratio, 1 - clip_coef, 1 + clip_coef\n",
    "                )\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                value = value.flatten()\n",
    "                v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "                v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                    value - b_values[batch_index],\n",
    "                    -clip_coef,\n",
    "                    clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "                team_optimizers[team_name].zero_grad()\n",
    "                loss.backward()\n",
    "                team_optimizers[team_name].step()\n",
    "\n",
    "        # Store returns for the training agents\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # Accumulate returns for all agents\n",
    "    for i in range(num_agents):\n",
    "        all_returns[i].append(total_episodic_return[i])\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Training episode {episode}\")\n",
    "        print(f\"Episodic Return: {(total_episodic_return)}\")\n",
    "\n",
    "        # Print smoothed returns for each agent\n",
    "        for i in range(num_agents):\n",
    "            status = None\n",
    "\n",
    "            if IS_TRAINING[i]:\n",
    "                status = \"Training\"\n",
    "            elif INITIALIZATIONS[i] == RANDOM:\n",
    "                status = \"Random\"\n",
    "            else:\n",
    "                status = \"Frozen\"\n",
    "                \n",
    "            print(f\"Smoothed Returns for agent_{i} ({status}): {np.mean(all_returns[i][-20:])}\")\n",
    "\n",
    "        print(f\"Episode Length: {end_step}\")\n",
    "        print(\"\")\n",
    "        print(f\"Value Loss: {v_loss.item()}\")\n",
    "        print(f\"Policy Loss: {pg_loss.item()}\")\n",
    "        print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
    "        print(f\"Approx KL: {approx_kl.item()}\")\n",
    "        print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "        print(f\"Explained Variance: {explained_var.item()}\")\n",
    "        print(\"\\n-------------------------------------------\\n\")\n",
    "\n",
    "    #log all with wandb\n",
    "    wandb.log({\n",
    "        \"Ep return pred1\": total_episodic_return[0],\n",
    "        \"Ep return pred2\": total_episodic_return[1],\n",
    "        \"Ep return hider1\": total_episodic_return[2],\n",
    "        \"Ep return hider2\": total_episodic_return[3],\n",
    "        \"Episode Length\": end_step,\n",
    "        \"Value Loss\": v_loss.item(),\n",
    "        \"Policy Loss\": pg_loss.item(),\n",
    "        \"Old Approx KL\": old_approx_kl.item(),\n",
    "        \"Approx KL\": approx_kl.item(),\n",
    "        \"Clip Fraction\": np.mean(clip_fracs),\n",
    "        \"Explained Variance\": explained_var.item()\n",
    "    })\n",
    "\n",
    "    #if for pred_1 (index 0) episode return and smoothed are greater than -200, save the model\n",
    "\n",
    "    # if total_episodic_return[0] > -210 and np.mean(all_returns[0][-20:]) > -210:\n",
    "    #     #create dir\n",
    "    #     import os\n",
    "    #     if not os.path.exists('./models'):\n",
    "    #         os.makedirs('./models')f\n",
    "    #     #save just state dict for 0\n",
    "    #     torch.save(agents[0].state_dict(), f'./models/agentwalls_{episode}.ckpt')\n",
    "    #     exit(1)\n",
    "\n",
    "\n",
    "\n",
    "    #if reward greater than 600 for hider_1 both for last and smoothed for last 5\n",
    "    #every 100 epochs save the 2 models\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        if SAVE_PRED_POL:\n",
    "            torch.save(pred_policy.state_dict(), f'./models/PRED_{ckpt_name}_{episode}.ckpt')\n",
    "        if SAVE_HIDER_POL:\n",
    "            torch.save(hider_policy.state_dict(), f'./models/HIDER_{ckpt_name}_{episode}.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9811a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c8ea2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672304be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3410733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL project env",
   "language": "python",
   "name": "drl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
