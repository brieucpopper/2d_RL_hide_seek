{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31ea5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import parallel\n",
    "import torch\n",
    "import movable_wall_parallel\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "import wandb\n",
    "RANDOM = 42\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c46a3cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using WALLS\n",
      "DONT FORGET TO ADD CODE TO SAVE CHECKPOINTS IF YOU WANT TO DO THAT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############################ HIGHLY IMPORTANT VARIABLES TO SET######################################\n",
    "GRID_SIZE = 7\n",
    "NUM_THINGS = 5 # number of things in the grid wall, pred1, pred2, h1, h2, movablewall\n",
    "\n",
    "\n",
    "INITIALIZATIONS = [\n",
    "    './models/PRED_GROUP_POLICY_BOTH_INDIV_RWD_2700.ckpt', # pred_policy\n",
    "    './models/PRED_GROUP_POLICY_BOTH_INDIV_RWD_2700.ckpt',    # hider_policy\n",
    "    './models/HIDER_GROUP_POLICY_BOTH_INDIV_RWD_1800.ckpt',\n",
    "    './models/HIDER_GROUP_POLICY_BOTH_INDIV_RWD_1800.ckpt',\n",
    "    ]    # hider_2\n",
    "#should be either RANDOM ; or a path to a pretrained checkpoint (a String)\n",
    "\n",
    "\n",
    "\n",
    "IS_TRAINING =   [\n",
    "    True, #pred1\n",
    "    True, #pred2\n",
    "    False, #hider1\n",
    "    False #hider2\n",
    "]\n",
    "#either True or False, if False, weights are frozen (or if random it will stay random)\n",
    "\n",
    "\n",
    "envname = 'mparallel-walls' #just for wandb logging\n",
    "CUSTOMENV = movable_wall_parallel.parallel_env(grid_size=GRID_SIZE,walls=True)\n",
    "# change architecture if needed\n",
    "\n",
    "ent_coef = 0.4\n",
    "vf_coef = 0.2\n",
    "clip_coef = 0.1\n",
    "gamma = 0.975\n",
    "batch_size = 64\n",
    "max_cycles = 200\n",
    "total_episodes = 2000\n",
    "PPO_STEPS = 3\n",
    "\n",
    "reminder = '''DONT FORGET TO ADD CODE TO SAVE CHECKPOINTS IF YOU WANT TO DO THAT'''\n",
    "ckpt_name= \"GROUP_POLICY_BOTH_INDIV_RWD_ROUND2\"\n",
    "SAVE_PRED_POL = True\n",
    "SAVE_HIDER_POL = False\n",
    "print(reminder)\n",
    "\n",
    "##################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5747b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Super_Agent(nn.Module):\n",
    "    #Common agent class for all hiders/seekers\n",
    "    \n",
    "    def __init__(self, num_actions, num_agents):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN architecture inspired by DQN for Atari\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(NUM_THINGS, 32, kernel_size=3, stride=1, padding=1),  # Output: 32 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Output: 64 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Output: 64 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),  # Output: 64 * 7 * 7 = 3136\n",
    "        )\n",
    "        self.actor = self._layer_init(nn.Linear(64*GRID_SIZE**2, num_actions), std=0.01) #TODO depends on GRID_SIZE\n",
    "        self.critic = self._layer_init(nn.Linear(64*GRID_SIZE**2, 1))\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.network(x / 1.0))  # Normalize input to [0, 1]\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x / 1.0)  # Normalize input to [0, 1]\n",
    "        \n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
    "\n",
    "    \n",
    "\n",
    "def batchify_obs(obs, device):\n",
    "    \"\"\"Converts PZ style observations to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    obs = np.stack([obs[a] for a in obs], axis=0)\n",
    "    # convert to torch\n",
    "    obs = torch.tensor(obs).to(device)\n",
    "\n",
    "    return obs\n",
    "\n",
    "\n",
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unbatchify(x, env):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {a: x[i] for i, a in enumerate(env.possible_agents)}\n",
    "\n",
    "    return x\n",
    "\n",
    "def reshape_obs(observations, env):\n",
    "    modified_observations = {}\n",
    "    for self_name, obs in observations.items():\n",
    "        self_layer = env.agent_layers[self_name]\n",
    "        enemy_layers = []\n",
    "        for name, layer_idx in env.agent_layers.items():\n",
    "            \n",
    "            if name == self_name:\n",
    "                self_layer = obs[layer_idx]\n",
    "            elif name.startswith(self_name[:4]): #starts with the same 4 letters: pred or hide\n",
    "                friend_layer = obs[layer_idx]\n",
    "            else:\n",
    "                enemy_layers.append(obs[layer_idx])\n",
    "         \n",
    "        new_obs = [obs[0]] #walls\n",
    "        new_obs.append(self_layer) #self\n",
    "        new_obs.append(friend_layer) #friend\n",
    "        new_obs.append(sum(enemy_layers)) #enemies\n",
    "        new_obs.append(obs[-1]) #movable walls\n",
    "\n",
    "        modified_observations[self_name] = np.stack(new_obs, axis = 0)\n",
    "        \n",
    "    return modified_observations\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99ded135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:fkofk13t) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Approx KL</td><td>▂▂▁▁▁▂▁▁▂█▁▂▃▁▂▁▁▁▂▁▃▁▂▃▄▂▁▂▂▁▁▆▁▁▁▁▁▁▅▂</td></tr><tr><td>Clip Fraction</td><td>▆▂█▃▂█▅▁▄▃▅▂▄▅▄▃▁▄▄▂▅▃▃▄▅▃▂▂▅▂▄▆▅▃▆▄▃▃▃▁</td></tr><tr><td>Ep return hider1</td><td>▇▇▇██▆█▆▆▇▅▇▅█▆▄▅▇▆▆▆▆▃▄▅▇▄▂▄▃▄▃▂▄▃▂▁▂▂▂</td></tr><tr><td>Ep return hider2</td><td>▆█▆▆▅█▅▆▇▅▆██▅▅▂█▅▆▅▇▇█▅▄▆▄▅▇▂█▄▅▅▆▃▂█▁▃</td></tr><tr><td>Ep return pred1</td><td>▂▁▂▁▃▂▂▂▂▃▂▂▂▂▁▂▂▂▂▁▃▂▂▂▂█▃▃▂▃▃▃▃▅▃▄▅▄▄▄</td></tr><tr><td>Ep return pred2</td><td>▃▃▂▃▃▃▄▃▃▂▃▂▃▄▂▃▃▄▃▂▄▅▅▃▄▄▅▆▅▅▅▅█▁▇█▆▆█▃</td></tr><tr><td>Episode Length</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Explained Variance</td><td>▅▅▅▅▅▆▅▅▅▆▆▆▆▆▆▆▆▅▆█▆▇▇▆▆▇▇▇▃▇▅▅▃▇▁▄▅▇▅▆</td></tr><tr><td>Old Approx KL</td><td>▄▆▅▃▃▅▂▅▃▂▅▂▄▄▃▄▃▅▄▄▅▄▄▄▄▃▄█▂▃▃▁▂▄▃▄▄▃▅▄</td></tr><tr><td>Policy Loss</td><td>█▆▂▄▆▄▄▅▆▇▅▆▅▅▅▄▆▆▅▅▇▇▁▃▅▅▆▆▄▆▃▅▆▄▇▅▅▄▄▅</td></tr><tr><td>Value Loss</td><td>▁▂▂▁▄▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁█▁▁▂▁▁▁▁▁▂▁▁▁▁▁▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Approx KL</td><td>0.00012</td></tr><tr><td>Clip Fraction</td><td>0.0638</td></tr><tr><td>Ep return hider1</td><td>446.29742</td></tr><tr><td>Ep return hider2</td><td>-736.66669</td></tr><tr><td>Ep return pred1</td><td>144.72395</td></tr><tr><td>Ep return pred2</td><td>-213.58389</td></tr><tr><td>Episode Length</td><td>199</td></tr><tr><td>Explained Variance</td><td>0.05426</td></tr><tr><td>Old Approx KL</td><td>0.01161</td></tr><tr><td>Policy Loss</td><td>1.13616</td></tr><tr><td>Value Loss</td><td>776.74933</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worldly-wood-53</strong> at: <a href='https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo/runs/fkofk13t' target=\"_blank\">https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo/runs/fkofk13t</a><br/> View project at: <a href='https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo' target=\"_blank\">https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241206_194944-fkofk13t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:fkofk13t). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hice1/ibaali3/2d_RL_hide_seek/PARALLEL/wandb/run-20241206_195359-6miaoqtq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo/runs/6miaoqtq' target=\"_blank\">GROUP_POLICY_BOTH_INDIV_RWD_ROUND2</a></strong> to <a href='https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo' target=\"_blank\">https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo/runs/6miaoqtq' target=\"_blank\">https://wandb.ai/ilias-baali-georgia-institute-of-technology/multi-agent-ppo/runs/6miaoqtq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/1025256/ipykernel_541344/2609091301.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent.load_state_dict(torch.load(INITIALIZATIONS[0]))\n",
      "/scratch/1025256/ipykernel_541344/2609091301.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent.load_state_dict(torch.load(INITIALIZATIONS[1]))\n",
      "/scratch/1025256/ipykernel_541344/2609091301.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent.load_state_dict(torch.load(init))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from ./models/PRED_GROUP_POLICY_BOTH_INDIV_RWD_2700.ckpt\n",
      "loaded from ./models/PRED_GROUP_POLICY_BOTH_INDIV_RWD_2700.ckpt\n",
      " loaded from ./models/HIDER_GROUP_POLICY_BOTH_INDIV_RWD_1800.ckpt\n",
      " loaded from ./models/HIDER_GROUP_POLICY_BOTH_INDIV_RWD_1800.ckpt\n",
      "Training episode 10\n",
      "Episodic Return: [-321.58313 -795.39465  439.53745  310.74405]\n",
      "Smoothed Returns for agent_0 (Training): -607.5576782226562\n",
      "Smoothed Returns for agent_1 (Training): -575.2372436523438\n",
      "Smoothed Returns for agent_2 (Frozen): 412.5675354003906\n",
      "Smoothed Returns for agent_3 (Frozen): 399.1104736328125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3048.09423828125\n",
      "Policy Loss: 17.68434715270996\n",
      "Old Approx KL: 0.018755769357085228\n",
      "Approx KL: 0.002177843125537038\n",
      "Clip Fraction: 0.5522693457702795\n",
      "Explained Variance: 0.0006704926490783691\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 20\n",
      "Episodic Return: [ 153.04854  111.45593 -584.3666  -157.32454]\n",
      "Smoothed Returns for agent_0 (Training): -280.62139892578125\n",
      "Smoothed Returns for agent_1 (Training): -308.62860107421875\n",
      "Smoothed Returns for agent_2 (Frozen): 127.8983383178711\n",
      "Smoothed Returns for agent_3 (Frozen): 108.03038024902344\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 4120.62109375\n",
      "Policy Loss: -27.27367401123047\n",
      "Old Approx KL: -0.04983973875641823\n",
      "Approx KL: 0.003764535766094923\n",
      "Clip Fraction: 0.1934523843228817\n",
      "Explained Variance: 0.0009118914604187012\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 30\n",
      "Episodic Return: [ 482.3917   -548.2225     16.212772 -396.43152 ]\n",
      "Smoothed Returns for agent_0 (Training): 89.8209457397461\n",
      "Smoothed Returns for agent_1 (Training): 11.95520305633545\n",
      "Smoothed Returns for agent_2 (Frozen): -239.0220947265625\n",
      "Smoothed Returns for agent_3 (Frozen): -160.6991424560547\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2644.25634765625\n",
      "Policy Loss: 39.43809509277344\n",
      "Old Approx KL: 0.0671607255935669\n",
      "Approx KL: 0.0064748781733214855\n",
      "Clip Fraction: 0.242745541036129\n",
      "Explained Variance: 0.002983272075653076\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 40\n",
      "Episodic Return: [-257.81967  511.6889  -534.31445 -132.1338 ]\n",
      "Smoothed Returns for agent_0 (Training): 14.327610969543457\n",
      "Smoothed Returns for agent_1 (Training): -71.88790130615234\n",
      "Smoothed Returns for agent_2 (Frozen): -138.27001953125\n",
      "Smoothed Returns for agent_3 (Frozen): -18.794071197509766\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 6381.7001953125\n",
      "Policy Loss: -48.655433654785156\n",
      "Old Approx KL: -0.002202579053118825\n",
      "Approx KL: 0.0003815378586295992\n",
      "Clip Fraction: 0.013020833333333334\n",
      "Explained Variance: 0.003604412078857422\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 50\n",
      "Episodic Return: [  529.9977     509.38275    -91.454315 -1220.0449  ]\n",
      "Smoothed Returns for agent_0 (Training): -18.947460174560547\n",
      "Smoothed Returns for agent_1 (Training): 38.49689865112305\n",
      "Smoothed Returns for agent_2 (Frozen): 10.551019668579102\n",
      "Smoothed Returns for agent_3 (Frozen): -207.62265014648438\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 779.5772705078125\n",
      "Policy Loss: -17.806026458740234\n",
      "Old Approx KL: -0.02773812972009182\n",
      "Approx KL: 0.005337809212505817\n",
      "Clip Fraction: 0.4460565522313118\n",
      "Explained Variance: 0.00485837459564209\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 60\n",
      "Episodic Return: [ 211.20901  298.63013 -621.10565  275.0937 ]\n",
      "Smoothed Returns for agent_0 (Training): 181.04177856445312\n",
      "Smoothed Returns for agent_1 (Training): 365.78814697265625\n",
      "Smoothed Returns for agent_2 (Frozen): -257.3343811035156\n",
      "Smoothed Returns for agent_3 (Frozen): -413.93829345703125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1749.1199951171875\n",
      "Policy Loss: 8.079765319824219\n",
      "Old Approx KL: 0.0828804224729538\n",
      "Approx KL: 0.010743430815637112\n",
      "Clip Fraction: 0.043340774873892464\n",
      "Explained Variance: 0.001977980136871338\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 70\n",
      "Episodic Return: [-235.59998  -85.80488  906.4847  -226.30557]\n",
      "Smoothed Returns for agent_0 (Training): 192.53311157226562\n",
      "Smoothed Returns for agent_1 (Training): 354.771240234375\n",
      "Smoothed Returns for agent_2 (Frozen): -301.88433837890625\n",
      "Smoothed Returns for agent_3 (Frozen): -253.62893676757812\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1795.445556640625\n",
      "Policy Loss: 27.3638973236084\n",
      "Old Approx KL: 0.02257607690989971\n",
      "Approx KL: 0.0037579964846372604\n",
      "Clip Fraction: 0.2650669664144516\n",
      "Explained Variance: 0.0019850730895996094\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 80\n",
      "Episodic Return: [ 283.39102  263.276    136.80624 -778.3385 ]\n",
      "Smoothed Returns for agent_0 (Training): 281.1975402832031\n",
      "Smoothed Returns for agent_1 (Training): 224.7803192138672\n",
      "Smoothed Returns for agent_2 (Frozen): -159.42764282226562\n",
      "Smoothed Returns for agent_3 (Frozen): -389.32525634765625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 70.92816162109375\n",
      "Policy Loss: -3.9618210792541504\n",
      "Old Approx KL: -0.05752841383218765\n",
      "Approx KL: 0.0037044715136289597\n",
      "Clip Fraction: 0.24665178855260214\n",
      "Explained Variance: 0.015938639640808105\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 90\n",
      "Episodic Return: [-107.14162   478.96152   -21.024515 -262.76706 ]\n",
      "Smoothed Returns for agent_0 (Training): 411.18243408203125\n",
      "Smoothed Returns for agent_1 (Training): 388.4832763671875\n",
      "Smoothed Returns for agent_2 (Frozen): -203.28082275390625\n",
      "Smoothed Returns for agent_3 (Frozen): -689.4378662109375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2058.365234375\n",
      "Policy Loss: 6.092698097229004\n",
      "Old Approx KL: 0.011286753229796886\n",
      "Approx KL: 0.0005181857850402594\n",
      "Clip Fraction: 0.04296875\n",
      "Explained Variance: -0.0016202926635742188\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 100\n",
      "Episodic Return: [ 683.79407  838.0121  -653.3973  -989.364  ]\n",
      "Smoothed Returns for agent_0 (Training): 391.7217712402344\n",
      "Smoothed Returns for agent_1 (Training): 498.25341796875\n",
      "Smoothed Returns for agent_2 (Frozen): -313.19500732421875\n",
      "Smoothed Returns for agent_3 (Frozen): -522.7110595703125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 4496.88720703125\n",
      "Policy Loss: -8.586282730102539\n",
      "Old Approx KL: 0.020187344402074814\n",
      "Approx KL: 0.002542257308959961\n",
      "Clip Fraction: 0.1763392873108387\n",
      "Explained Variance: 0.0006194114685058594\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 110\n",
      "Episodic Return: [  396.28766  1836.7676  -2034.7452   -356.37952]\n",
      "Smoothed Returns for agent_0 (Training): 420.10418701171875\n",
      "Smoothed Returns for agent_1 (Training): 481.97412109375\n",
      "Smoothed Returns for agent_2 (Frozen): -485.81524658203125\n",
      "Smoothed Returns for agent_3 (Frozen): -287.6299743652344\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 11377.05078125\n",
      "Policy Loss: -92.68671417236328\n",
      "Old Approx KL: -0.03777759522199631\n",
      "Approx KL: 0.0019193036714568734\n",
      "Clip Fraction: 0.15699404974778494\n",
      "Explained Variance: 0.023424208164215088\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 120\n",
      "Episodic Return: [  867.61      846.94916   800.52045 -2160.549  ]\n",
      "Smoothed Returns for agent_0 (Training): 449.604736328125\n",
      "Smoothed Returns for agent_1 (Training): 581.3468017578125\n",
      "Smoothed Returns for agent_2 (Frozen): -491.504150390625\n",
      "Smoothed Returns for agent_3 (Frozen): -410.89544677734375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 915.380126953125\n",
      "Policy Loss: -32.78788375854492\n",
      "Old Approx KL: 0.0545712411403656\n",
      "Approx KL: 0.020343372598290443\n",
      "Clip Fraction: 0.3623511965076129\n",
      "Explained Variance: 0.05018693208694458\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 130\n",
      "Episodic Return: [  645.12933   751.35516 -1774.6691    682.79144]\n",
      "Smoothed Returns for agent_0 (Training): 447.5172424316406\n",
      "Smoothed Returns for agent_1 (Training): 539.332275390625\n",
      "Smoothed Returns for agent_2 (Frozen): -283.66619873046875\n",
      "Smoothed Returns for agent_3 (Frozen): -528.32373046875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1039.4442138671875\n",
      "Policy Loss: -34.67146301269531\n",
      "Old Approx KL: -0.027118327096104622\n",
      "Approx KL: 0.00142059160862118\n",
      "Clip Fraction: 0.1685267873108387\n",
      "Explained Variance: 0.006054937839508057\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 140\n",
      "Episodic Return: [  319.24063   707.67163  -100.41686 -1176.5514 ]\n",
      "Smoothed Returns for agent_0 (Training): 331.74969482421875\n",
      "Smoothed Returns for agent_1 (Training): 444.00811767578125\n",
      "Smoothed Returns for agent_2 (Frozen): -307.6831970214844\n",
      "Smoothed Returns for agent_3 (Frozen): -295.870361328125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 650.6908569335938\n",
      "Policy Loss: 0.9221835136413574\n",
      "Old Approx KL: 0.03217741474509239\n",
      "Approx KL: 0.0006864326423965394\n",
      "Clip Fraction: 0.0809151791036129\n",
      "Explained Variance: 0.006201624870300293\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 150\n",
      "Episodic Return: [-752.7502  -742.9088   832.32874  589.23724]\n",
      "Smoothed Returns for agent_0 (Training): 306.48834228515625\n",
      "Smoothed Returns for agent_1 (Training): 617.4381713867188\n",
      "Smoothed Returns for agent_2 (Frozen): -592.8497924804688\n",
      "Smoothed Returns for agent_3 (Frozen): -212.4578094482422\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 558.926025390625\n",
      "Policy Loss: 27.576251983642578\n",
      "Old Approx KL: -0.023229174315929413\n",
      "Approx KL: 0.0017623135354369879\n",
      "Clip Fraction: 0.09393601243694623\n",
      "Explained Variance: -0.11678802967071533\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 160\n",
      "Episodic Return: [  682.8243    705.02795   768.54614 -1753.0159 ]\n",
      "Smoothed Returns for agent_0 (Training): 464.863525390625\n",
      "Smoothed Returns for agent_1 (Training): 814.304931640625\n",
      "Smoothed Returns for agent_2 (Frozen): -665.8410034179688\n",
      "Smoothed Returns for agent_3 (Frozen): -466.177978515625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1802.0709228515625\n",
      "Policy Loss: -19.52788734436035\n",
      "Old Approx KL: -0.03722584247589111\n",
      "Approx KL: 0.0015291486633941531\n",
      "Clip Fraction: 0.24795387064417204\n",
      "Explained Variance: 0.042470574378967285\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 170\n",
      "Episodic Return: [  859.2328   926.2884 -2010.3699   566.5642]\n",
      "Smoothed Returns for agent_0 (Training): 561.5484619140625\n",
      "Smoothed Returns for agent_1 (Training): 781.1889038085938\n",
      "Smoothed Returns for agent_2 (Frozen): -701.9307250976562\n",
      "Smoothed Returns for agent_3 (Frozen): -444.42059326171875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 893.78515625\n",
      "Policy Loss: 21.74457359313965\n",
      "Old Approx KL: 0.0004567078431136906\n",
      "Approx KL: 0.00047464031376875937\n",
      "Clip Fraction: 0.049479166666666664\n",
      "Explained Variance: 0.03339415788650513\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 180\n",
      "Episodic Return: [ 1354.13     1172.5616  -2113.2156   -162.24977]\n",
      "Smoothed Returns for agent_0 (Training): 766.6397705078125\n",
      "Smoothed Returns for agent_1 (Training): 905.8428955078125\n",
      "Smoothed Returns for agent_2 (Frozen): -1079.315185546875\n",
      "Smoothed Returns for agent_3 (Frozen): -479.8839416503906\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 4055.41259765625\n",
      "Policy Loss: -33.57488250732422\n",
      "Old Approx KL: 0.0016038758913055062\n",
      "Approx KL: 0.0010496292961761355\n",
      "Clip Fraction: 0.2168898843228817\n",
      "Explained Variance: -0.00337374210357666\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 190\n",
      "Episodic Return: [ 1437.5537  2488.657  -1172.5461 -2991.991 ]\n",
      "Smoothed Returns for agent_0 (Training): 1143.790283203125\n",
      "Smoothed Returns for agent_1 (Training): 1290.112060546875\n",
      "Smoothed Returns for agent_2 (Frozen): -1645.2415771484375\n",
      "Smoothed Returns for agent_3 (Frozen): -819.4869995117188\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2605.151123046875\n",
      "Policy Loss: -56.08953094482422\n",
      "Old Approx KL: -0.014709064736962318\n",
      "Approx KL: 0.0006312898476608098\n",
      "Clip Fraction: 0.16871280098954836\n",
      "Explained Variance: 0.052004337310791016\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 200\n",
      "Episodic Return: [ 1090.1089  2668.5295 -3550.2769  -662.7581]\n",
      "Smoothed Returns for agent_0 (Training): 1260.637939453125\n",
      "Smoothed Returns for agent_1 (Training): 1653.8499755859375\n",
      "Smoothed Returns for agent_2 (Frozen): -1681.695556640625\n",
      "Smoothed Returns for agent_3 (Frozen): -1453.6011962890625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2383.274658203125\n",
      "Policy Loss: -29.687170028686523\n",
      "Old Approx KL: 0.1921306997537613\n",
      "Approx KL: 0.04157797247171402\n",
      "Clip Fraction: 0.2431175634264946\n",
      "Explained Variance: 0.03458493947982788\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 210\n",
      "Episodic Return: [ 1327.5914   1543.9783  -2873.8247   -226.12103]\n",
      "Smoothed Returns for agent_0 (Training): 1400.10546875\n",
      "Smoothed Returns for agent_1 (Training): 1877.5345458984375\n",
      "Smoothed Returns for agent_2 (Frozen): -1712.775146484375\n",
      "Smoothed Returns for agent_3 (Frozen): -1791.947021484375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 4805.88427734375\n",
      "Policy Loss: 58.6588249206543\n",
      "Old Approx KL: 0.02097286656498909\n",
      "Approx KL: 0.03527291491627693\n",
      "Clip Fraction: 0.350818461428086\n",
      "Explained Variance: 0.047560274600982666\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 220\n",
      "Episodic Return: [ 2145.281   2148.297  -1666.3501 -2786.725 ]\n",
      "Smoothed Returns for agent_0 (Training): 1693.62109375\n",
      "Smoothed Returns for agent_1 (Training): 2123.5859375\n",
      "Smoothed Returns for agent_2 (Frozen): -2318.70166015625\n",
      "Smoothed Returns for agent_3 (Frozen): -1696.625732421875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 180.05242919921875\n",
      "Policy Loss: 1.1785510778427124\n",
      "Old Approx KL: -0.019877076148986816\n",
      "Approx KL: 0.007616749964654446\n",
      "Clip Fraction: 0.2025669664144516\n",
      "Explained Variance: 0.011332571506500244\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 230\n",
      "Episodic Return: [ 3069.388   2430.6501 -4757.3267  -713.4451]\n",
      "Smoothed Returns for agent_0 (Training): 1923.7076416015625\n",
      "Smoothed Returns for agent_1 (Training): 2396.94921875\n",
      "Smoothed Returns for agent_2 (Frozen): -2824.683837890625\n",
      "Smoothed Returns for agent_3 (Frozen): -1655.173828125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1195.7625732421875\n",
      "Policy Loss: -25.134723663330078\n",
      "Old Approx KL: 0.04247709736227989\n",
      "Approx KL: 0.0012983083724975586\n",
      "Clip Fraction: 0.1484375\n",
      "Explained Variance: 0.1398562788963318\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 240\n",
      "Episodic Return: [  879.5955  3987.3787 -4801.096   -343.9534]\n",
      "Smoothed Returns for agent_0 (Training): 2010.681640625\n",
      "Smoothed Returns for agent_1 (Training): 2597.448486328125\n",
      "Smoothed Returns for agent_2 (Frozen): -2398.410400390625\n",
      "Smoothed Returns for agent_3 (Frozen): -2356.22900390625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 17158.94921875\n",
      "Policy Loss: -135.55027770996094\n",
      "Old Approx KL: -0.07679077982902527\n",
      "Approx KL: 0.009881812147796154\n",
      "Clip Fraction: 0.24962797885139784\n",
      "Explained Variance: 0.05422329902648926\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 250\n",
      "Episodic Return: [ 3104.7495  3115.0623 -4356.1836 -1917.5645]\n",
      "Smoothed Returns for agent_0 (Training): 2458.125732421875\n",
      "Smoothed Returns for agent_1 (Training): 2802.445068359375\n",
      "Smoothed Returns for agent_2 (Frozen): -2483.247314453125\n",
      "Smoothed Returns for agent_3 (Frozen): -3085.04052734375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3186.7919921875\n",
      "Policy Loss: -17.561634063720703\n",
      "Old Approx KL: 0.005883966572582722\n",
      "Approx KL: 0.0011795504251495004\n",
      "Clip Fraction: 0.15029762188593546\n",
      "Explained Variance: -0.003449082374572754\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 260\n",
      "Episodic Return: [ 3808.954   3138.1309 -2116.2646 -4634.968 ]\n",
      "Smoothed Returns for agent_0 (Training): 2891.332763671875\n",
      "Smoothed Returns for agent_1 (Training): 2756.770751953125\n",
      "Smoothed Returns for agent_2 (Frozen): -2857.868896484375\n",
      "Smoothed Returns for agent_3 (Frozen): -2922.462646484375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1452.827880859375\n",
      "Policy Loss: -27.662813186645508\n",
      "Old Approx KL: 0.04876990243792534\n",
      "Approx KL: 0.008910655975341797\n",
      "Clip Fraction: 0.2135416716337204\n",
      "Explained Variance: 0.033953070640563965\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 270\n",
      "Episodic Return: [ 2070.866   2910.283  -4108.3135 -1397.3024]\n",
      "Smoothed Returns for agent_0 (Training): 2659.14306640625\n",
      "Smoothed Returns for agent_1 (Training): 2781.771240234375\n",
      "Smoothed Returns for agent_2 (Frozen): -2451.880126953125\n",
      "Smoothed Returns for agent_3 (Frozen): -2815.393798828125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2656.8203125\n",
      "Policy Loss: -46.92361068725586\n",
      "Old Approx KL: -0.05604042485356331\n",
      "Approx KL: 0.01233862154185772\n",
      "Clip Fraction: 0.2812500049670537\n",
      "Explained Variance: 0.10499882698059082\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 280\n",
      "Episodic Return: [  481.37903  2265.658   -2030.112    -899.90894]\n",
      "Smoothed Returns for agent_0 (Training): 2167.40478515625\n",
      "Smoothed Returns for agent_1 (Training): 3263.61865234375\n",
      "Smoothed Returns for agent_2 (Frozen): -2881.235107421875\n",
      "Smoothed Returns for agent_3 (Frozen): -2482.75634765625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 6049.232421875\n",
      "Policy Loss: 57.433868408203125\n",
      "Old Approx KL: 0.05357293039560318\n",
      "Approx KL: 0.007518342696130276\n",
      "Clip Fraction: 0.4207589315871398\n",
      "Explained Variance: 0.0019374489784240723\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 290\n",
      "Episodic Return: [  938.75836  4826.305   -2127.785   -4530.53   ]\n",
      "Smoothed Returns for agent_0 (Training): 2142.53369140625\n",
      "Smoothed Returns for agent_1 (Training): 3923.475341796875\n",
      "Smoothed Returns for agent_2 (Frozen): -3698.028564453125\n",
      "Smoothed Returns for agent_3 (Frozen): -2722.1806640625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3893.01416015625\n",
      "Policy Loss: -37.39149856567383\n",
      "Old Approx KL: 0.011045558378100395\n",
      "Approx KL: 0.00019931794668082148\n",
      "Clip Fraction: 0.09375\n",
      "Explained Variance: 0.07444632053375244\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 300\n",
      "Episodic Return: [ 1907.6195  2429.5693 -1287.141  -3141.1992]\n",
      "Smoothed Returns for agent_0 (Training): 2545.05224609375\n",
      "Smoothed Returns for agent_1 (Training): 3882.990966796875\n",
      "Smoothed Returns for agent_2 (Frozen): -3798.568359375\n",
      "Smoothed Returns for agent_3 (Frozen): -3033.57861328125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3027.47607421875\n",
      "Policy Loss: 64.99368286132812\n",
      "Old Approx KL: -0.11052557826042175\n",
      "Approx KL: 0.02186105214059353\n",
      "Clip Fraction: 0.20386905098954836\n",
      "Explained Variance: 0.031344473361968994\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 310\n",
      "Episodic Return: [  939.26825  2893.9573  -3529.0452   -994.0038 ]\n",
      "Smoothed Returns for agent_0 (Training): 2495.1806640625\n",
      "Smoothed Returns for agent_1 (Training): 3423.134765625\n",
      "Smoothed Returns for agent_2 (Frozen): -4118.8896484375\n",
      "Smoothed Returns for agent_3 (Frozen): -1960.6959228515625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2201.4599609375\n",
      "Policy Loss: 37.97550964355469\n",
      "Old Approx KL: 0.06552049517631531\n",
      "Approx KL: 0.014867800287902355\n",
      "Clip Fraction: 0.10193452487389247\n",
      "Explained Variance: 0.15768444538116455\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 320\n",
      "Episodic Return: [ -862.45447 -1139.5917    876.07367   889.2198 ]\n",
      "Smoothed Returns for agent_0 (Training): 1080.065673828125\n",
      "Smoothed Returns for agent_1 (Training): 1875.1519775390625\n",
      "Smoothed Returns for agent_2 (Frozen): -2671.16943359375\n",
      "Smoothed Returns for agent_3 (Frozen): -569.9349365234375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 483.0867919921875\n",
      "Policy Loss: 12.871288299560547\n",
      "Old Approx KL: 0.0001329013321083039\n",
      "Approx KL: 5.10896995820076e-08\n",
      "Clip Fraction: 0.00390625\n",
      "Explained Variance: -0.12492883205413818\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 330\n",
      "Episodic Return: [ -852.6411  -1331.6621    866.48413   874.7302 ]\n",
      "Smoothed Returns for agent_0 (Training): -505.0511779785156\n",
      "Smoothed Returns for agent_1 (Training): -472.50897216796875\n",
      "Smoothed Returns for agent_2 (Frozen): 66.2891845703125\n",
      "Smoothed Returns for agent_3 (Frozen): 462.88720703125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 138.07467651367188\n",
      "Policy Loss: -14.983543395996094\n",
      "Old Approx KL: 0.00023078919912222773\n",
      "Approx KL: 5.960464477539063e-08\n",
      "Clip Fraction: 0.0013020833333333333\n",
      "Explained Variance: -0.1283574104309082\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 340\n",
      "Episodic Return: [ -868.4176 -1314.37     886.3643   884.3833]\n",
      "Smoothed Returns for agent_0 (Training): -729.2619018554688\n",
      "Smoothed Returns for agent_1 (Training): -1177.5380859375\n",
      "Smoothed Returns for agent_2 (Frozen): 711.0765991210938\n",
      "Smoothed Returns for agent_3 (Frozen): 768.0398559570312\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 25.070966720581055\n",
      "Policy Loss: 0.4731091260910034\n",
      "Old Approx KL: 1.1989049198746216e-05\n",
      "Approx KL: 2.55448497910038e-08\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.1795419454574585\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 350\n",
      "Episodic Return: [ -443.33237 -1285.3171    504.41632   678.2445 ]\n",
      "Smoothed Returns for agent_0 (Training): -699.0816040039062\n",
      "Smoothed Returns for agent_1 (Training): -1142.531494140625\n",
      "Smoothed Returns for agent_2 (Frozen): 691.9126586914062\n",
      "Smoothed Returns for agent_3 (Frozen): 752.4208374023438\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 18.616418838500977\n",
      "Policy Loss: 2.494077444076538\n",
      "Old Approx KL: -0.002170017920434475\n",
      "Approx KL: 7.757119419693481e-06\n",
      "Clip Fraction: 0.0026041666666666665\n",
      "Explained Variance: -0.10998094081878662\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 360\n",
      "Episodic Return: [ -851.2049  -1307.7092    863.93805   870.12366]\n",
      "Smoothed Returns for agent_0 (Training): -827.6900634765625\n",
      "Smoothed Returns for agent_1 (Training): -1242.0823974609375\n",
      "Smoothed Returns for agent_2 (Frozen): 853.0418090820312\n",
      "Smoothed Returns for agent_3 (Frozen): 857.9803466796875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 247.29348754882812\n",
      "Policy Loss: -20.622188568115234\n",
      "Old Approx KL: -0.00030871800845488906\n",
      "Approx KL: 6.811959707420101e-08\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.01912236213684082\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 370\n",
      "Episodic Return: [ -878.8764 -1052.534    884.033    887.5161]\n",
      "Smoothed Returns for agent_0 (Training): -862.8953247070312\n",
      "Smoothed Returns for agent_1 (Training): -1224.3553466796875\n",
      "Smoothed Returns for agent_2 (Frozen): 876.36767578125\n",
      "Smoothed Returns for agent_3 (Frozen): 877.4039916992188\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 368.5949401855469\n",
      "Policy Loss: -25.138368606567383\n",
      "Old Approx KL: 0.0005592618836089969\n",
      "Approx KL: 2.8099333349018707e-07\n",
      "Clip Fraction: 0.0026041666666666665\n",
      "Explained Variance: -0.09159541130065918\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 380\n",
      "Episodic Return: [ -855.1367  -1334.847     863.9043    871.86145]\n",
      "Smoothed Returns for agent_0 (Training): -859.7203369140625\n",
      "Smoothed Returns for agent_1 (Training): -821.1399536132812\n",
      "Smoothed Returns for agent_2 (Frozen): 667.9142456054688\n",
      "Smoothed Returns for agent_3 (Frozen): 623.4910278320312\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 179.54225158691406\n",
      "Policy Loss: 17.471792221069336\n",
      "Old Approx KL: 0.02363654598593712\n",
      "Approx KL: 0.00046844995813444257\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.012894630432128906\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 390\n",
      "Episodic Return: [ -842.8895  4784.261  -3482.964  -1629.99  ]\n",
      "Smoothed Returns for agent_0 (Training): -858.9200439453125\n",
      "Smoothed Returns for agent_1 (Training): 352.8147277832031\n",
      "Smoothed Returns for agent_2 (Frozen): -63.97198486328125\n",
      "Smoothed Returns for agent_3 (Frozen): 28.544885635375977\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 4152.15771484375\n",
      "Policy Loss: -59.889041900634766\n",
      "Old Approx KL: -0.07464991509914398\n",
      "Approx KL: 0.06477190554141998\n",
      "Clip Fraction: 0.2821800634264946\n",
      "Explained Variance: 0.21410298347473145\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 400\n",
      "Episodic Return: [  758.28204  2631.2615   -471.18707 -3698.5789 ]\n",
      "Smoothed Returns for agent_0 (Training): -748.1882934570312\n",
      "Smoothed Returns for agent_1 (Training): 1350.600830078125\n",
      "Smoothed Returns for agent_2 (Frozen): -619.1845703125\n",
      "Smoothed Returns for agent_3 (Frozen): -637.5074462890625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2605.619873046875\n",
      "Policy Loss: 11.289676666259766\n",
      "Old Approx KL: 0.015692269429564476\n",
      "Approx KL: 0.0008602142916060984\n",
      "Clip Fraction: 0.060081845770279564\n",
      "Explained Variance: 0.045715391635894775\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 410\n",
      "Episodic Return: [ 2024.2072   2497.114     124.41284 -4463.4204 ]\n",
      "Smoothed Returns for agent_0 (Training): -388.94891357421875\n",
      "Smoothed Returns for agent_1 (Training): 1221.1712646484375\n",
      "Smoothed Returns for agent_2 (Frozen): -434.02081298828125\n",
      "Smoothed Returns for agent_3 (Frozen): -854.9344482421875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 10176.5771484375\n",
      "Policy Loss: 66.76216125488281\n",
      "Old Approx KL: 0.11051389575004578\n",
      "Approx KL: 0.020901493728160858\n",
      "Clip Fraction: 0.1595982164144516\n",
      "Explained Variance: 0.02066957950592041\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 420\n",
      "Episodic Return: [ -845.78094 -1277.4617    864.56555   854.166  ]\n",
      "Smoothed Returns for agent_0 (Training): -327.54510498046875\n",
      "Smoothed Returns for agent_1 (Training): 257.77008056640625\n",
      "Smoothed Returns for agent_2 (Frozen): 67.50135803222656\n",
      "Smoothed Returns for agent_3 (Frozen): -242.6231231689453\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 140.95339965820312\n",
      "Policy Loss: 12.4288969039917\n",
      "Old Approx KL: 0.0011046954896301031\n",
      "Approx KL: 8.940697284742782e-07\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: 0.00664132833480835\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 430\n",
      "Episodic Return: [ -828.9431  3165.7952  -519.4048 -2415.4702]\n",
      "Smoothed Returns for agent_0 (Training): -687.6026611328125\n",
      "Smoothed Returns for agent_1 (Training): -505.74249267578125\n",
      "Smoothed Returns for agent_2 (Frozen): 530.1954956054688\n",
      "Smoothed Returns for agent_3 (Frozen): 388.11529541015625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 11571.7880859375\n",
      "Policy Loss: -120.22271728515625\n",
      "Old Approx KL: -0.04420610889792442\n",
      "Approx KL: 0.006666362751275301\n",
      "Clip Fraction: 0.0848214291036129\n",
      "Explained Variance: 0.12443464994430542\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 440\n",
      "Episodic Return: [-865.5096  -369.11667  367.8737   571.3036 ]\n",
      "Smoothed Returns for agent_0 (Training): -778.3743896484375\n",
      "Smoothed Returns for agent_1 (Training): 784.9898681640625\n",
      "Smoothed Returns for agent_2 (Frozen): -15.448710441589355\n",
      "Smoothed Returns for agent_3 (Frozen): -388.8085021972656\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 74.86509704589844\n",
      "Policy Loss: -4.148743629455566\n",
      "Old Approx KL: -0.10190583020448685\n",
      "Approx KL: 0.021693911403417587\n",
      "Clip Fraction: 0.0407366082072258\n",
      "Explained Variance: 0.31033021211624146\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 450\n",
      "Episodic Return: [ -852.7058 -1168.4569   861.8221   871.5925]\n",
      "Smoothed Returns for agent_0 (Training): -724.8046264648438\n",
      "Smoothed Returns for agent_1 (Training): 604.7655639648438\n",
      "Smoothed Returns for agent_2 (Frozen): 31.403411865234375\n",
      "Smoothed Returns for agent_3 (Frozen): -312.70355224609375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 67.23546600341797\n",
      "Policy Loss: -6.601156711578369\n",
      "Old Approx KL: 0.0005179133149795234\n",
      "Approx KL: 2.4693355271665496e-07\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.01876819133758545\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 460\n",
      "Episodic Return: [-906.59717   123.392715  -67.03511   534.55634 ]\n",
      "Smoothed Returns for agent_0 (Training): -791.3582763671875\n",
      "Smoothed Returns for agent_1 (Training): -770.4430541992188\n",
      "Smoothed Returns for agent_2 (Frozen): 685.8897705078125\n",
      "Smoothed Returns for agent_3 (Frozen): 577.5729370117188\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 28957.630859375\n",
      "Policy Loss: 196.81651306152344\n",
      "Old Approx KL: -0.0007824216736480594\n",
      "Approx KL: 0.00046532496344298124\n",
      "Clip Fraction: 0.006510416666666667\n",
      "Explained Variance: -0.03836941719055176\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 470\n",
      "Episodic Return: [ -852.9504  -1247.8158    862.5002    867.74347]\n",
      "Smoothed Returns for agent_0 (Training): -848.2799072265625\n",
      "Smoothed Returns for agent_1 (Training): -515.1049194335938\n",
      "Smoothed Returns for agent_2 (Frozen): 532.8887329101562\n",
      "Smoothed Returns for agent_3 (Frozen): 515.8118896484375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3.777623414993286\n",
      "Policy Loss: -1.3604191541671753\n",
      "Old Approx KL: -0.0029826166573911905\n",
      "Approx KL: 6.369182301568799e-06\n",
      "Clip Fraction: 0.00390625\n",
      "Explained Variance: 0.061754584312438965\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 480\n",
      "Episodic Return: [ -857.14355 -1310.0092    871.80304   872.0423 ]\n",
      "Smoothed Returns for agent_0 (Training): -862.1767578125\n",
      "Smoothed Returns for agent_1 (Training): -862.7713623046875\n",
      "Smoothed Returns for agent_2 (Frozen): 680.8211059570312\n",
      "Smoothed Returns for agent_3 (Frozen): 705.9180297851562\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3.425557851791382\n",
      "Policy Loss: -0.9325249791145325\n",
      "Old Approx KL: 0.0001636232773307711\n",
      "Approx KL: 5.960464477539063e-08\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.047695159912109375\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 490\n",
      "Episodic Return: [ -865.6118 -1096.7302   871.3131   883.0949]\n",
      "Smoothed Returns for agent_0 (Training): -860.33056640625\n",
      "Smoothed Returns for agent_1 (Training): -1063.97119140625\n",
      "Smoothed Returns for agent_2 (Frozen): 831.9655151367188\n",
      "Smoothed Returns for agent_3 (Frozen): 792.5472412109375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 27.11836051940918\n",
      "Policy Loss: 1.9293581247329712\n",
      "Old Approx KL: 0.0010398456361144781\n",
      "Approx KL: 1.2942723515152466e-06\n",
      "Clip Fraction: 0.00390625\n",
      "Explained Variance: -0.1507112979888916\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 500\n",
      "Episodic Return: [ -869.16406 -1161.5865    879.6411    882.9016 ]\n",
      "Smoothed Returns for agent_0 (Training): -866.3439331054688\n",
      "Smoothed Returns for agent_1 (Training): -968.4881591796875\n",
      "Smoothed Returns for agent_2 (Frozen): 782.9874267578125\n",
      "Smoothed Returns for agent_3 (Frozen): 781.3129272460938\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 32.4153938293457\n",
      "Policy Loss: -7.284320831298828\n",
      "Old Approx KL: 0.00023426329426001757\n",
      "Approx KL: 1.1920928955078125e-07\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.0142974853515625\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 510\n",
      "Episodic Return: [ -873.8273  -1163.0288    890.72766   883.29816]\n",
      "Smoothed Returns for agent_0 (Training): -865.2735595703125\n",
      "Smoothed Returns for agent_1 (Training): -1066.75\n",
      "Smoothed Returns for agent_2 (Frozen): 792.5819702148438\n",
      "Smoothed Returns for agent_3 (Frozen): 850.6448974609375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 34.46134567260742\n",
      "Policy Loss: 8.021707534790039\n",
      "Old Approx KL: -6.825583841418847e-05\n",
      "Approx KL: 0.0\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.21466708183288574\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 520\n",
      "Episodic Return: [ -868.40125 -1076.4772    879.0027    880.26184]\n",
      "Smoothed Returns for agent_0 (Training): -871.40625\n",
      "Smoothed Returns for agent_1 (Training): -647.5886840820312\n",
      "Smoothed Returns for agent_2 (Frozen): 524.6517944335938\n",
      "Smoothed Returns for agent_3 (Frozen): 648.7308349609375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 309.16259765625\n",
      "Policy Loss: -22.809602737426758\n",
      "Old Approx KL: 0.00010395050776423886\n",
      "Approx KL: 3.4059798537100505e-08\n",
      "Clip Fraction: 0.0013020833333333333\n",
      "Explained Variance: -0.011652469635009766\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 530\n",
      "Episodic Return: [ -855.67377 -1099.0442    868.15094   871.85016]\n",
      "Smoothed Returns for agent_0 (Training): -870.7282104492188\n",
      "Smoothed Returns for agent_1 (Training): -401.78790283203125\n",
      "Smoothed Returns for agent_2 (Frozen): 344.3548583984375\n",
      "Smoothed Returns for agent_3 (Frozen): 564.4832763671875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 75.99940490722656\n",
      "Policy Loss: -11.5775785446167\n",
      "Old Approx KL: 2.4250575734185986e-05\n",
      "Approx KL: 2.55448497910038e-08\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.13302528858184814\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 540\n",
      "Episodic Return: [ -857.48517 -1177.0234    870.4503    866.477  ]\n",
      "Smoothed Returns for agent_0 (Training): -858.0130615234375\n",
      "Smoothed Returns for agent_1 (Training): -443.3487243652344\n",
      "Smoothed Returns for agent_2 (Frozen): 384.4586486816406\n",
      "Smoothed Returns for agent_3 (Frozen): 583.3036499023438\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 17.950199127197266\n",
      "Policy Loss: 5.310556411743164\n",
      "Old Approx KL: 0.00034645627602003515\n",
      "Approx KL: 1.7029898913278885e-07\n",
      "Clip Fraction: 0.00390625\n",
      "Explained Variance: -0.30351150035858154\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 550\n",
      "Episodic Return: [-863.16077 -459.884    310.10522  564.1061 ]\n",
      "Smoothed Returns for agent_0 (Training): -854.0772705078125\n",
      "Smoothed Returns for agent_1 (Training): -140.6455535888672\n",
      "Smoothed Returns for agent_2 (Frozen): 302.8133850097656\n",
      "Smoothed Returns for agent_3 (Frozen): 300.21795654296875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 21.269916534423828\n",
      "Policy Loss: -6.170560359954834\n",
      "Old Approx KL: -0.0006302425172179937\n",
      "Approx KL: 2.043587983280304e-07\n",
      "Clip Fraction: 0.03515625\n",
      "Explained Variance: 0.5414484739303589\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 560\n",
      "Episodic Return: [ -861.33386 -1265.2429    876.4905    873.54114]\n",
      "Smoothed Returns for agent_0 (Training): -857.5867309570312\n",
      "Smoothed Returns for agent_1 (Training): -576.0537719726562\n",
      "Smoothed Returns for agent_2 (Frozen): 582.0983276367188\n",
      "Smoothed Returns for agent_3 (Frozen): 496.2605895996094\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 53.16017532348633\n",
      "Policy Loss: -8.6106595993042\n",
      "Old Approx KL: -0.011653083376586437\n",
      "Approx KL: 0.0005142007721588016\n",
      "Clip Fraction: 0.0013020833333333333\n",
      "Explained Variance: -0.13733041286468506\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 570\n",
      "Episodic Return: [ -852.11536 -1313.7523    863.4633    871.7238 ]\n",
      "Smoothed Returns for agent_0 (Training): -860.5667724609375\n",
      "Smoothed Returns for agent_1 (Training): -779.9595336914062\n",
      "Smoothed Returns for agent_2 (Frozen): 642.911865234375\n",
      "Smoothed Returns for agent_3 (Frozen): 647.4986572265625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 6.5351080894470215\n",
      "Policy Loss: -2.317525625228882\n",
      "Old Approx KL: -0.0010299001587554812\n",
      "Approx KL: 9.025846452459518e-07\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.013865232467651367\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 580\n",
      "Episodic Return: [  110.695015  2008.7056   -1854.9377    -995.3377  ]\n",
      "Smoothed Returns for agent_0 (Training): -800.592041015625\n",
      "Smoothed Returns for agent_1 (Training): -594.6871337890625\n",
      "Smoothed Returns for agent_2 (Frozen): 492.54217529296875\n",
      "Smoothed Returns for agent_3 (Frozen): 542.5738525390625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3774.332763671875\n",
      "Policy Loss: -36.95780944824219\n",
      "Old Approx KL: -0.052536334842443466\n",
      "Approx KL: 0.0020460912492126226\n",
      "Clip Fraction: 0.23642113308111826\n",
      "Explained Variance: 0.1285862922668457\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 590\n",
      "Episodic Return: [  298.44592  4177.3696   -664.3136  -4316.713  ]\n",
      "Smoothed Returns for agent_0 (Training): -424.97589111328125\n",
      "Smoothed Returns for agent_1 (Training): 811.9912109375\n",
      "Smoothed Returns for agent_2 (Frozen): -299.4462585449219\n",
      "Smoothed Returns for agent_3 (Frozen): -467.04315185546875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1504.5167236328125\n",
      "Policy Loss: -11.120074272155762\n",
      "Old Approx KL: -0.0024318015202879906\n",
      "Approx KL: 6.012406083755195e-05\n",
      "Clip Fraction: 0.23511905098954836\n",
      "Explained Variance: 0.23555320501327515\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 600\n",
      "Episodic Return: [ -871.3525  -1172.591     888.50555   879.568  ]\n",
      "Smoothed Returns for agent_0 (Training): -72.10528564453125\n",
      "Smoothed Returns for agent_1 (Training): 1466.014404296875\n",
      "Smoothed Returns for agent_2 (Frozen): -1171.7806396484375\n",
      "Smoothed Returns for agent_3 (Frozen): -647.5599365234375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 33.838741302490234\n",
      "Policy Loss: 8.110767364501953\n",
      "Old Approx KL: -4.495893335842993e-06\n",
      "Approx KL: 0.0\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.0684809684753418\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 610\n",
      "Episodic Return: [ -852.6275  -1329.8379    873.5404    867.66034]\n",
      "Smoothed Returns for agent_0 (Training): -446.1507263183594\n",
      "Smoothed Returns for agent_1 (Training): -395.90972900390625\n",
      "Smoothed Returns for agent_2 (Frozen): -150.7896728515625\n",
      "Smoothed Returns for agent_3 (Frozen): 588.9732055664062\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 12.238753318786621\n",
      "Policy Loss: 3.7763166427612305\n",
      "Old Approx KL: 0.00014352798461914062\n",
      "Approx KL: 5.960464477539063e-08\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.02429032325744629\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 620\n",
      "Episodic Return: [ -829.686  -1242.8848   862.1224   862.4923]\n",
      "Smoothed Returns for agent_0 (Training): -854.8424072265625\n",
      "Smoothed Returns for agent_1 (Training): -1300.5487060546875\n",
      "Smoothed Returns for agent_2 (Frozen): 869.7335205078125\n",
      "Smoothed Returns for agent_3 (Frozen): 874.3555908203125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 139.2277374267578\n",
      "Policy Loss: 14.907071113586426\n",
      "Old Approx KL: 0.01908915489912033\n",
      "Approx KL: 0.012362650595605373\n",
      "Clip Fraction: 0.21875000496705374\n",
      "Explained Variance: -0.1516098976135254\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 630\n",
      "Episodic Return: [ -869.5464 -1330.7324   885.8592   884.6885]\n",
      "Smoothed Returns for agent_0 (Training): -862.5130615234375\n",
      "Smoothed Returns for agent_1 (Training): -1290.3565673828125\n",
      "Smoothed Returns for agent_2 (Frozen): 878.0062255859375\n",
      "Smoothed Returns for agent_3 (Frozen): 880.2738037109375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 14.3069486618042\n",
      "Policy Loss: 4.161481857299805\n",
      "Old Approx KL: 0.00033290046849288046\n",
      "Approx KL: 5.10896995820076e-08\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.09288442134857178\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 640\n",
      "Episodic Return: [ -853.20703 -1344.031     864.72455   864.8939 ]\n",
      "Smoothed Returns for agent_0 (Training): -861.1473388671875\n",
      "Smoothed Returns for agent_1 (Training): -1299.929931640625\n",
      "Smoothed Returns for agent_2 (Frozen): 876.4172973632812\n",
      "Smoothed Returns for agent_3 (Frozen): 874.4525146484375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 14.048757553100586\n",
      "Policy Loss: -1.4602792263031006\n",
      "Old Approx KL: 0.0004574230988509953\n",
      "Approx KL: 1.4475413934178505e-07\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.004301309585571289\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 650\n",
      "Episodic Return: [ -879.30237 -1311.4005    891.29315   889.1858 ]\n",
      "Smoothed Returns for agent_0 (Training): -858.3369140625\n",
      "Smoothed Returns for agent_1 (Training): -1249.233154296875\n",
      "Smoothed Returns for agent_2 (Frozen): 873.6416015625\n",
      "Smoothed Returns for agent_3 (Frozen): 872.13232421875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 10.493929862976074\n",
      "Policy Loss: 1.3002232313156128\n",
      "Old Approx KL: 0.00042840413516387343\n",
      "Approx KL: 1.1920928955078125e-07\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.004034638404846191\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 660\n",
      "Episodic Return: [ -872.11774 -1242.7596    889.695     884.3063 ]\n",
      "Smoothed Returns for agent_0 (Training): -864.8284912109375\n",
      "Smoothed Returns for agent_1 (Training): -1211.988525390625\n",
      "Smoothed Returns for agent_2 (Frozen): 879.76806640625\n",
      "Smoothed Returns for agent_3 (Frozen): 879.7906494140625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 11.97933292388916\n",
      "Policy Loss: -0.523445725440979\n",
      "Old Approx KL: 0.0061576711013913155\n",
      "Approx KL: 6.247418787097558e-05\n",
      "Clip Fraction: 0.00390625\n",
      "Explained Variance: 0.07907414436340332\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 670\n",
      "Episodic Return: [ -870.8909  -1124.541     887.10736   885.1411 ]\n",
      "Smoothed Returns for agent_0 (Training): -760.9636840820312\n",
      "Smoothed Returns for agent_1 (Training): -989.2185668945312\n",
      "Smoothed Returns for agent_2 (Frozen): 790.7579345703125\n",
      "Smoothed Returns for agent_3 (Frozen): 636.8001708984375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 16.478862762451172\n",
      "Policy Loss: 2.9601070880889893\n",
      "Old Approx KL: -0.0005945478333160281\n",
      "Approx KL: 4.427773774295929e-07\n",
      "Clip Fraction: 0.0013020833333333333\n",
      "Explained Variance: -0.01079094409942627\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 680\n",
      "Episodic Return: [ -848.10626 -1063.6143    867.04034   879.85706]\n",
      "Smoothed Returns for agent_0 (Training): -697.1048583984375\n",
      "Smoothed Returns for agent_1 (Training): -857.9015502929688\n",
      "Smoothed Returns for agent_2 (Frozen): 656.2596435546875\n",
      "Smoothed Returns for agent_3 (Frozen): 601.14892578125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 196.66497802734375\n",
      "Policy Loss: 18.154857635498047\n",
      "Old Approx KL: -0.014968532137572765\n",
      "Approx KL: 0.0002926758606918156\n",
      "Clip Fraction: 0.0809151791036129\n",
      "Explained Variance: -0.10616087913513184\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 690\n",
      "Episodic Return: [ -866.98315 -1335.6816    877.8205    882.1629 ]\n",
      "Smoothed Returns for agent_0 (Training): -790.1759643554688\n",
      "Smoothed Returns for agent_1 (Training): -1044.978271484375\n",
      "Smoothed Returns for agent_2 (Frozen): 736.9429931640625\n",
      "Smoothed Returns for agent_3 (Frozen): 834.6305541992188\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 20.185997009277344\n",
      "Policy Loss: -2.6349804401397705\n",
      "Old Approx KL: 0.0007592610199935734\n",
      "Approx KL: 3.661428422674362e-07\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.03712725639343262\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 700\n",
      "Episodic Return: [ -867.34357 -1084.5264    881.6883    880.5671 ]\n",
      "Smoothed Returns for agent_0 (Training): -628.3064575195312\n",
      "Smoothed Returns for agent_1 (Training): -846.3318481445312\n",
      "Smoothed Returns for agent_2 (Frozen): 671.906494140625\n",
      "Smoothed Returns for agent_3 (Frozen): 518.5499877929688\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 6.899528980255127\n",
      "Policy Loss: -2.505841016769409\n",
      "Old Approx KL: 0.0014771734131500125\n",
      "Approx KL: 4.351139068603516e-06\n",
      "Clip Fraction: 0.010416666666666666\n",
      "Explained Variance: 0.11772418022155762\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 710\n",
      "Episodic Return: [ -863.0807 -1120.414    874.0141   873.6088]\n",
      "Smoothed Returns for agent_0 (Training): -552.4761962890625\n",
      "Smoothed Returns for agent_1 (Training): -723.3065185546875\n",
      "Smoothed Returns for agent_2 (Frozen): 547.4088745117188\n",
      "Smoothed Returns for agent_3 (Frozen): 413.0006408691406\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 44.36425018310547\n",
      "Policy Loss: -8.847639083862305\n",
      "Old Approx KL: 0.0016557149356231093\n",
      "Approx KL: 2.162797272831085e-06\n",
      "Clip Fraction: 0.028831845770279568\n",
      "Explained Variance: 0.022156715393066406\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 720\n",
      "Episodic Return: [ -869.6142  -1123.4858    890.56354   881.6673 ]\n",
      "Smoothed Returns for agent_0 (Training): 83.25618743896484\n",
      "Smoothed Returns for agent_1 (Training): 11.543475151062012\n",
      "Smoothed Returns for agent_2 (Frozen): -274.5625915527344\n",
      "Smoothed Returns for agent_3 (Frozen): -209.87350463867188\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 32.984291076660156\n",
      "Policy Loss: 7.284465789794922\n",
      "Old Approx KL: 0.01804334856569767\n",
      "Approx KL: 0.003523537190631032\n",
      "Clip Fraction: 0.10621279974778493\n",
      "Explained Variance: 0.0007756352424621582\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 730\n",
      "Episodic Return: [ 2742.9587  2200.2397 -3704.4675 -2376.8914]\n",
      "Smoothed Returns for agent_0 (Training): 891.4905395507812\n",
      "Smoothed Returns for agent_1 (Training): 885.7154541015625\n",
      "Smoothed Returns for agent_2 (Frozen): -1295.199951171875\n",
      "Smoothed Returns for agent_3 (Frozen): -929.3883056640625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1208.90234375\n",
      "Policy Loss: -34.08380889892578\n",
      "Old Approx KL: 0.059421099722385406\n",
      "Approx KL: 0.00864642858505249\n",
      "Clip Fraction: 0.21614583830038706\n",
      "Explained Variance: 0.11196625232696533\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 740\n",
      "Episodic Return: [ 2400.1577  2427.66   -3735.9805 -1122.0677]\n",
      "Smoothed Returns for agent_0 (Training): 1441.096923828125\n",
      "Smoothed Returns for agent_1 (Training): 1460.2982177734375\n",
      "Smoothed Returns for agent_2 (Frozen): -1916.677490234375\n",
      "Smoothed Returns for agent_3 (Frozen): -1410.68701171875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 7505.17724609375\n",
      "Policy Loss: 72.10940551757812\n",
      "Old Approx KL: 0.060893625020980835\n",
      "Approx KL: 0.0045116376131772995\n",
      "Clip Fraction: 0.19847470397750536\n",
      "Explained Variance: -0.010217905044555664\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 750\n",
      "Episodic Return: [ 2063.9348   3063.794   -4753.555    -376.77505]\n",
      "Smoothed Returns for agent_0 (Training): 1708.383544921875\n",
      "Smoothed Returns for agent_1 (Training): 1868.7991943359375\n",
      "Smoothed Returns for agent_2 (Frozen): -2139.3125\n",
      "Smoothed Returns for agent_3 (Frozen): -1807.8480224609375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1372.7027587890625\n",
      "Policy Loss: -40.805477142333984\n",
      "Old Approx KL: -0.010172623209655285\n",
      "Approx KL: 0.020004060119390488\n",
      "Clip Fraction: 0.1860119104385376\n",
      "Explained Variance: 0.011441171169281006\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 760\n",
      "Episodic Return: [ 3808.9363  4771.2734  -675.4467 -8000.2324]\n",
      "Smoothed Returns for agent_0 (Training): 2364.8447265625\n",
      "Smoothed Returns for agent_1 (Training): 2449.4248046875\n",
      "Smoothed Returns for agent_2 (Frozen): -2405.66552734375\n",
      "Smoothed Returns for agent_3 (Frozen): -2659.72216796875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1849.2950439453125\n",
      "Policy Loss: -60.23546600341797\n",
      "Old Approx KL: -0.04913916066288948\n",
      "Approx KL: 0.004160847049206495\n",
      "Clip Fraction: 0.07849702487389247\n",
      "Explained Variance: 0.09641885757446289\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 770\n",
      "Episodic Return: [ 3789.0552  3331.131  -3906.4182 -3322.0627]\n",
      "Smoothed Returns for agent_0 (Training): 3642.469970703125\n",
      "Smoothed Returns for agent_1 (Training): 3743.93017578125\n",
      "Smoothed Returns for agent_2 (Frozen): -3180.409423828125\n",
      "Smoothed Returns for agent_3 (Frozen): -4279.61962890625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 6634.90185546875\n",
      "Policy Loss: 97.0307846069336\n",
      "Old Approx KL: 0.025425810366868973\n",
      "Approx KL: 0.003369203768670559\n",
      "Clip Fraction: 0.12313988308111827\n",
      "Explained Variance: 0.16146314144134521\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 780\n",
      "Episodic Return: [ 5469.097    1269.35     -940.67474 -5971.316  ]\n",
      "Smoothed Returns for agent_0 (Training): 3933.08203125\n",
      "Smoothed Returns for agent_1 (Training): 4406.1025390625\n",
      "Smoothed Returns for agent_2 (Frozen): -3846.26806640625\n",
      "Smoothed Returns for agent_3 (Frozen): -4555.763671875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 22738.669921875\n",
      "Policy Loss: 166.80552673339844\n",
      "Old Approx KL: -0.006550039630383253\n",
      "Approx KL: 0.00019734246598090976\n",
      "Clip Fraction: 0.12295387064417203\n",
      "Explained Variance: -0.1378105878829956\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 790\n",
      "Episodic Return: [  611.04834  5753.7314  -5165.142   -1842.192  ]\n",
      "Smoothed Returns for agent_0 (Training): 3522.35693359375\n",
      "Smoothed Returns for agent_1 (Training): 4476.2685546875\n",
      "Smoothed Returns for agent_2 (Frozen): -4443.2001953125\n",
      "Smoothed Returns for agent_3 (Frozen): -3939.162109375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 6008.52197265625\n",
      "Policy Loss: -49.2534065246582\n",
      "Old Approx KL: 0.07707841694355011\n",
      "Approx KL: 0.01430580485612154\n",
      "Clip Fraction: 0.13820684577027956\n",
      "Explained Variance: 0.050708234310150146\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 800\n",
      "Episodic Return: [ 5106.5083  4982.752  -9631.508  -1092.4357]\n",
      "Smoothed Returns for agent_0 (Training): 3587.020751953125\n",
      "Smoothed Returns for agent_1 (Training): 4414.1005859375\n",
      "Smoothed Returns for agent_2 (Frozen): -5000.439453125\n",
      "Smoothed Returns for agent_3 (Frozen): -3472.61962890625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2201.0703125\n",
      "Policy Loss: -55.10133743286133\n",
      "Old Approx KL: -0.02529202215373516\n",
      "Approx KL: 0.00444462476298213\n",
      "Clip Fraction: 0.0262276791036129\n",
      "Explained Variance: 0.13889819383621216\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 810\n",
      "Episodic Return: [ 6239.884   1808.653  -6115.706  -2506.6772]\n",
      "Smoothed Returns for agent_0 (Training): 4173.0341796875\n",
      "Smoothed Returns for agent_1 (Training): 4319.5498046875\n",
      "Smoothed Returns for agent_2 (Frozen): -5458.666015625\n",
      "Smoothed Returns for agent_3 (Frozen): -3638.224609375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 288.9695739746094\n",
      "Policy Loss: -14.999326705932617\n",
      "Old Approx KL: -0.01955413818359375\n",
      "Approx KL: 0.014109271578490734\n",
      "Clip Fraction: 0.3097098246216774\n",
      "Explained Variance: 0.6878754496574402\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 820\n",
      "Episodic Return: [ 4026.8154   -407.63593 -4280.4316    632.9948 ]\n",
      "Smoothed Returns for agent_0 (Training): 3785.303466796875\n",
      "Smoothed Returns for agent_1 (Training): 3716.83349609375\n",
      "Smoothed Returns for agent_2 (Frozen): -4691.14794921875\n",
      "Smoothed Returns for agent_3 (Frozen): -3398.79736328125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1406.7606201171875\n",
      "Policy Loss: -49.56435012817383\n",
      "Old Approx KL: 0.05421005189418793\n",
      "Approx KL: 0.035632193088531494\n",
      "Clip Fraction: 0.5556175683935484\n",
      "Explained Variance: -0.10499811172485352\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 830\n",
      "Episodic Return: [ -847.6    -1251.615    862.5864   862.2617]\n",
      "Smoothed Returns for agent_0 (Training): 1933.929931640625\n",
      "Smoothed Returns for agent_1 (Training): 2146.103759765625\n",
      "Smoothed Returns for agent_2 (Frozen): -2703.986083984375\n",
      "Smoothed Returns for agent_3 (Frozen): -1924.92578125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 197.75308227539062\n",
      "Policy Loss: -19.471847534179688\n",
      "Old Approx KL: 0.00496111623942852\n",
      "Approx KL: 0.00014076063234824687\n",
      "Clip Fraction: 0.1707589291036129\n",
      "Explained Variance: -0.06620490550994873\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 840\n",
      "Episodic Return: [ 2104.7437  3536.3337 -5995.1816   834.4852]\n",
      "Smoothed Returns for agent_0 (Training): 711.3839111328125\n",
      "Smoothed Returns for agent_1 (Training): 942.3312377929688\n",
      "Smoothed Returns for agent_2 (Frozen): -920.9220581054688\n",
      "Smoothed Returns for agent_3 (Frozen): -1179.8179931640625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1736.8170166015625\n",
      "Policy Loss: 32.6054801940918\n",
      "Old Approx KL: -0.0011531285708770156\n",
      "Approx KL: 5.841255642735632e-06\n",
      "Clip Fraction: 0.08221726243694623\n",
      "Explained Variance: -0.17384767532348633\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 850\n",
      "Episodic Return: [ -871.3543 -1265.7202   892.8425   877.6135]\n",
      "Smoothed Returns for agent_0 (Training): 416.82440185546875\n",
      "Smoothed Returns for agent_1 (Training): 652.0771484375\n",
      "Smoothed Returns for agent_2 (Frozen): -304.8997497558594\n",
      "Smoothed Returns for agent_3 (Frozen): -960.6103515625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 121.83158874511719\n",
      "Policy Loss: -13.957283020019531\n",
      "Old Approx KL: 0.047934193164110184\n",
      "Approx KL: 0.0099295973777771\n",
      "Clip Fraction: 0.10863095397750537\n",
      "Explained Variance: -0.2115485668182373\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 860\n",
      "Episodic Return: [ -875.1385  -1156.3005    886.22656   888.94086]\n",
      "Smoothed Returns for agent_0 (Training): -300.25823974609375\n",
      "Smoothed Returns for agent_1 (Training): -215.1400604248047\n",
      "Smoothed Returns for agent_2 (Frozen): 239.84579467773438\n",
      "Smoothed Returns for agent_3 (Frozen): -33.472660064697266\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 47.880455017089844\n",
      "Policy Loss: 9.183050155639648\n",
      "Old Approx KL: -0.0014356885803863406\n",
      "Approx KL: 1.4645713690697448e-06\n",
      "Clip Fraction: 0.022135416666666668\n",
      "Explained Variance: -0.2806360721588135\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 870\n",
      "Episodic Return: [  167.30226   365.89017   606.48584 -1253.4596 ]\n",
      "Smoothed Returns for agent_0 (Training): -368.10943603515625\n",
      "Smoothed Returns for agent_1 (Training): -529.6005859375\n",
      "Smoothed Returns for agent_2 (Frozen): 341.45025634765625\n",
      "Smoothed Returns for agent_3 (Frozen): 268.10076904296875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 459.59918212890625\n",
      "Policy Loss: 30.206010818481445\n",
      "Old Approx KL: 0.0026358196046203375\n",
      "Approx KL: 4.785401870321948e-06\n",
      "Clip Fraction: 0.0418526791036129\n",
      "Explained Variance: 0.15656858682632446\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 880\n",
      "Episodic Return: [ -42.572945  293.0515   -894.6601    184.55434 ]\n",
      "Smoothed Returns for agent_0 (Training): -207.49951171875\n",
      "Smoothed Returns for agent_1 (Training): -309.9767150878906\n",
      "Smoothed Returns for agent_2 (Frozen): -88.0121078491211\n",
      "Smoothed Returns for agent_3 (Frozen): 320.8919677734375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 866.7657470703125\n",
      "Policy Loss: -14.726771354675293\n",
      "Old Approx KL: -0.010704313404858112\n",
      "Approx KL: 0.0004457490867935121\n",
      "Clip Fraction: 0.0418526791036129\n",
      "Explained Variance: 0.1210668683052063\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 890\n",
      "Episodic Return: [ -867.93756 -1085.5121    880.5183    874.6982 ]\n",
      "Smoothed Returns for agent_0 (Training): -593.195556640625\n",
      "Smoothed Returns for agent_1 (Training): -808.1844482421875\n",
      "Smoothed Returns for agent_2 (Frozen): 395.27459716796875\n",
      "Smoothed Returns for agent_3 (Frozen): 700.5535278320312\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 19.09070587158203\n",
      "Policy Loss: -5.986800670623779\n",
      "Old Approx KL: 0.0010315350955352187\n",
      "Approx KL: 1.1495181979626068e-06\n",
      "Clip Fraction: 0.0078125\n",
      "Explained Variance: -0.2121751308441162\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 900\n",
      "Episodic Return: [ -854.7032  -1309.5951    871.8318    870.67017]\n",
      "Smoothed Returns for agent_0 (Training): -843.9255981445312\n",
      "Smoothed Returns for agent_1 (Training): -967.99169921875\n",
      "Smoothed Returns for agent_2 (Frozen): 774.7764892578125\n",
      "Smoothed Returns for agent_3 (Frozen): 739.9095458984375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 18.75319480895996\n",
      "Policy Loss: -5.599234580993652\n",
      "Old Approx KL: 0.0003181185165885836\n",
      "Approx KL: 1.1920928955078125e-07\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.0246884822845459\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 910\n",
      "Episodic Return: [ -850.43    -1241.2205    867.66394   865.5174 ]\n",
      "Smoothed Returns for agent_0 (Training): -857.1013793945312\n",
      "Smoothed Returns for agent_1 (Training): -999.59228515625\n",
      "Smoothed Returns for agent_2 (Frozen): 790.0196533203125\n",
      "Smoothed Returns for agent_3 (Frozen): 764.6009521484375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 47.41490173339844\n",
      "Policy Loss: 8.985301971435547\n",
      "Old Approx KL: 0.038504328578710556\n",
      "Approx KL: 0.005020508076995611\n",
      "Clip Fraction: 0.0145089291036129\n",
      "Explained Variance: 0.016605496406555176\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 920\n",
      "Episodic Return: [ -852.37396 -1321.1661    869.1739    868.1095 ]\n",
      "Smoothed Returns for agent_0 (Training): -860.1827392578125\n",
      "Smoothed Returns for agent_1 (Training): -1218.507080078125\n",
      "Smoothed Returns for agent_2 (Frozen): 873.8278198242188\n",
      "Smoothed Returns for agent_3 (Frozen): 876.2888793945312\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 9.604008674621582\n",
      "Policy Loss: 3.394369602203369\n",
      "Old Approx KL: 6.30106296739541e-05\n",
      "Approx KL: 9.366444686520481e-08\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.0030536651611328125\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 930\n",
      "Episodic Return: [ -846.1686  -1312.0033    861.34814   865.63617]\n",
      "Smoothed Returns for agent_0 (Training): -851.6725463867188\n",
      "Smoothed Returns for agent_1 (Training): -1217.7435302734375\n",
      "Smoothed Returns for agent_2 (Frozen): 863.8767700195312\n",
      "Smoothed Returns for agent_3 (Frozen): 838.0875854492188\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 12.440085411071777\n",
      "Policy Loss: -4.654302597045898\n",
      "Old Approx KL: -4.863739377469756e-05\n",
      "Approx KL: 0.0\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.17216753959655762\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 940\n",
      "Episodic Return: [ -844.5843  -1335.3556    861.40894   861.2717 ]\n",
      "Smoothed Returns for agent_0 (Training): -848.3614501953125\n",
      "Smoothed Returns for agent_1 (Training): -1231.702880859375\n",
      "Smoothed Returns for agent_2 (Frozen): 863.0657958984375\n",
      "Smoothed Returns for agent_3 (Frozen): 838.0509033203125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 6.544036388397217\n",
      "Policy Loss: 3.144702196121216\n",
      "Old Approx KL: -0.00064849853515625\n",
      "Approx KL: 3.576278970740532e-07\n",
      "Clip Fraction: 0.0026041666666666665\n",
      "Explained Variance: 0.004829287528991699\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 950\n",
      "Episodic Return: [ -873.0542  -1354.6759    892.5822    882.58734]\n",
      "Smoothed Returns for agent_0 (Training): -854.740234375\n",
      "Smoothed Returns for agent_1 (Training): -1299.022216796875\n",
      "Smoothed Returns for agent_2 (Frozen): 870.7052612304688\n",
      "Smoothed Returns for agent_3 (Frozen): 874.5286865234375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 9.47718620300293\n",
      "Policy Loss: -1.6352307796478271\n",
      "Old Approx KL: 0.00010068076517200097\n",
      "Approx KL: 4.257474728319721e-08\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.05999553203582764\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 960\n",
      "Episodic Return: [ -859.79364 -1264.7944    881.39276   875.97034]\n",
      "Smoothed Returns for agent_0 (Training): -858.7784423828125\n",
      "Smoothed Returns for agent_1 (Training): -1317.137451171875\n",
      "Smoothed Returns for agent_2 (Frozen): 874.9777221679688\n",
      "Smoothed Returns for agent_3 (Frozen): 874.3952026367188\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 53.8956413269043\n",
      "Policy Loss: 1.0028245449066162\n",
      "Old Approx KL: 0.00010122571984538808\n",
      "Approx KL: 3.4059798537100505e-08\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.17952167987823486\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 970\n",
      "Episodic Return: [ -860.53906 -1343.8097    881.264     880.4632 ]\n",
      "Smoothed Returns for agent_0 (Training): -861.6443481445312\n",
      "Smoothed Returns for agent_1 (Training): -1297.5867919921875\n",
      "Smoothed Returns for agent_2 (Frozen): 878.4117431640625\n",
      "Smoothed Returns for agent_3 (Frozen): 876.2371826171875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 107.7247085571289\n",
      "Policy Loss: 14.511637687683105\n",
      "Old Approx KL: 0.0006063325563445687\n",
      "Approx KL: 5.364418029785156e-07\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: 0.001877427101135254\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 980\n",
      "Episodic Return: [ -866.4052 -1176.498    878.0412   877.4349]\n",
      "Smoothed Returns for agent_0 (Training): -861.1466064453125\n",
      "Smoothed Returns for agent_1 (Training): -1256.234130859375\n",
      "Smoothed Returns for agent_2 (Frozen): 876.6155395507812\n",
      "Smoothed Returns for agent_3 (Frozen): 876.9363403320312\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 13.442231178283691\n",
      "Policy Loss: -0.10701397806406021\n",
      "Old Approx KL: 2.7656556994770654e-05\n",
      "Approx KL: 8.514949634275126e-09\n",
      "Clip Fraction: 0.00390625\n",
      "Explained Variance: 0.03777962923049927\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 990\n",
      "Episodic Return: [ -874.6282 -1307.713    888.9996   886.3531]\n",
      "Smoothed Returns for agent_0 (Training): -864.2481689453125\n",
      "Smoothed Returns for agent_1 (Training): -1253.6632080078125\n",
      "Smoothed Returns for agent_2 (Frozen): 879.0682373046875\n",
      "Smoothed Returns for agent_3 (Frozen): 879.52978515625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 126.77630615234375\n",
      "Policy Loss: 15.827461242675781\n",
      "Old Approx KL: 8.405958215007558e-05\n",
      "Approx KL: 1.3623919414840202e-07\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: 0.11922550201416016\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 1000\n",
      "Episodic Return: [ -870.6874 -1121.3702   885.8896   884.8268]\n",
      "Smoothed Returns for agent_0 (Training): -670.37158203125\n",
      "Smoothed Returns for agent_1 (Training): -1049.84423828125\n",
      "Smoothed Returns for agent_2 (Frozen): 698.9071044921875\n",
      "Smoothed Returns for agent_3 (Frozen): 692.3676147460938\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 52.426856994628906\n",
      "Policy Loss: -10.175975799560547\n",
      "Old Approx KL: 0.0017337800236418843\n",
      "Approx KL: 3.899846888089087e-06\n",
      "Clip Fraction: 0.00390625\n",
      "Explained Variance: -0.12842059135437012\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1010\n",
      "Episodic Return: [ -858.842   -1288.0125    875.14307   881.3475 ]\n",
      "Smoothed Returns for agent_0 (Training): 198.89761352539062\n",
      "Smoothed Returns for agent_1 (Training): -86.25946807861328\n",
      "Smoothed Returns for agent_2 (Frozen): 134.97000122070312\n",
      "Smoothed Returns for agent_3 (Frozen): -518.2606201171875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 7.35326623916626\n",
      "Policy Loss: 0.4612472355365753\n",
      "Old Approx KL: -0.0011488370364531875\n",
      "Approx KL: 1.481601202613092e-06\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.22052526473999023\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1020\n",
      "Episodic Return: [ -860.4696  -1094.8312    884.40234   873.68726]\n",
      "Smoothed Returns for agent_0 (Training): 203.9127197265625\n",
      "Smoothed Returns for agent_1 (Training): -18.398168563842773\n",
      "Smoothed Returns for agent_2 (Frozen): -65.8021469116211\n",
      "Smoothed Returns for agent_3 (Frozen): -378.7620544433594\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 5.589646339416504\n",
      "Policy Loss: -1.958917260169983\n",
      "Old Approx KL: -0.00016961779328994453\n",
      "Approx KL: 3.4059798537100505e-08\n",
      "Clip Fraction: 0.0013020833333333333\n",
      "Explained Variance: -0.15982282161712646\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1030\n",
      "Episodic Return: [ -817.48425 -1303.082     838.25867   824.3711 ]\n",
      "Smoothed Returns for agent_0 (Training): -438.31610107421875\n",
      "Smoothed Returns for agent_1 (Training): -743.6809692382812\n",
      "Smoothed Returns for agent_2 (Frozen): 357.4341125488281\n",
      "Smoothed Returns for agent_3 (Frozen): 557.0197143554688\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 95.63260650634766\n",
      "Policy Loss: 13.18029499053955\n",
      "Old Approx KL: 0.03865494206547737\n",
      "Approx KL: 0.0013250709744170308\n",
      "Clip Fraction: 0.1203497052192688\n",
      "Explained Variance: -0.10624921321868896\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1040\n",
      "Episodic Return: [ -873.3921  -1096.2028    887.47296   884.32526]\n",
      "Smoothed Returns for agent_0 (Training): -113.01910400390625\n",
      "Smoothed Returns for agent_1 (Training): -296.2020263671875\n",
      "Smoothed Returns for agent_2 (Frozen): -55.575599670410156\n",
      "Smoothed Returns for agent_3 (Frozen): 245.5039520263672\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 116.1596908569336\n",
      "Policy Loss: -15.32970905303955\n",
      "Old Approx KL: -0.0009646416292525828\n",
      "Approx KL: 6.982258469179214e-07\n",
      "Clip Fraction: 0.0145089291036129\n",
      "Explained Variance: -0.02119886875152588\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1050\n",
      "Episodic Return: [ 3275.181    3660.2573  -6796.08      469.82663]\n",
      "Smoothed Returns for agent_0 (Training): 430.41131591796875\n",
      "Smoothed Returns for agent_1 (Training): 941.6696166992188\n",
      "Smoothed Returns for agent_2 (Frozen): -1323.27880859375\n",
      "Smoothed Returns for agent_3 (Frozen): -343.207763671875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 69.4411392211914\n",
      "Policy Loss: -3.575758934020996\n",
      "Old Approx KL: 0.0010518687777221203\n",
      "Approx KL: 0.0008389779832214117\n",
      "Clip Fraction: 0.045572916666666664\n",
      "Explained Variance: 0.02251368761062622\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1060\n",
      "Episodic Return: [ -866.8862  -1168.4669    885.775     879.91895]\n",
      "Smoothed Returns for agent_0 (Training): 687.6404418945312\n",
      "Smoothed Returns for agent_1 (Training): 1209.0731201171875\n",
      "Smoothed Returns for agent_2 (Frozen): -1221.136962890625\n",
      "Smoothed Returns for agent_3 (Frozen): -1056.878173828125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 41.47141647338867\n",
      "Policy Loss: -7.575474739074707\n",
      "Old Approx KL: -0.007272584363818169\n",
      "Approx KL: 0.0038269164506345987\n",
      "Clip Fraction: 0.19996279974778494\n",
      "Explained Variance: -0.33386147022247314\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1070\n",
      "Episodic Return: [  628.73535   712.3409   -955.5212  -1025.3386 ]\n",
      "Smoothed Returns for agent_0 (Training): 28.516992568969727\n",
      "Smoothed Returns for agent_1 (Training): 100.7170181274414\n",
      "Smoothed Returns for agent_2 (Frozen): -148.29104614257812\n",
      "Smoothed Returns for agent_3 (Frozen): -346.2994079589844\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 439.84405517578125\n",
      "Policy Loss: 12.580327033996582\n",
      "Old Approx KL: 0.01700587011873722\n",
      "Approx KL: 0.00032233342062681913\n",
      "Clip Fraction: 0.16703869154055914\n",
      "Explained Variance: -0.2193925380706787\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1080\n",
      "Episodic Return: [ -859.57886 -1335.5636    870.53265   873.5978 ]\n",
      "Smoothed Returns for agent_0 (Training): -752.640869140625\n",
      "Smoothed Returns for agent_1 (Training): -911.6121826171875\n",
      "Smoothed Returns for agent_2 (Frozen): 545.3305053710938\n",
      "Smoothed Returns for agent_3 (Frozen): 727.4181518554688\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 7.671694755554199\n",
      "Policy Loss: -0.6903798580169678\n",
      "Old Approx KL: -0.00036409925087355077\n",
      "Approx KL: 0.0019491316052153707\n",
      "Clip Fraction: 0.1322544664144516\n",
      "Explained Variance: 0.03943675756454468\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1090\n",
      "Episodic Return: [ -882.2567  -1146.255     890.3704    893.03143]\n",
      "Smoothed Returns for agent_0 (Training): -861.5993041992188\n",
      "Smoothed Returns for agent_1 (Training): -1247.2093505859375\n",
      "Smoothed Returns for agent_2 (Frozen): 879.240234375\n",
      "Smoothed Returns for agent_3 (Frozen): 879.0382080078125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2.3211417198181152\n",
      "Policy Loss: -0.75599604845047\n",
      "Old Approx KL: -0.0014609609497711062\n",
      "Approx KL: 2.1908965209149756e-05\n",
      "Clip Fraction: 0.14862351243694624\n",
      "Explained Variance: -0.011359333992004395\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1100\n",
      "Episodic Return: [ -859.50726 -1265.5552    889.1635    870.3104 ]\n",
      "Smoothed Returns for agent_0 (Training): -860.9344482421875\n",
      "Smoothed Returns for agent_1 (Training): -1201.96484375\n",
      "Smoothed Returns for agent_2 (Frozen): 877.9562377929688\n",
      "Smoothed Returns for agent_3 (Frozen): 878.96826171875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 4.819512844085693\n",
      "Policy Loss: -0.033191341906785965\n",
      "Old Approx KL: 0.03813430294394493\n",
      "Approx KL: 0.0013809205265715718\n",
      "Clip Fraction: 0.09523809577027957\n",
      "Explained Variance: 0.02564316987991333\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1110\n",
      "Episodic Return: [ -866.35205 -1159.7484    885.4935    877.0876 ]\n",
      "Smoothed Returns for agent_0 (Training): -13.089465141296387\n",
      "Smoothed Returns for agent_1 (Training): -472.16229248046875\n",
      "Smoothed Returns for agent_2 (Frozen): -214.1170196533203\n",
      "Smoothed Returns for agent_3 (Frozen): 323.96002197265625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 64.02953338623047\n",
      "Policy Loss: -10.308616638183594\n",
      "Old Approx KL: 0.027988502755761147\n",
      "Approx KL: 0.002048415830358863\n",
      "Clip Fraction: 0.06510416666666667\n",
      "Explained Variance: -0.3752323389053345\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1120\n",
      "Episodic Return: [ -364.46512 -1177.638     491.13348   598.33734]\n",
      "Smoothed Returns for agent_0 (Training): 522.7756958007812\n",
      "Smoothed Returns for agent_1 (Training): 14.5145263671875\n",
      "Smoothed Returns for agent_2 (Frozen): -974.203125\n",
      "Smoothed Returns for agent_3 (Frozen): 1.2698028087615967\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3.827266216278076\n",
      "Policy Loss: -2.4289355278015137\n",
      "Old Approx KL: -0.03323330357670784\n",
      "Approx KL: 0.0038198574911803007\n",
      "Clip Fraction: 0.09170387064417203\n",
      "Explained Variance: -0.23177576065063477\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1130\n",
      "Episodic Return: [ 2.3768953e+03  2.8882109e+03 -5.6523945e+03 -2.5489547e+00]\n",
      "Smoothed Returns for agent_0 (Training): 371.67803955078125\n",
      "Smoothed Returns for agent_1 (Training): 397.4068908691406\n",
      "Smoothed Returns for agent_2 (Frozen): -1016.8206787109375\n",
      "Smoothed Returns for agent_3 (Frozen): -120.05657958984375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 589.1105346679688\n",
      "Policy Loss: 8.164905548095703\n",
      "Old Approx KL: 0.014492444694042206\n",
      "Approx KL: 0.0006338953971862793\n",
      "Clip Fraction: 0.04296875\n",
      "Explained Variance: 0.30796992778778076\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 1140\n",
      "Episodic Return: [ -878.7333 -1177.4946   890.6223   889.9944]\n",
      "Smoothed Returns for agent_0 (Training): 613.29736328125\n",
      "Smoothed Returns for agent_1 (Training): 1516.802978515625\n",
      "Smoothed Returns for agent_2 (Frozen): -1723.504150390625\n",
      "Smoothed Returns for agent_3 (Frozen): -724.690185546875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 107.18485260009766\n",
      "Policy Loss: -14.001142501831055\n",
      "Old Approx KL: 0.05385085940361023\n",
      "Approx KL: 0.03182181343436241\n",
      "Clip Fraction: 0.542782741288344\n",
      "Explained Variance: -0.14066946506500244\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1150\n",
      "Episodic Return: [ -869.46924 -1228.5609    889.0368    879.99756]\n",
      "Smoothed Returns for agent_0 (Training): -82.87213134765625\n",
      "Smoothed Returns for agent_1 (Training): 656.6347045898438\n",
      "Smoothed Returns for agent_2 (Frozen): -664.8201904296875\n",
      "Smoothed Returns for agent_3 (Frozen): -233.4114532470703\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 19.97218132019043\n",
      "Policy Loss: -5.566396713256836\n",
      "Old Approx KL: 0.021866662427783012\n",
      "Approx KL: 0.0003014036628883332\n",
      "Clip Fraction: 0.040364583333333336\n",
      "Explained Variance: -0.19936037063598633\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1160\n",
      "Episodic Return: [ 2407.0752   2363.496     698.18256 -5209.2095 ]\n",
      "Smoothed Returns for agent_0 (Training): -85.52656555175781\n",
      "Smoothed Returns for agent_1 (Training): 135.9557647705078\n",
      "Smoothed Returns for agent_2 (Frozen): 281.44677734375\n",
      "Smoothed Returns for agent_3 (Frozen): -579.5703735351562\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3946.16845703125\n",
      "Policy Loss: 66.19242858886719\n",
      "Old Approx KL: 0.04141249135136604\n",
      "Approx KL: 0.024589428678154945\n",
      "Clip Fraction: 0.23846726740399996\n",
      "Explained Variance: 0.2554728388786316\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1170\n",
      "Episodic Return: [ -851.7943 -1088.2577   876.2659   868.3764]\n",
      "Smoothed Returns for agent_0 (Training): 315.22723388671875\n",
      "Smoothed Returns for agent_1 (Training): 973.1111450195312\n",
      "Smoothed Returns for agent_2 (Frozen): -518.3541870117188\n",
      "Smoothed Returns for agent_3 (Frozen): -1017.6491088867188\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 10.696104049682617\n",
      "Policy Loss: 2.515078067779541\n",
      "Old Approx KL: 8.119855920085683e-05\n",
      "Approx KL: 0.00018978120351675898\n",
      "Clip Fraction: 0.0234375\n",
      "Explained Variance: -0.012362241744995117\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1180\n",
      "Episodic Return: [ -883.9294  -1330.3728    894.2634    891.78125]\n",
      "Smoothed Returns for agent_0 (Training): -165.1969451904297\n",
      "Smoothed Returns for agent_1 (Training): 411.503173828125\n",
      "Smoothed Returns for agent_2 (Frozen): -431.68768310546875\n",
      "Smoothed Returns for agent_3 (Frozen): -115.8680648803711\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 147.44232177734375\n",
      "Policy Loss: -12.47063159942627\n",
      "Old Approx KL: 0.6827218532562256\n",
      "Approx KL: 0.6857084631919861\n",
      "Clip Fraction: 0.9815848271052042\n",
      "Explained Variance: -0.10615599155426025\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1190\n",
      "Episodic Return: [ -883.0455  -1329.6852    892.52      891.25543]\n",
      "Smoothed Returns for agent_0 (Training): -503.5244140625\n",
      "Smoothed Returns for agent_1 (Training): -600.4608154296875\n",
      "Smoothed Returns for agent_2 (Frozen): 386.95928955078125\n",
      "Smoothed Returns for agent_3 (Frozen): 395.96612548828125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2.0518548488616943\n",
      "Policy Loss: -0.9573071599006653\n",
      "Old Approx KL: 0.0029608863405883312\n",
      "Approx KL: 5.0153053052781615e-06\n",
      "Clip Fraction: 0.0026041666666666665\n",
      "Explained Variance: 0.009563267230987549\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1200\n",
      "Episodic Return: [ -877.50616 -1314.5824    887.17487   890.8238 ]\n",
      "Smoothed Returns for agent_0 (Training): -800.3463134765625\n",
      "Smoothed Returns for agent_1 (Training): -1157.0147705078125\n",
      "Smoothed Returns for agent_2 (Frozen): 821.1300659179688\n",
      "Smoothed Returns for agent_3 (Frozen): 770.6339721679688\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 32.037925720214844\n",
      "Policy Loss: -7.744591236114502\n",
      "Old Approx KL: -0.017609596252441406\n",
      "Approx KL: 0.0001851490669650957\n",
      "Clip Fraction: 0.0613839291036129\n",
      "Explained Variance: 0.035410284996032715\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1210\n",
      "Episodic Return: [ -856.4704 -1265.8448   874.0025   879.864 ]\n",
      "Smoothed Returns for agent_0 (Training): -413.88018798828125\n",
      "Smoothed Returns for agent_1 (Training): -1277.377685546875\n",
      "Smoothed Returns for agent_2 (Frozen): 581.083984375\n",
      "Smoothed Returns for agent_3 (Frozen): 564.7958374023438\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 426.9779052734375\n",
      "Policy Loss: 19.974332809448242\n",
      "Old Approx KL: -0.0419355146586895\n",
      "Approx KL: 0.008043358102440834\n",
      "Clip Fraction: 0.058779762436946235\n",
      "Explained Variance: -0.16417765617370605\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1220\n",
      "Episodic Return: [ -866.6609 -1328.9857   882.3553   886.189 ]\n",
      "Smoothed Returns for agent_0 (Training): -419.20318603515625\n",
      "Smoothed Returns for agent_1 (Training): -1313.759033203125\n",
      "Smoothed Returns for agent_2 (Frozen): 586.7372436523438\n",
      "Smoothed Returns for agent_3 (Frozen): 566.2354125976562\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 13.779864311218262\n",
      "Policy Loss: 5.307431221008301\n",
      "Old Approx KL: -0.0031379973515868187\n",
      "Approx KL: 1.0286059477948584e-05\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.011036038398742676\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1230\n",
      "Episodic Return: [ -879.2436 -1331.362    892.7462   888.3808]\n",
      "Smoothed Returns for agent_0 (Training): -868.3644409179688\n",
      "Smoothed Returns for agent_1 (Training): -1306.6905517578125\n",
      "Smoothed Returns for agent_2 (Frozen): 885.8021240234375\n",
      "Smoothed Returns for agent_3 (Frozen): 883.3004150390625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 36.6766357421875\n",
      "Policy Loss: -8.428980827331543\n",
      "Old Approx KL: -0.04875551164150238\n",
      "Approx KL: 0.0034066950902342796\n",
      "Clip Fraction: 0.044456845770279564\n",
      "Explained Variance: 0.004421830177307129\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1240\n",
      "Episodic Return: [ -848.43994 -1191.0537    867.02167   876.11536]\n",
      "Smoothed Returns for agent_0 (Training): -830.2013549804688\n",
      "Smoothed Returns for agent_1 (Training): -1257.8560791015625\n",
      "Smoothed Returns for agent_2 (Frozen): 854.6224365234375\n",
      "Smoothed Returns for agent_3 (Frozen): 870.9449462890625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2.9785513877868652\n",
      "Policy Loss: -1.6210100650787354\n",
      "Old Approx KL: 0.0014911380130797625\n",
      "Approx KL: 4.174028435954824e-05\n",
      "Clip Fraction: 0.014322916666666666\n",
      "Explained Variance: -0.30130279064178467\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1250\n",
      "Episodic Return: [ -823.84033 -1216.9019    868.92316   849.61426]\n",
      "Smoothed Returns for agent_0 (Training): -826.22412109375\n",
      "Smoothed Returns for agent_1 (Training): -1244.804931640625\n",
      "Smoothed Returns for agent_2 (Frozen): 852.8084716796875\n",
      "Smoothed Returns for agent_3 (Frozen): 868.1478271484375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 96.31346893310547\n",
      "Policy Loss: -11.981438636779785\n",
      "Old Approx KL: -0.0036193304695189\n",
      "Approx KL: 1.6868114471435547e-05\n",
      "Clip Fraction: 0.010416666666666666\n",
      "Explained Variance: -0.05891764163970947\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1260\n",
      "Episodic Return: [ -875.53625 -1279.7032    890.526     887.58923]\n",
      "Smoothed Returns for agent_0 (Training): -855.2747192382812\n",
      "Smoothed Returns for agent_1 (Training): -1260.0487060546875\n",
      "Smoothed Returns for agent_2 (Frozen): 877.3193359375\n",
      "Smoothed Returns for agent_3 (Frozen): 874.1266479492188\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 62.36936950683594\n",
      "Policy Loss: 10.571420669555664\n",
      "Old Approx KL: 0.03220653533935547\n",
      "Approx KL: 0.0015111821703612804\n",
      "Clip Fraction: 0.048177083333333336\n",
      "Explained Variance: -0.21512556076049805\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1270\n",
      "Episodic Return: [  959.0109   2655.3242   -400.03995 -4103.168  ]\n",
      "Smoothed Returns for agent_0 (Training): -491.8873596191406\n",
      "Smoothed Returns for agent_1 (Training): -1031.5789794921875\n",
      "Smoothed Returns for agent_2 (Frozen): 653.0965576171875\n",
      "Smoothed Returns for agent_3 (Frozen): 419.8502502441406\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1540.8145751953125\n",
      "Policy Loss: -38.764060974121094\n",
      "Old Approx KL: 0.04931504651904106\n",
      "Approx KL: 0.017578620463609695\n",
      "Clip Fraction: 0.0641741082072258\n",
      "Explained Variance: 0.37660759687423706\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 1280\n",
      "Episodic Return: [ -358.20557  3007.7334    108.1548  -2987.6724 ]\n",
      "Smoothed Returns for agent_0 (Training): -128.33724975585938\n",
      "Smoothed Returns for agent_1 (Training): -182.902099609375\n",
      "Smoothed Returns for agent_2 (Frozen): -132.17120361328125\n",
      "Smoothed Returns for agent_3 (Frozen): 77.89765930175781\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1238.887939453125\n",
      "Policy Loss: -37.36002731323242\n",
      "Old Approx KL: 0.008259705267846584\n",
      "Approx KL: 0.0011269961250945926\n",
      "Clip Fraction: 0.0652901791036129\n",
      "Explained Variance: 0.25410860776901245\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1290\n",
      "Episodic Return: [ -845.40594   2036.2117      91.830795 -1636.3624  ]\n",
      "Smoothed Returns for agent_0 (Training): -444.56793212890625\n",
      "Smoothed Returns for agent_1 (Training): 369.84619140625\n",
      "Smoothed Returns for agent_2 (Frozen): -237.2029571533203\n",
      "Smoothed Returns for agent_3 (Frozen): 38.66210174560547\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2875.941650390625\n",
      "Policy Loss: 15.630708694458008\n",
      "Old Approx KL: 0.037863221019506454\n",
      "Approx KL: 0.013066786341369152\n",
      "Clip Fraction: 0.17801339675982794\n",
      "Explained Variance: 0.14902162551879883\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1300\n",
      "Episodic Return: [  748.278    2812.356   -3283.4348   -511.27924]\n",
      "Smoothed Returns for agent_0 (Training): -493.2816467285156\n",
      "Smoothed Returns for agent_1 (Training): 523.531982421875\n",
      "Smoothed Returns for agent_2 (Frozen): -312.55194091796875\n",
      "Smoothed Returns for agent_3 (Frozen): -152.20208740234375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 559.626708984375\n",
      "Policy Loss: 2.158372163772583\n",
      "Old Approx KL: 0.12434176355600357\n",
      "Approx KL: 0.018638696521520615\n",
      "Clip Fraction: 0.1532738134264946\n",
      "Explained Variance: 0.19095438718795776\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1310\n",
      "Episodic Return: [ -882.3283  -1255.2096    895.03754   892.90546]\n",
      "Smoothed Returns for agent_0 (Training): -499.3211975097656\n",
      "Smoothed Returns for agent_1 (Training): -174.55978393554688\n",
      "Smoothed Returns for agent_2 (Frozen): -22.020706176757812\n",
      "Smoothed Returns for agent_3 (Frozen): 296.7561950683594\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 24.691530227661133\n",
      "Policy Loss: 6.215865135192871\n",
      "Old Approx KL: 0.16391563415527344\n",
      "Approx KL: 0.013055469840765\n",
      "Clip Fraction: 0.3411458333333333\n",
      "Explained Variance: 0.27845263481140137\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1320\n",
      "Episodic Return: [ -848.0782  -1233.9227    864.96515   860.6382 ]\n",
      "Smoothed Returns for agent_0 (Training): -724.943115234375\n",
      "Smoothed Returns for agent_1 (Training): -872.4235229492188\n",
      "Smoothed Returns for agent_2 (Frozen): 659.3638916015625\n",
      "Smoothed Returns for agent_3 (Frozen): 667.6536254882812\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 110.08094024658203\n",
      "Policy Loss: 14.657563209533691\n",
      "Old Approx KL: 0.05730043351650238\n",
      "Approx KL: 0.013426287099719048\n",
      "Clip Fraction: 0.6542038731276989\n",
      "Explained Variance: 0.11532950401306152\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1330\n",
      "Episodic Return: [ 2960.1965  3086.4167 -5378.259  -1069.52  ]\n",
      "Smoothed Returns for agent_0 (Training): -222.136962890625\n",
      "Smoothed Returns for agent_1 (Training): 335.2794189453125\n",
      "Smoothed Returns for agent_2 (Frozen): -241.50527954101562\n",
      "Smoothed Returns for agent_3 (Frozen): -194.81698608398438\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2073.477294921875\n",
      "Policy Loss: -8.528581619262695\n",
      "Old Approx KL: 0.010457618162035942\n",
      "Approx KL: 0.0035995077341794968\n",
      "Clip Fraction: 0.23028274128834406\n",
      "Explained Variance: 0.20060855150222778\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1340\n",
      "Episodic Return: [ -811.33826  4522.143   -3883.7336   -566.84595]\n",
      "Smoothed Returns for agent_0 (Training): 878.6306762695312\n",
      "Smoothed Returns for agent_1 (Training): 2584.5634765625\n",
      "Smoothed Returns for agent_2 (Frozen): -1754.342041015625\n",
      "Smoothed Returns for agent_3 (Frozen): -2117.5029296875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 8747.3505859375\n",
      "Policy Loss: 120.66470336914062\n",
      "Old Approx KL: -0.01114130113273859\n",
      "Approx KL: 0.00031821217271499336\n",
      "Clip Fraction: 0.1227678582072258\n",
      "Explained Variance: 0.008206963539123535\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1350\n",
      "Episodic Return: [  235.70673  3702.7722  -3260.7878  -1252.0513 ]\n",
      "Smoothed Returns for agent_0 (Training): 521.68505859375\n",
      "Smoothed Returns for agent_1 (Training): 1976.458251953125\n",
      "Smoothed Returns for agent_2 (Frozen): -1236.072265625\n",
      "Smoothed Returns for agent_3 (Frozen): -1672.5062255859375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1863.2032470703125\n",
      "Policy Loss: 8.539027214050293\n",
      "Old Approx KL: -0.031808242201805115\n",
      "Approx KL: 0.001200386555865407\n",
      "Clip Fraction: 0.1032366082072258\n",
      "Explained Variance: 0.24019533395767212\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1360\n",
      "Episodic Return: [ -864.1863  -1127.4194    874.43384   881.23175]\n",
      "Smoothed Returns for agent_0 (Training): -638.429443359375\n",
      "Smoothed Returns for agent_1 (Training): -517.3796997070312\n",
      "Smoothed Returns for agent_2 (Frozen): 441.5730895996094\n",
      "Smoothed Returns for agent_3 (Frozen): 365.60699462890625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 6.063752174377441\n",
      "Policy Loss: 2.851443290710449\n",
      "Old Approx KL: 0.026044370606541634\n",
      "Approx KL: 0.00042711838614195585\n",
      "Clip Fraction: 0.04296875\n",
      "Explained Variance: -0.010707855224609375\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1370\n",
      "Episodic Return: [  -80.74644 -1077.0819    417.73227   360.35757]\n",
      "Smoothed Returns for agent_0 (Training): -792.0003662109375\n",
      "Smoothed Returns for agent_1 (Training): -940.3892822265625\n",
      "Smoothed Returns for agent_2 (Frozen): 750.367431640625\n",
      "Smoothed Returns for agent_3 (Frozen): 665.2503051757812\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 16.63829803466797\n",
      "Policy Loss: -5.10401725769043\n",
      "Old Approx KL: 0.0014984267763793468\n",
      "Approx KL: 1.3257777027320117e-05\n",
      "Clip Fraction: 0.0026041666666666665\n",
      "Explained Variance: 0.05617886781692505\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1380\n",
      "Episodic Return: [ -854.2505  -1046.6641    874.56036   875.17175]\n",
      "Smoothed Returns for agent_0 (Training): -643.4627685546875\n",
      "Smoothed Returns for agent_1 (Training): -301.90130615234375\n",
      "Smoothed Returns for agent_2 (Frozen): 200.27456665039062\n",
      "Smoothed Returns for agent_3 (Frozen): 380.8141174316406\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3.1191892623901367\n",
      "Policy Loss: -0.09905038774013519\n",
      "Old Approx KL: -0.010764667764306068\n",
      "Approx KL: 0.0006088785012252629\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.07689142227172852\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1390\n",
      "Episodic Return: [ 1797.4509 -1027.1945  -957.358  -1314.259 ]\n",
      "Smoothed Returns for agent_0 (Training): -63.206748962402344\n",
      "Smoothed Returns for agent_1 (Training): 684.2756958007812\n",
      "Smoothed Returns for agent_2 (Frozen): -403.0069885253906\n",
      "Smoothed Returns for agent_3 (Frozen): -719.0971069335938\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 23.7481746673584\n",
      "Policy Loss: -0.16847965121269226\n",
      "Old Approx KL: 0.024046558886766434\n",
      "Approx KL: 0.001523239305242896\n",
      "Clip Fraction: 0.09021577487389247\n",
      "Explained Variance: -0.050864458084106445\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1400\n",
      "Episodic Return: [ -841.86365 -1222.1201    873.38196   864.33777]\n",
      "Smoothed Returns for agent_0 (Training): -128.68185424804688\n",
      "Smoothed Returns for agent_1 (Training): 153.4200439453125\n",
      "Smoothed Returns for agent_2 (Frozen): 49.12555694580078\n",
      "Smoothed Returns for agent_3 (Frozen): -480.96368408203125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 103.93595123291016\n",
      "Policy Loss: -12.509379386901855\n",
      "Old Approx KL: 0.0010799680603668094\n",
      "Approx KL: 0.00015236651233863086\n",
      "Clip Fraction: 0.09802827487389247\n",
      "Explained Variance: -0.028311848640441895\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1410\n",
      "Episodic Return: [ -872.04785 -1146.6779    887.30884   884.536  ]\n",
      "Smoothed Returns for agent_0 (Training): -419.1922912597656\n",
      "Smoothed Returns for agent_1 (Training): -598.7559204101562\n",
      "Smoothed Returns for agent_2 (Frozen): 428.77728271484375\n",
      "Smoothed Returns for agent_3 (Frozen): 365.07147216796875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 22.047504425048828\n",
      "Policy Loss: -4.092406749725342\n",
      "Old Approx KL: -0.021824633702635765\n",
      "Approx KL: 0.0005221026367507875\n",
      "Clip Fraction: 0.4334077400465806\n",
      "Explained Variance: -0.2997748851776123\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 1420\n",
      "Episodic Return: [ -867.6076  -1195.569     874.42664   890.42804]\n",
      "Smoothed Returns for agent_0 (Training): -350.7993469238281\n",
      "Smoothed Returns for agent_1 (Training): -732.0870361328125\n",
      "Smoothed Returns for agent_2 (Frozen): 483.7811584472656\n",
      "Smoothed Returns for agent_3 (Frozen): 323.5356140136719\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 14.423738479614258\n",
      "Policy Loss: 4.895479679107666\n",
      "Old Approx KL: -0.0016825540224090219\n",
      "Approx KL: 0.00020112311176490039\n",
      "Clip Fraction: 0.049851191540559135\n",
      "Explained Variance: 0.3011478781700134\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1430\n",
      "Episodic Return: [ 1057.3108  2428.6523 -1466.7745 -2090.605 ]\n",
      "Smoothed Returns for agent_0 (Training): -111.13069915771484\n",
      "Smoothed Returns for agent_1 (Training): -727.1298828125\n",
      "Smoothed Returns for agent_2 (Frozen): 433.0518493652344\n",
      "Smoothed Returns for agent_3 (Frozen): 29.648670196533203\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 8218.046875\n",
      "Policy Loss: -108.32699584960938\n",
      "Old Approx KL: -0.006351811811327934\n",
      "Approx KL: 6.323201523628086e-05\n",
      "Clip Fraction: 0.022135416666666668\n",
      "Explained Variance: 0.23881512880325317\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1440\n",
      "Episodic Return: [ 2534.835   2308.0244 -1330.2314 -4280.6343]\n",
      "Smoothed Returns for agent_0 (Training): 1171.275390625\n",
      "Smoothed Returns for agent_1 (Training): 1161.532958984375\n",
      "Smoothed Returns for agent_2 (Frozen): -26.529369354248047\n",
      "Smoothed Returns for agent_3 (Frozen): -2609.288330078125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 7277.38427734375\n",
      "Policy Loss: -89.55220031738281\n",
      "Old Approx KL: 0.05993911623954773\n",
      "Approx KL: 0.018240725621581078\n",
      "Clip Fraction: 0.20014881218473116\n",
      "Explained Variance: 0.2196405529975891\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1450\n",
      "Episodic Return: [ 2913.7368    5834.724    -8559.52        30.239014]\n",
      "Smoothed Returns for agent_0 (Training): 2697.00341796875\n",
      "Smoothed Returns for agent_1 (Training): 3107.38134765625\n",
      "Smoothed Returns for agent_2 (Frozen): -2843.05126953125\n",
      "Smoothed Returns for agent_3 (Frozen): -3179.467529296875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2241.791748046875\n",
      "Policy Loss: -22.956148147583008\n",
      "Old Approx KL: 0.0205941554158926\n",
      "Approx KL: 0.001403187052346766\n",
      "Clip Fraction: 0.06287202487389247\n",
      "Explained Variance: 0.04299801588058472\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1460\n",
      "Episodic Return: [ 5454.564   1292.3147 -5933.319   -912.4782]\n",
      "Smoothed Returns for agent_0 (Training): 3510.936767578125\n",
      "Smoothed Returns for agent_1 (Training): 3744.16943359375\n",
      "Smoothed Returns for agent_2 (Frozen): -4226.1123046875\n",
      "Smoothed Returns for agent_3 (Frozen): -3303.021484375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 17524.716796875\n",
      "Policy Loss: -135.45114135742188\n",
      "Old Approx KL: 0.018013818189501762\n",
      "Approx KL: 0.06107700988650322\n",
      "Clip Fraction: 0.1713169664144516\n",
      "Explained Variance: -0.010030508041381836\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1470\n",
      "Episodic Return: [ 1761.055   1282.4427  -669.2703 -3062.7507]\n",
      "Smoothed Returns for agent_0 (Training): 1702.480224609375\n",
      "Smoothed Returns for agent_1 (Training): 1675.300537109375\n",
      "Smoothed Returns for agent_2 (Frozen): -1145.85888671875\n",
      "Smoothed Returns for agent_3 (Frozen): -2529.6669921875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 808.3677978515625\n",
      "Policy Loss: 16.90073013305664\n",
      "Old Approx KL: -0.013097967952489853\n",
      "Approx KL: 0.00019802367023658007\n",
      "Clip Fraction: 0.10305059577027957\n",
      "Explained Variance: -0.0189359188079834\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1480\n",
      "Episodic Return: [ -874.6752  -1204.5558    885.83093   889.5686 ]\n",
      "Smoothed Returns for agent_0 (Training): -505.2359924316406\n",
      "Smoothed Returns for agent_1 (Training): -715.2782592773438\n",
      "Smoothed Returns for agent_2 (Frozen): 567.218017578125\n",
      "Smoothed Returns for agent_3 (Frozen): 353.8941345214844\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 6.5185322761535645\n",
      "Policy Loss: -3.456942319869995\n",
      "Old Approx KL: -0.03183896467089653\n",
      "Approx KL: 0.002594973426312208\n",
      "Clip Fraction: 0.053757441540559135\n",
      "Explained Variance: 0.1848812699317932\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1490\n",
      "Episodic Return: [ -874.33765 -1122.3127    889.56854   885.2617 ]\n",
      "Smoothed Returns for agent_0 (Training): -653.1223754882812\n",
      "Smoothed Returns for agent_1 (Training): -1065.5369873046875\n",
      "Smoothed Returns for agent_2 (Frozen): 603.1795654296875\n",
      "Smoothed Returns for agent_3 (Frozen): 787.2169799804688\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 20.540592193603516\n",
      "Policy Loss: -6.1076979637146\n",
      "Old Approx KL: -0.002285003662109375\n",
      "Approx KL: 2.7528833015821874e-05\n",
      "Clip Fraction: 0.020833333333333332\n",
      "Explained Variance: 0.05749976634979248\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1500\n",
      "Episodic Return: [ -866.2541  -1113.9432    884.53625   880.3559 ]\n",
      "Smoothed Returns for agent_0 (Training): -356.37103271484375\n",
      "Smoothed Returns for agent_1 (Training): -1054.46337890625\n",
      "Smoothed Returns for agent_2 (Frozen): 493.9839782714844\n",
      "Smoothed Returns for agent_3 (Frozen): 497.00616455078125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1.9291597604751587\n",
      "Policy Loss: -1.9422204494476318\n",
      "Old Approx KL: -0.008231435902416706\n",
      "Approx KL: 0.0013870341936126351\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.08445155620574951\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1510\n",
      "Episodic Return: [-349.8523     51.938446 -225.95149    26.108105]\n",
      "Smoothed Returns for agent_0 (Training): -259.8797302246094\n",
      "Smoothed Returns for agent_1 (Training): -712.934814453125\n",
      "Smoothed Returns for agent_2 (Frozen): 364.92596435546875\n",
      "Smoothed Returns for agent_3 (Frozen): 221.55709838867188\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 146.51626586914062\n",
      "Policy Loss: -6.318003177642822\n",
      "Old Approx KL: 0.025379931554198265\n",
      "Approx KL: 0.0014110974734649062\n",
      "Clip Fraction: 0.035528274873892464\n",
      "Explained Variance: 0.4213826656341553\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1520\n",
      "Episodic Return: [ -852.24426 -1337.1324    876.24304   871.8569 ]\n",
      "Smoothed Returns for agent_0 (Training): -462.04632568359375\n",
      "Smoothed Returns for agent_1 (Training): -619.0940551757812\n",
      "Smoothed Returns for agent_2 (Frozen): 483.2377014160156\n",
      "Smoothed Returns for agent_3 (Frozen): 239.52587890625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 4.322296142578125\n",
      "Policy Loss: 1.3567620515823364\n",
      "Old Approx KL: 0.03560229763388634\n",
      "Approx KL: 0.0013573425821959972\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: 0.005370199680328369\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1530\n",
      "Episodic Return: [ -879.03094 -1168.135     888.63196   888.3415 ]\n",
      "Smoothed Returns for agent_0 (Training): -150.05038452148438\n",
      "Smoothed Returns for agent_1 (Training): -896.5914306640625\n",
      "Smoothed Returns for agent_2 (Frozen): 306.4838562011719\n",
      "Smoothed Returns for agent_3 (Frozen): 199.85232543945312\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2.269620180130005\n",
      "Policy Loss: -1.6967270374298096\n",
      "Old Approx KL: 0.004458631854504347\n",
      "Approx KL: 0.00020901646348647773\n",
      "Clip Fraction: 0.0013020833333333333\n",
      "Explained Variance: 0.15537035465240479\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1540\n",
      "Episodic Return: [ -875.74805 -1177.7654    887.0093    884.8269 ]\n",
      "Smoothed Returns for agent_0 (Training): 114.4012222290039\n",
      "Smoothed Returns for agent_1 (Training): -699.8509521484375\n",
      "Smoothed Returns for agent_2 (Frozen): -121.6457290649414\n",
      "Smoothed Returns for agent_3 (Frozen): 143.45968627929688\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 17.37140655517578\n",
      "Policy Loss: -5.035015106201172\n",
      "Old Approx KL: 0.05262838304042816\n",
      "Approx KL: 0.00669831084087491\n",
      "Clip Fraction: 0.3216145895421505\n",
      "Explained Variance: -0.32998645305633545\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1550\n",
      "Episodic Return: [ 6396.4395  1801.7816 -7376.0786  -714.9346]\n",
      "Smoothed Returns for agent_0 (Training): 1471.146484375\n",
      "Smoothed Returns for agent_1 (Training): 1310.796142578125\n",
      "Smoothed Returns for agent_2 (Frozen): -1873.26953125\n",
      "Smoothed Returns for agent_3 (Frozen): -1364.8968505859375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 4716.1943359375\n",
      "Policy Loss: -64.40518188476562\n",
      "Old Approx KL: -0.011685372330248356\n",
      "Approx KL: 0.00041822026832960546\n",
      "Clip Fraction: 0.07849702487389247\n",
      "Explained Variance: 0.044139564037323\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 1560\n",
      "Episodic Return: [ 2471.2822  3791.0334 -4313.1787 -2575.1843]\n",
      "Smoothed Returns for agent_0 (Training): 3536.219482421875\n",
      "Smoothed Returns for agent_1 (Training): 3723.66845703125\n",
      "Smoothed Returns for agent_2 (Frozen): -3441.180419921875\n",
      "Smoothed Returns for agent_3 (Frozen): -4170.81640625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1219.390625\n",
      "Policy Loss: -36.63357162475586\n",
      "Old Approx KL: -0.05380242317914963\n",
      "Approx KL: 0.0056876796297729015\n",
      "Clip Fraction: 0.09691220397750537\n",
      "Explained Variance: 0.3733801245689392\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1570\n",
      "Episodic Return: [ 4740.1855  3250.8499 -6149.3413 -2126.8772]\n",
      "Smoothed Returns for agent_0 (Training): 4441.58984375\n",
      "Smoothed Returns for agent_1 (Training): 4338.43017578125\n",
      "Smoothed Returns for agent_2 (Frozen): -4631.01611328125\n",
      "Smoothed Returns for agent_3 (Frozen): -4474.802734375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 90.97383880615234\n",
      "Policy Loss: -0.305184930562973\n",
      "Old Approx KL: 0.00026716507272794843\n",
      "Approx KL: 2.9802324661432067e-06\n",
      "Clip Fraction: 0.029947916666666668\n",
      "Explained Variance: 0.2504613399505615\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1580\n",
      "Episodic Return: [ 4975.4463  5695.2104 -2930.196  -8368.958 ]\n",
      "Smoothed Returns for agent_0 (Training): 4604.7529296875\n",
      "Smoothed Returns for agent_1 (Training): 4310.21142578125\n",
      "Smoothed Returns for agent_2 (Frozen): -5054.8603515625\n",
      "Smoothed Returns for agent_3 (Frozen): -4380.138671875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3141.391357421875\n",
      "Policy Loss: 9.045883178710938\n",
      "Old Approx KL: 0.08893922716379166\n",
      "Approx KL: 0.013161812908947468\n",
      "Clip Fraction: 0.13485863308111826\n",
      "Explained Variance: 0.2806902527809143\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1590\n",
      "Episodic Return: [ 4938.9355  4607.0327 -9466.511   -668.9672]\n",
      "Smoothed Returns for agent_0 (Training): 4599.6552734375\n",
      "Smoothed Returns for agent_1 (Training): 4321.72998046875\n",
      "Smoothed Returns for agent_2 (Frozen): -4943.0244140625\n",
      "Smoothed Returns for agent_3 (Frozen): -4474.17724609375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3273.0537109375\n",
      "Policy Loss: -66.67630004882812\n",
      "Old Approx KL: 0.1479284167289734\n",
      "Approx KL: 0.06262383610010147\n",
      "Clip Fraction: 0.06398809577027957\n",
      "Explained Variance: 0.13337641954421997\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1600\n",
      "Episodic Return: [ 4496.3633   4837.059   -9101.241    -270.52478]\n",
      "Smoothed Returns for agent_0 (Training): 4579.48828125\n",
      "Smoothed Returns for agent_1 (Training): 4509.1201171875\n",
      "Smoothed Returns for agent_2 (Frozen): -5381.25390625\n",
      "Smoothed Returns for agent_3 (Frozen): -4060.98486328125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 475.16241455078125\n",
      "Policy Loss: -26.553014755249023\n",
      "Old Approx KL: 0.0005666188080795109\n",
      "Approx KL: 1.6433853033959167e-06\n",
      "Clip Fraction: 0.014322916666666666\n",
      "Explained Variance: -0.2283647060394287\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1610\n",
      "Episodic Return: [ 4897.6426   4297.8975   -460.38956 -8737.888  ]\n",
      "Smoothed Returns for agent_0 (Training): 4541.578125\n",
      "Smoothed Returns for agent_1 (Training): 4653.30078125\n",
      "Smoothed Returns for agent_2 (Frozen): -4072.839111328125\n",
      "Smoothed Returns for agent_3 (Frozen): -5354.4013671875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1662.086181640625\n",
      "Policy Loss: -38.484710693359375\n",
      "Old Approx KL: 0.030682088807225227\n",
      "Approx KL: 0.0036539521533995867\n",
      "Clip Fraction: 0.14025297885139784\n",
      "Explained Variance: 0.1585468053817749\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1620\n",
      "Episodic Return: [ 6010.2456  -1003.80304 -4089.8093  -3916.443  ]\n",
      "Smoothed Returns for agent_0 (Training): 4459.58544921875\n",
      "Smoothed Returns for agent_1 (Training): 4179.74609375\n",
      "Smoothed Returns for agent_2 (Frozen): -2964.36865234375\n",
      "Smoothed Returns for agent_3 (Frozen): -6050.94970703125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 414.7698974609375\n",
      "Policy Loss: 27.21893882751465\n",
      "Old Approx KL: 0.00885772705078125\n",
      "Approx KL: 6.268706056289375e-05\n",
      "Clip Fraction: 0.06268601243694623\n",
      "Explained Variance: -0.07039093971252441\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1630\n",
      "Episodic Return: [ -873.32263 -1118.8918    889.1089    883.8627 ]\n",
      "Smoothed Returns for agent_0 (Training): 3671.93505859375\n",
      "Smoothed Returns for agent_1 (Training): 3634.12060546875\n",
      "Smoothed Returns for agent_2 (Frozen): -2952.5771484375\n",
      "Smoothed Returns for agent_3 (Frozen): -4832.1435546875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 133.59268188476562\n",
      "Policy Loss: -11.650668144226074\n",
      "Old Approx KL: -0.0015844617737457156\n",
      "Approx KL: 0.0007077966583892703\n",
      "Clip Fraction: 0.09021577487389247\n",
      "Explained Variance: -0.2184281349182129\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1640\n",
      "Episodic Return: [ 1704.9135  1700.9121 -3785.8262   337.2479]\n",
      "Smoothed Returns for agent_0 (Training): 2798.50732421875\n",
      "Smoothed Returns for agent_1 (Training): 2451.391845703125\n",
      "Smoothed Returns for agent_2 (Frozen): -2793.38671875\n",
      "Smoothed Returns for agent_3 (Frozen): -2868.63720703125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1354.918212890625\n",
      "Policy Loss: -37.7123908996582\n",
      "Old Approx KL: 0.01524646021425724\n",
      "Approx KL: 0.0015799233224242926\n",
      "Clip Fraction: 0.0943080373108387\n",
      "Explained Variance: 0.4024772644042969\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1650\n",
      "Episodic Return: [ -878.8671  -1270.1395    889.3274    889.27454]\n",
      "Smoothed Returns for agent_0 (Training): 2410.48974609375\n",
      "Smoothed Returns for agent_1 (Training): 1351.334716796875\n",
      "Smoothed Returns for agent_2 (Frozen): -2837.3984375\n",
      "Smoothed Returns for agent_3 (Frozen): -1303.691650390625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 703.3390502929688\n",
      "Policy Loss: -33.81671142578125\n",
      "Old Approx KL: 0.024552686139941216\n",
      "Approx KL: 0.008122563362121582\n",
      "Clip Fraction: 0.08389137064417203\n",
      "Explained Variance: -0.14584243297576904\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1660\n",
      "Episodic Return: [ -902.7093  -1151.5724    907.911     919.77356]\n",
      "Smoothed Returns for agent_0 (Training): 1810.682373046875\n",
      "Smoothed Returns for agent_1 (Training): 998.2946166992188\n",
      "Smoothed Returns for agent_2 (Frozen): -1907.1058349609375\n",
      "Smoothed Returns for agent_3 (Frozen): -1248.8172607421875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 4.910192966461182\n",
      "Policy Loss: -2.8604533672332764\n",
      "Old Approx KL: -0.00440107099711895\n",
      "Approx KL: 1.0541507435846142e-05\n",
      "Clip Fraction: 0.043340774873892464\n",
      "Explained Variance: -0.2297196388244629\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1670\n",
      "Episodic Return: [ 1526.1821  -1152.877    -784.9552   -568.25134]\n",
      "Smoothed Returns for agent_0 (Training): 907.3438720703125\n",
      "Smoothed Returns for agent_1 (Training): 84.7217788696289\n",
      "Smoothed Returns for agent_2 (Frozen): -724.516845703125\n",
      "Smoothed Returns for agent_3 (Frozen): -811.478515625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 201.2826690673828\n",
      "Policy Loss: 19.75166893005371\n",
      "Old Approx KL: -0.03245721757411957\n",
      "Approx KL: 0.002016825368627906\n",
      "Clip Fraction: 0.09802827487389247\n",
      "Explained Variance: -0.03842127323150635\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1680\n",
      "Episodic Return: [ -865.6365  -1167.3394    879.2837    876.83936]\n",
      "Smoothed Returns for agent_0 (Training): 297.626220703125\n",
      "Smoothed Returns for agent_1 (Training): -403.14483642578125\n",
      "Smoothed Returns for agent_2 (Frozen): -281.09210205078125\n",
      "Smoothed Returns for agent_3 (Frozen): -80.00312805175781\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 21.52806854248047\n",
      "Policy Loss: 5.4890241622924805\n",
      "Old Approx KL: -0.026090623810887337\n",
      "Approx KL: 0.003293735673651099\n",
      "Clip Fraction: 0.024925595770279568\n",
      "Explained Variance: -0.32420778274536133\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1690\n",
      "Episodic Return: [ -852.138  -1135.2008   866.6315   868.0602]\n",
      "Smoothed Returns for agent_0 (Training): -383.49249267578125\n",
      "Smoothed Returns for agent_1 (Training): -689.9910278320312\n",
      "Smoothed Returns for agent_2 (Frozen): 478.52838134765625\n",
      "Smoothed Returns for agent_3 (Frozen): 286.05780029296875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 48.083885192871094\n",
      "Policy Loss: 6.225214004516602\n",
      "Old Approx KL: -0.0009821483399719\n",
      "Approx KL: 1.6178404393940582e-06\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.36396586894989014\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 1700\n",
      "Episodic Return: [ -850.7756  -1224.055     869.32294   869.01587]\n",
      "Smoothed Returns for agent_0 (Training): -839.6887817382812\n",
      "Smoothed Returns for agent_1 (Training): -1164.061279296875\n",
      "Smoothed Returns for agent_2 (Frozen): 849.2918090820312\n",
      "Smoothed Returns for agent_3 (Frozen): 805.330322265625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 10.582571983337402\n",
      "Policy Loss: -2.896498680114746\n",
      "Old Approx KL: 0.0010858264286071062\n",
      "Approx KL: 8.344650836988876e-07\n",
      "Clip Fraction: 0.0013020833333333333\n",
      "Explained Variance: -0.006035804748535156\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1710\n",
      "Episodic Return: [ -852.483   -1304.524     868.26917   871.566  ]\n",
      "Smoothed Returns for agent_0 (Training): -147.32418823242188\n",
      "Smoothed Returns for agent_1 (Training): -552.9990234375\n",
      "Smoothed Returns for agent_2 (Frozen): 20.17669105529785\n",
      "Smoothed Returns for agent_3 (Frozen): 369.05926513671875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 7.820875644683838\n",
      "Policy Loss: 3.641035556793213\n",
      "Old Approx KL: 0.000553131103515625\n",
      "Approx KL: 4.512923226229759e-07\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.009453892707824707\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1720\n",
      "Episodic Return: [ 3058.7039   -156.25336 -2654.6077  -1934.1187 ]\n",
      "Smoothed Returns for agent_0 (Training): 186.3243865966797\n",
      "Smoothed Returns for agent_1 (Training): -368.98834228515625\n",
      "Smoothed Returns for agent_2 (Frozen): -405.5752868652344\n",
      "Smoothed Returns for agent_3 (Frozen): 286.2182312011719\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 4.314840316772461\n",
      "Policy Loss: -2.581974983215332\n",
      "Old Approx KL: 0.02862446755170822\n",
      "Approx KL: 0.0018860017880797386\n",
      "Clip Fraction: 0.1568080373108387\n",
      "Explained Variance: -0.8510075807571411\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1730\n",
      "Episodic Return: [ 3040.4844   2096.6016   -412.28415 -4661.379  ]\n",
      "Smoothed Returns for agent_0 (Training): 448.81182861328125\n",
      "Smoothed Returns for agent_1 (Training): 16.119140625\n",
      "Smoothed Returns for agent_2 (Frozen): -339.8499450683594\n",
      "Smoothed Returns for agent_3 (Frozen): -433.49603271484375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3027.33642578125\n",
      "Policy Loss: 38.5203971862793\n",
      "Old Approx KL: 0.19898514449596405\n",
      "Approx KL: 0.05432357266545296\n",
      "Clip Fraction: 0.23716518407066664\n",
      "Explained Variance: -0.03838241100311279\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1740\n",
      "Episodic Return: [ 1413.2985     1382.2198        5.7159834 -2757.5598   ]\n",
      "Smoothed Returns for agent_0 (Training): 821.0831909179688\n",
      "Smoothed Returns for agent_1 (Training): 656.3966064453125\n",
      "Smoothed Returns for agent_2 (Frozen): -595.884033203125\n",
      "Smoothed Returns for agent_3 (Frozen): -1138.1588134765625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 338.20556640625\n",
      "Policy Loss: -15.679161071777344\n",
      "Old Approx KL: -0.33426907658576965\n",
      "Approx KL: 0.20492686331272125\n",
      "Clip Fraction: 0.3167782798409462\n",
      "Explained Variance: 0.01672893762588501\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1750\n",
      "Episodic Return: [ 3183.3337    82.5444 -2395.6838 -1472.9355]\n",
      "Smoothed Returns for agent_0 (Training): 1291.9227294921875\n",
      "Smoothed Returns for agent_1 (Training): 1106.5753173828125\n",
      "Smoothed Returns for agent_2 (Frozen): -777.7443237304688\n",
      "Smoothed Returns for agent_3 (Frozen): -1904.260498046875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 423.7802429199219\n",
      "Policy Loss: 28.743675231933594\n",
      "Old Approx KL: 0.06926274299621582\n",
      "Approx KL: 0.020113443955779076\n",
      "Clip Fraction: 0.3417038768529892\n",
      "Explained Variance: -0.21045148372650146\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1760\n",
      "Episodic Return: [ -860.13715 -1317.4441    869.4993    870.9891 ]\n",
      "Smoothed Returns for agent_0 (Training): 3327.595703125\n",
      "Smoothed Returns for agent_1 (Training): 1417.80126953125\n",
      "Smoothed Returns for agent_2 (Frozen): -2694.034423828125\n",
      "Smoothed Returns for agent_3 (Frozen): -3079.962890625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 84.13761138916016\n",
      "Policy Loss: -12.770895957946777\n",
      "Old Approx KL: -0.009316990152001381\n",
      "Approx KL: 5.9894155128858984e-05\n",
      "Clip Fraction: 0.0078125\n",
      "Explained Variance: -0.2558596134185791\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1770\n",
      "Episodic Return: [ 7099.4673  -977.792  -4394.2393 -3611.236 ]\n",
      "Smoothed Returns for agent_0 (Training): 6089.0146484375\n",
      "Smoothed Returns for agent_1 (Training): 64.11041259765625\n",
      "Smoothed Returns for agent_2 (Frozen): -4535.61767578125\n",
      "Smoothed Returns for agent_3 (Frozen): -3879.060546875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 5.403066158294678\n",
      "Policy Loss: 3.414259910583496\n",
      "Old Approx KL: -0.01894739829003811\n",
      "Approx KL: 0.0006095597054809332\n",
      "Clip Fraction: 0.0078125\n",
      "Explained Variance: -0.007534146308898926\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1780\n",
      "Episodic Return: [ 3656.8708  -1117.0186    -14.91082 -3336.1528 ]\n",
      "Smoothed Returns for agent_0 (Training): 6697.9619140625\n",
      "Smoothed Returns for agent_1 (Training): -947.2718505859375\n",
      "Smoothed Returns for agent_2 (Frozen): -4067.699951171875\n",
      "Smoothed Returns for agent_3 (Frozen): -3835.53271484375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 13.802711486816406\n",
      "Policy Loss: 0.7642346024513245\n",
      "Old Approx KL: 0.017511164769530296\n",
      "Approx KL: 0.0006203310913406312\n",
      "Clip Fraction: 0.006510416666666667\n",
      "Explained Variance: 0.11116337776184082\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1790\n",
      "Episodic Return: [ 7772.673   -994.8021 -4078.103  -5077.2896]\n",
      "Smoothed Returns for agent_0 (Training): 6303.0498046875\n",
      "Smoothed Returns for agent_1 (Training): -688.7650146484375\n",
      "Smoothed Returns for agent_2 (Frozen): -3512.90576171875\n",
      "Smoothed Returns for agent_3 (Frozen): -3904.98779296875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 35.890506744384766\n",
      "Policy Loss: -3.9037652015686035\n",
      "Old Approx KL: -0.005300113465636969\n",
      "Approx KL: 0.0004312907112762332\n",
      "Clip Fraction: 0.11774553855260213\n",
      "Explained Variance: -0.1666574478149414\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1800\n",
      "Episodic Return: [ 6279.377  -1030.9109 -5237.635  -3606.2039]\n",
      "Smoothed Returns for agent_0 (Training): 6682.3515625\n",
      "Smoothed Returns for agent_1 (Training): -674.0858154296875\n",
      "Smoothed Returns for agent_2 (Frozen): -3699.966064453125\n",
      "Smoothed Returns for agent_3 (Frozen): -4579.5400390625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 42.40574645996094\n",
      "Policy Loss: -3.1040732860565186\n",
      "Old Approx KL: 0.0017022406682372093\n",
      "Approx KL: 6.863049202365801e-05\n",
      "Clip Fraction: 0.21800595397750536\n",
      "Explained Variance: -0.1407262086868286\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1810\n",
      "Episodic Return: [ 8062.7837  -738.0626 -6675.0044 -2948.4385]\n",
      "Smoothed Returns for agent_0 (Training): 6734.8623046875\n",
      "Smoothed Returns for agent_1 (Training): -733.683837890625\n",
      "Smoothed Returns for agent_2 (Frozen): -3997.889404296875\n",
      "Smoothed Returns for agent_3 (Frozen): -4330.302734375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 82.97279357910156\n",
      "Policy Loss: 10.519278526306152\n",
      "Old Approx KL: 0.002409867011010647\n",
      "Approx KL: 0.0012250032741576433\n",
      "Clip Fraction: 0.09523809577027957\n",
      "Explained Variance: 0.32416218519210815\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1820\n",
      "Episodic Return: [  377.602   -1285.2614   -260.1345    628.84534]\n",
      "Smoothed Returns for agent_0 (Training): 6421.75732421875\n",
      "Smoothed Returns for agent_1 (Training): -855.8689575195312\n",
      "Smoothed Returns for agent_2 (Frozen): -3530.115966796875\n",
      "Smoothed Returns for agent_3 (Frozen): -4184.4443359375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 22.269489288330078\n",
      "Policy Loss: -5.296757698059082\n",
      "Old Approx KL: -0.006723472382873297\n",
      "Approx KL: 0.000556222046725452\n",
      "Clip Fraction: 0.010416666666666666\n",
      "Explained Variance: 0.035468876361846924\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1830\n",
      "Episodic Return: [ 7937.4595 -1002.3346 -4669.3267 -4680.18  ]\n",
      "Smoothed Returns for agent_0 (Training): 6268.78369140625\n",
      "Smoothed Returns for agent_1 (Training): -1075.119873046875\n",
      "Smoothed Returns for agent_2 (Frozen): -3238.796875\n",
      "Smoothed Returns for agent_3 (Frozen): -4129.4375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 11.1408109664917\n",
      "Policy Loss: 2.6565606594085693\n",
      "Old Approx KL: 0.08364459872245789\n",
      "Approx KL: 0.007956096902489662\n",
      "Clip Fraction: 0.2042410746216774\n",
      "Explained Variance: -0.3510873317718506\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 1840\n",
      "Episodic Return: [ 7711.117   -980.1463 -3305.5596 -5679.7236]\n",
      "Smoothed Returns for agent_0 (Training): 7062.2470703125\n",
      "Smoothed Returns for agent_1 (Training): -1044.681884765625\n",
      "Smoothed Returns for agent_2 (Frozen): -4079.658203125\n",
      "Smoothed Returns for agent_3 (Frozen): -4333.5087890625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 21.56920051574707\n",
      "Policy Loss: -6.666763782501221\n",
      "Old Approx KL: -0.012866701930761337\n",
      "Approx KL: 0.0015240652719512582\n",
      "Clip Fraction: 0.06789434577027957\n",
      "Explained Variance: 0.010694503784179688\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1850\n",
      "Episodic Return: [ 7774.3433 -1062.1012 -5742.2627 -2952.9028]\n",
      "Smoothed Returns for agent_0 (Training): 7663.97119140625\n",
      "Smoothed Returns for agent_1 (Training): -1017.9275512695312\n",
      "Smoothed Returns for agent_2 (Frozen): -4823.66748046875\n",
      "Smoothed Returns for agent_3 (Frozen): -4185.419921875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 32.30327224731445\n",
      "Policy Loss: -7.234496116638184\n",
      "Old Approx KL: -0.0048266141675412655\n",
      "Approx KL: 0.0002218910667579621\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.12686645984649658\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1860\n",
      "Episodic Return: [ 7361.2417 -1039.097  -5172.2285 -3565.9102]\n",
      "Smoothed Returns for agent_0 (Training): 7489.36083984375\n",
      "Smoothed Returns for agent_1 (Training): -793.6041259765625\n",
      "Smoothed Returns for agent_2 (Frozen): -4556.15576171875\n",
      "Smoothed Returns for agent_3 (Frozen): -4164.58447265625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 6.586547374725342\n",
      "Policy Loss: -1.460010290145874\n",
      "Old Approx KL: 0.003032888751477003\n",
      "Approx KL: 9.051391316461377e-06\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: -0.16109251976013184\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1870\n",
      "Episodic Return: [ 7560.6646  -1020.24207 -2870.2783  -4641.998  ]\n",
      "Smoothed Returns for agent_0 (Training): 7344.9052734375\n",
      "Smoothed Returns for agent_1 (Training): -535.10888671875\n",
      "Smoothed Returns for agent_2 (Frozen): -4244.22802734375\n",
      "Smoothed Returns for agent_3 (Frozen): -4418.75634765625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 49.48796081542969\n",
      "Policy Loss: -9.870983123779297\n",
      "Old Approx KL: 0.008341176435351372\n",
      "Approx KL: 0.0008124454179778695\n",
      "Clip Fraction: 0.01953125\n",
      "Explained Variance: -0.024225950241088867\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1880\n",
      "Episodic Return: [ 8854.217    -932.07605 -6528.5986  -3670.1357 ]\n",
      "Smoothed Returns for agent_0 (Training): 7041.9560546875\n",
      "Smoothed Returns for agent_1 (Training): -135.4974822998047\n",
      "Smoothed Returns for agent_2 (Frozen): -4492.50341796875\n",
      "Smoothed Returns for agent_3 (Frozen): -4071.50732421875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 0.33894339203834534\n",
      "Policy Loss: 0.2618260979652405\n",
      "Old Approx KL: -0.0018388204043731093\n",
      "Approx KL: 0.004749383311718702\n",
      "Clip Fraction: 0.4324776853124301\n",
      "Explained Variance: 0.03751260042190552\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1890\n",
      "Episodic Return: [ -851.42017 -1328.0393    863.13184   870.49536]\n",
      "Smoothed Returns for agent_0 (Training): 6060.5771484375\n",
      "Smoothed Returns for agent_1 (Training): -447.3929138183594\n",
      "Smoothed Returns for agent_2 (Frozen): -3847.848388671875\n",
      "Smoothed Returns for agent_3 (Frozen): -3458.416015625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 1.4328752756118774\n",
      "Policy Loss: 0.6043923497200012\n",
      "Old Approx KL: 0.0020494461059570312\n",
      "Approx KL: 2.6992390758096008e-06\n",
      "Clip Fraction: 0.0078125\n",
      "Explained Variance: -0.01443493366241455\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1900\n",
      "Episodic Return: [ 7256.6187 -1135.9465 -5848.1104 -1885.1324]\n",
      "Smoothed Returns for agent_0 (Training): 5663.5791015625\n",
      "Smoothed Returns for agent_1 (Training): -1120.711669921875\n",
      "Smoothed Returns for agent_2 (Frozen): -3693.896484375\n",
      "Smoothed Returns for agent_3 (Frozen): -2913.1728515625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 5.506910800933838\n",
      "Policy Loss: 0.3866037130355835\n",
      "Old Approx KL: 0.03470250591635704\n",
      "Approx KL: 0.0034874933771789074\n",
      "Clip Fraction: 0.2652529788513978\n",
      "Explained Variance: -0.12020516395568848\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1910\n",
      "Episodic Return: [ 8644.771  -1041.2958 -5435.9214 -5182.8823]\n",
      "Smoothed Returns for agent_0 (Training): 5275.44140625\n",
      "Smoothed Returns for agent_1 (Training): -1106.2938232421875\n",
      "Smoothed Returns for agent_2 (Frozen): -3380.408203125\n",
      "Smoothed Returns for agent_3 (Frozen): -2783.632080078125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 2.2591121196746826\n",
      "Policy Loss: 1.673408031463623\n",
      "Old Approx KL: -0.003685201983898878\n",
      "Approx KL: 4.0420465666102245e-05\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: 0.01324397325515747\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1920\n",
      "Episodic Return: [ 8389.376  -1031.1241 -6162.147  -3842.1667]\n",
      "Smoothed Returns for agent_0 (Training): 5805.47802734375\n",
      "Smoothed Returns for agent_1 (Training): -1084.46728515625\n",
      "Smoothed Returns for agent_2 (Frozen): -3236.007080078125\n",
      "Smoothed Returns for agent_3 (Frozen): -3535.846435546875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 22.217363357543945\n",
      "Policy Loss: -5.507856369018555\n",
      "Old Approx KL: -0.0046781813725829124\n",
      "Approx KL: 0.00017832007142715156\n",
      "Clip Fraction: 0.014322916666666666\n",
      "Explained Variance: -0.022519469261169434\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1930\n",
      "Episodic Return: [ 7039.1553 -1151.6642 -4999.423  -3227.6306]\n",
      "Smoothed Returns for agent_0 (Training): 6830.5068359375\n",
      "Smoothed Returns for agent_1 (Training): -833.8511962890625\n",
      "Smoothed Returns for agent_2 (Frozen): -3755.048095703125\n",
      "Smoothed Returns for agent_3 (Frozen): -4316.1845703125\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 32.8836784362793\n",
      "Policy Loss: 8.105208396911621\n",
      "Old Approx KL: -0.008076736703515053\n",
      "Approx KL: 0.0008398720528930426\n",
      "Clip Fraction: 0.02734375\n",
      "Explained Variance: 0.00618666410446167\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1940\n",
      "Episodic Return: [ -349.19797 -1298.3446    466.33932   673.0525 ]\n",
      "Smoothed Returns for agent_0 (Training): 4834.7724609375\n",
      "Smoothed Returns for agent_1 (Training): -897.4631958007812\n",
      "Smoothed Returns for agent_2 (Frozen): -2485.665283203125\n",
      "Smoothed Returns for agent_3 (Frozen): -2989.17333984375\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 14.551729202270508\n",
      "Policy Loss: -5.207727432250977\n",
      "Old Approx KL: -0.0015439988346770406\n",
      "Approx KL: 0.00011963504221057519\n",
      "Clip Fraction: 0.0013020833333333333\n",
      "Explained Variance: 0.022005438804626465\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1950\n",
      "Episodic Return: [ -848.9377  -1304.3331    858.91345   861.878  ]\n",
      "Smoothed Returns for agent_0 (Training): 1504.8480224609375\n",
      "Smoothed Returns for agent_1 (Training): -1249.4034423828125\n",
      "Smoothed Returns for agent_2 (Frozen): -527.6304321289062\n",
      "Smoothed Returns for agent_3 (Frozen): -690.8631591796875\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 6.912846565246582\n",
      "Policy Loss: 2.3474743366241455\n",
      "Old Approx KL: -0.00011498587991809472\n",
      "Approx KL: 2.043587983280304e-07\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: 0.07435393333435059\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1960\n",
      "Episodic Return: [ -862.25354 -1271.0049    875.3145    887.5443 ]\n",
      "Smoothed Returns for agent_0 (Training): 109.55046081542969\n",
      "Smoothed Returns for agent_1 (Training): -1273.7630615234375\n",
      "Smoothed Returns for agent_2 (Frozen): 332.1864318847656\n",
      "Smoothed Returns for agent_3 (Frozen): 232.0308837890625\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 27.664445877075195\n",
      "Policy Loss: 2.4886062145233154\n",
      "Old Approx KL: 0.00193582265637815\n",
      "Approx KL: 2.094677711284021e-06\n",
      "Clip Fraction: 0.032738095770279564\n",
      "Explained Variance: -0.10657954216003418\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1970\n",
      "Episodic Return: [ 7458.3228 -1109.3746 -4011.9312 -4179.7036]\n",
      "Smoothed Returns for agent_0 (Training): 741.4610595703125\n",
      "Smoothed Returns for agent_1 (Training): -1242.916748046875\n",
      "Smoothed Returns for agent_2 (Frozen): -39.97441482543945\n",
      "Smoothed Returns for agent_3 (Frozen): -130.3749237060547\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 3.2417314052581787\n",
      "Policy Loss: 2.4253854751586914\n",
      "Old Approx KL: 0.0011489051394164562\n",
      "Approx KL: 0.0003675477928481996\n",
      "Clip Fraction: 0.014322916666666666\n",
      "Explained Variance: 0.006499171257019043\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode 1980\n",
      "Episodic Return: [ -847.8052 -1310.8661   860.4817   872.9284]\n",
      "Smoothed Returns for agent_0 (Training): 676.1123046875\n",
      "Smoothed Returns for agent_1 (Training): -1249.09130859375\n",
      "Smoothed Returns for agent_2 (Frozen): -12.507159233093262\n",
      "Smoothed Returns for agent_3 (Frozen): -68.11869812011719\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 13.227852821350098\n",
      "Policy Loss: -4.740039348602295\n",
      "Old Approx KL: -0.009916987270116806\n",
      "Approx KL: 0.0009011882357299328\n",
      "Clip Fraction: 0.051153274873892464\n",
      "Explained Variance: -0.03548276424407959\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 1990\n",
      "Episodic Return: [ -853.1446  -1323.2157    876.98206   860.20685]\n",
      "Smoothed Returns for agent_0 (Training): -430.9779357910156\n",
      "Smoothed Returns for agent_1 (Training): -1301.114013671875\n",
      "Smoothed Returns for agent_2 (Frozen): 632.63720703125\n",
      "Smoothed Returns for agent_3 (Frozen): 625.8532104492188\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 5.825280666351318\n",
      "Policy Loss: 2.0688157081604004\n",
      "Old Approx KL: 3.651210499810986e-05\n",
      "Approx KL: 0.0\n",
      "Clip Fraction: 0.0\n",
      "Explained Variance: 0.055096983909606934\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "Training episode 2000\n",
      "Episodic Return: [ 4238.3296 -1207.5007 -1679.0406 -2345.4302]\n",
      "Smoothed Returns for agent_0 (Training): 730.2164916992188\n",
      "Smoothed Returns for agent_1 (Training): -1274.1976318359375\n",
      "Smoothed Returns for agent_2 (Frozen): 15.120758056640625\n",
      "Smoothed Returns for agent_3 (Frozen): -206.3856964111328\n",
      "Episode Length: 199\n",
      "\n",
      "Value Loss: 19.39352035522461\n",
      "Policy Loss: 5.067557334899902\n",
      "Old Approx KL: 0.028305668383836746\n",
      "Approx KL: 0.005587561056017876\n",
      "Clip Fraction: 0.20777530098954836\n",
      "Explained Variance: -0.14339101314544678\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    wandb.init(\n",
    "            project=\"multi-agent-ppo\",  # Set your project name\n",
    "            name=ckpt_name,\n",
    "            config={\n",
    "                \"env\": envname,\n",
    "                \"GRID_SIZE\": GRID_SIZE,\n",
    "                \"NUM_THINGS\": NUM_THINGS,\n",
    "                \"INITIALIZATIONS\": INITIALIZATIONS,\n",
    "                \"IS_TRAINING\": IS_TRAINING,\n",
    "                \"ent_coef\": ent_coef,\n",
    "                \"vf_coef\": vf_coef,\n",
    "                \"clip_coef\": clip_coef,\n",
    "                \"gamma\": gamma,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"max_cycles\": max_cycles,\n",
    "                \"total_episodes\": total_episodes,\n",
    "                \"PPO_STEPS\": PPO_STEPS,\n",
    "            }\n",
    "    )\n",
    "    \"\"\"ALGO PARAMS\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lr = 0.0001\n",
    "    \n",
    "\n",
    "    \"\"\" ENV SETUP \"\"\"\n",
    "    env = CUSTOMENV\n",
    "\n",
    "    num_agents = len(env.possible_agents)\n",
    "    num_actions = env.action_space(env.possible_agents[0]).n\n",
    "    observation_size = env.observation_space(env.possible_agents[0]).shape\n",
    "    \n",
    "    \"\"\" LEARNER SETUP \"\"\"\n",
    "    # Create a list of agents, one for each training agent\n",
    "    training_agents = []\n",
    "    optimizers = []\n",
    "    training_agent_indices = [i for i, training in enumerate(IS_TRAINING) if training]\n",
    "    frozen_agent_indices = [i for i, training in enumerate(IS_TRAINING) if not training and INITIALIZATIONS[i] != RANDOM]\n",
    "    \n",
    "    hiders = [2,3]\n",
    "    preds = [0,1]\n",
    "\n",
    "    pred_policy = Super_Agent(num_actions, num_agents).to(device)\n",
    "    pred_optimizer = optim.Adam(pred_policy.parameters(), lr=lr, eps=1e-5)\n",
    "    if INITIALIZATIONS[0] != RANDOM:\n",
    "        agent.load_state_dict(torch.load(INITIALIZATIONS[0]))\n",
    "        print(f'loaded from {INITIALIZATIONS[0]}')\n",
    "        \n",
    "    hider_policy = Super_Agent(num_actions, num_agents).to(device)\n",
    "    hider_optimizer = optim.Adam(hider_policy.parameters(), lr=lr, eps=1e-5)\n",
    "    if INITIALIZATIONS[2] != RANDOM:\n",
    "        agent.load_state_dict(torch.load(INITIALIZATIONS[1]))\n",
    "        print(f'loaded from {INITIALIZATIONS[1]}')\n",
    "        \n",
    "    # a frozen is one that is NOT TRAINING and NOT RANDOM\n",
    "    for idx in training_agent_indices:\n",
    "        if idx in hiders:\n",
    "            agent = hider_policy\n",
    "            optimizer = hider_optimizer\n",
    "        elif idx in preds:\n",
    "            agent = pred_policy\n",
    "            optimizer = pred_optimizer\n",
    "\n",
    "        training_agents.append(agent)\n",
    "        optimizers.append(optimizer)\n",
    "    \n",
    "    frozen_agents = [] # These agents are not random, but are NOT TRAINING ; initialized with a checkpoint\n",
    "    for idx, init in enumerate(INITIALIZATIONS):\n",
    "        if init != RANDOM and not IS_TRAINING[idx]:\n",
    "            agent = Super_Agent(num_actions=num_actions, num_agents=num_agents).to(device)\n",
    "            agent.load_state_dict(torch.load(init))\n",
    "            agent.eval()\n",
    "            #freeze weights\n",
    "            for param in agent.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(f' loaded from {init}')\n",
    "            frozen_agents.append(agent)\n",
    "\n",
    "    \"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "    end_step = 0\n",
    "    total_episodic_return = 0\n",
    "    rb_obs = torch.zeros((max_cycles, num_agents, NUM_THINGS,GRID_SIZE,GRID_SIZE)).to(device)\n",
    "    rb_actions = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_logprobs = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_rewards = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_terms = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_values = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "\n",
    "    \"\"\" TRAINING LOGIC \"\"\"\n",
    "    # Track returns for all agents\n",
    "    all_returns = [[] for _ in range(num_agents)]\n",
    "\n",
    "    for episode in range(1,total_episodes+1):\n",
    "        # collect an episode\n",
    "        with torch.no_grad():\n",
    "            # collect observations and convert to batch of torch tensors\n",
    "            next_obs, info = env.reset(seed=None)\n",
    "            # reset the episodic return\n",
    "            total_episodic_return = 0\n",
    "\n",
    "            # each episode has num_steps\n",
    "            for step in range(0, max_cycles):\n",
    "                #modify observation to get self, friends, enemy position layers\n",
    "                obs = reshape_obs(next_obs, env)\n",
    "                \n",
    "                # rollover the observation\n",
    "                obs = batchify_obs(obs, device)\n",
    "\n",
    "                # get action for first agent from the trained agents\n",
    "                # get random actions for other agents\n",
    "                actions = torch.zeros(num_agents, dtype=torch.long).to(device)\n",
    "                logprobs = torch.zeros(num_agents).to(device)\n",
    "                values = torch.zeros(num_agents).to(device)\n",
    "\n",
    "                # Process each agent\n",
    "                for i in range(num_agents):\n",
    "                    if IS_TRAINING[i]:\n",
    "                        # Find the index of this training agent among training agents\n",
    "                        train_idx = training_agent_indices.index(i)\n",
    "                        # Get action and value for training agent\n",
    "                        agent_obs = obs[i].unsqueeze(0)\n",
    "                        actions[i], logprobs[i], _, values[i] = training_agents[train_idx].get_action_and_value(agent_obs)\n",
    "                    elif INITIALIZATIONS[i] != RANDOM:\n",
    "                        #this is a frozen agent (not training, but not random because it has a checkpoint)\n",
    "                        frozen_idx = frozen_agent_indices.index(i)\n",
    "                        agent_obs = obs[i].unsqueeze(0)\n",
    "                        actions[i], logprobs[i], _, values[i] = frozen_agents[frozen_idx].get_action_and_value(agent_obs)\n",
    "\n",
    "                        logprobs[i] = torch.log(torch.tensor(1.0/num_actions))\n",
    "                        values[i] = 0.0  # No value estimation for frozen agents\n",
    "                    else:\n",
    "                        # Random action for random agents\n",
    "                        actions[i] = torch.randint(0, num_actions, (1,)).to(device)\n",
    "                        logprobs[i] = torch.log(torch.tensor(1.0/num_actions))\n",
    "                        values[i] = 0.0  # No value estimation for random agents\n",
    "\n",
    "                # execute the environment and log data\n",
    "                next_obs, rewards, terms, truncs, infos = env.step(\n",
    "                    unbatchify(actions, env)\n",
    "                )\n",
    "\n",
    "                # add to episode storage\n",
    "                rb_obs[step] = obs\n",
    "                rb_rewards[step] = batchify(rewards, device)\n",
    "                rb_terms[step] = batchify(terms, device)\n",
    "                rb_actions[step] = actions\n",
    "                rb_logprobs[step] = logprobs\n",
    "                rb_values[step] = values\n",
    "\n",
    "                # compute episodic return\n",
    "                total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "\n",
    "                # if we reach termination or truncation, end\n",
    "                if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
    "                    end_step = step\n",
    "                    break\n",
    "\n",
    "        # Train only the specified agents\n",
    "        for train_idx, agent_idx in enumerate(training_agent_indices):\n",
    "            # Bootstrap value and advantages only for the training agent\n",
    "            with torch.no_grad():\n",
    "                rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "                for t in reversed(range(end_step)):\n",
    "                    delta = (\n",
    "                        rb_rewards[t, agent_idx]  # Only specific agent's reward\n",
    "                        + gamma * rb_values[t + 1, agent_idx] * rb_terms[t + 1, agent_idx]\n",
    "                        - rb_values[t, agent_idx]\n",
    "                    )\n",
    "                    rb_advantages[t, agent_idx] = delta + gamma * gamma * rb_advantages[t + 1, agent_idx]\n",
    "                rb_returns = rb_advantages + rb_values\n",
    "\n",
    "            # convert our episodes to batch of individual transitions (only for specific agent)\n",
    "            b_obs = rb_obs[:end_step, agent_idx]\n",
    "            b_logprobs = rb_logprobs[:end_step, agent_idx]\n",
    "            b_actions = rb_actions[:end_step, agent_idx]\n",
    "            b_returns = rb_returns[:end_step, agent_idx]\n",
    "            b_values = rb_values[:end_step, agent_idx]\n",
    "            b_advantages = rb_advantages[:end_step, agent_idx]\n",
    "\n",
    "            # Optimizing the policy and value network\n",
    "            b_index = np.arange(len(b_obs))\n",
    "            clip_fracs = []\n",
    "            for repeat in range(PPO_STEPS):\n",
    "                # shuffle the indices we use to access the data\n",
    "                np.random.shuffle(b_index)\n",
    "                for start in range(0, len(b_obs), batch_size):\n",
    "                    # select the indices we want to train on\n",
    "                    end = start + batch_size\n",
    "                    batch_index = b_index[start:end]\n",
    "\n",
    "                    _, newlogprob, entropy, value = training_agents[train_idx].get_action_and_value(\n",
    "                        b_obs[batch_index], b_actions.long()[batch_index]\n",
    "                    )\n",
    "                    logratio = newlogprob - b_logprobs[batch_index]\n",
    "                    ratio = logratio.exp()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                        old_approx_kl = (-logratio).mean()\n",
    "                        approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                        clip_fracs += [\n",
    "                            ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                        ]\n",
    "\n",
    "                    # normalize advantages\n",
    "                    advantages = b_advantages[batch_index]\n",
    "                    advantages = (advantages - advantages.mean()) / (\n",
    "                        advantages.std() + 1e-8\n",
    "                    )\n",
    "\n",
    "                    # Policy loss\n",
    "                    pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "                    pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                        ratio, 1 - clip_coef, 1 + clip_coef\n",
    "                    )\n",
    "                    pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                    # Value loss\n",
    "                    value = value.flatten()\n",
    "                    v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "                    v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                        value - b_values[batch_index],\n",
    "                        -clip_coef,\n",
    "                        clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "                    entropy_loss = entropy.mean()\n",
    "                    loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "                    optimizers[train_idx].zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizers[train_idx].step()\n",
    "\n",
    "            # Store returns for the training agents\n",
    "            y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "            var_y = np.var(y_true)\n",
    "            explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # Accumulate returns for all agents\n",
    "        for i in range(num_agents):\n",
    "            all_returns[i].append(total_episodic_return[i])\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Training episode {episode}\")\n",
    "            print(f\"Episodic Return: {(total_episodic_return)}\")\n",
    "\n",
    "            # Print smoothed returns for each agent\n",
    "            for i in range(num_agents):\n",
    "                status = None\n",
    "\n",
    "                if IS_TRAINING[i]:\n",
    "                    status = \"Training\"\n",
    "                elif INITIALIZATIONS[i] == RANDOM:\n",
    "                    status = \"Random\"\n",
    "                else:\n",
    "                    status = \"Frozen\"\n",
    "                    \n",
    "                print(f\"Smoothed Returns for agent_{i} ({status}): {np.mean(all_returns[i][-20:])}\")\n",
    "\n",
    "            print(f\"Episode Length: {end_step}\")\n",
    "            print(\"\")\n",
    "            print(f\"Value Loss: {v_loss.item()}\")\n",
    "            print(f\"Policy Loss: {pg_loss.item()}\")\n",
    "            print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
    "            print(f\"Approx KL: {approx_kl.item()}\")\n",
    "            print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "            print(f\"Explained Variance: {explained_var.item()}\")\n",
    "            print(\"\\n-------------------------------------------\\n\")\n",
    "\n",
    "        #log all with wandb\n",
    "        wandb.log({\n",
    "            \"Ep return pred1\": total_episodic_return[0],\n",
    "            \"Ep return pred2\": total_episodic_return[1],\n",
    "            \"Ep return hider1\": total_episodic_return[2],\n",
    "            \"Ep return hider2\": total_episodic_return[3],\n",
    "            \"Episode Length\": end_step,\n",
    "            \"Value Loss\": v_loss.item(),\n",
    "            \"Policy Loss\": pg_loss.item(),\n",
    "            \"Old Approx KL\": old_approx_kl.item(),\n",
    "            \"Approx KL\": approx_kl.item(),\n",
    "            \"Clip Fraction\": np.mean(clip_fracs),\n",
    "            \"Explained Variance\": explained_var.item()\n",
    "        })\n",
    "\n",
    "        #if for pred_1 (index 0) episode return and smoothed are greater than -200, save the model\n",
    "\n",
    "        # if total_episodic_return[0] > -210 and np.mean(all_returns[0][-20:]) > -210:\n",
    "        #     #create dir\n",
    "        #     import os\n",
    "        #     if not os.path.exists('./models'):\n",
    "        #         os.makedirs('./models')f\n",
    "        #     #save just state dict for 0\n",
    "        #     torch.save(agents[0].state_dict(), f'./models/agentwalls_{episode}.ckpt')\n",
    "        #     exit(1)\n",
    "\n",
    "\n",
    "\n",
    "        #if reward greater than 600 for hider_1 both for last and smoothed for last 5\n",
    "        #every 100 epochs save the 2 models\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            if SAVE_PRED_POL:\n",
    "                torch.save(pred_policy.state_dict(), f'./models/PRED_{ckpt_name}_{episode}.ckpt')\n",
    "            if SAVE_HIDER_POL:\n",
    "                torch.save(hider_policy.state_dict(), f'./models/HIDER_{ckpt_name}_{episode}.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9811a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c8ea2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672304be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3410733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL project env",
   "language": "python",
   "name": "drl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
