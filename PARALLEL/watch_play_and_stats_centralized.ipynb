{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "884374d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "import movable_wall_parallel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79416a37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RANDOM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#SET ALL OF THESE CAREFULLY\u001b[39;00m\n\u001b[1;32m      3\u001b[0m policies \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/PRED_TEST_1200.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# pred_policy\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/PRED_TEST_1200.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m,    \u001b[38;5;66;03m# hider_policy\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mRANDOM\u001b[49m,\n\u001b[1;32m      7\u001b[0m     RANDOM]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#either None (none here means random)  ; or a path to a pretrained checkpoint\u001b[39;00m\n\u001b[1;32m     10\u001b[0m GRID_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RANDOM' is not defined"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "#SET ALL OF THESE CAREFULLY\n",
    "policies = [\n",
    "    './models/PRED_TEST_1200.ckpt', # pred_policy\n",
    "    './models/PRED_TEST_1200.ckpt',    # hider_policy\n",
    "    RANDOM,\n",
    "    RANDOM]\n",
    "#either None (none here means random)  ; or a path to a pretrained checkpoint\n",
    "\n",
    "GRID_SIZE = 7\n",
    "NUM_THINGS = 5\n",
    "env = movable_wall_parallel.parallel_env(grid_size=GRID_SIZE,render_mode=\"human\",walls=True,generate_gif=True)\n",
    "\n",
    "#usually creating a GIF and using render mode human is the best, you can watch directly on your screen\n",
    "# by setting the variable IS_SCREEN in the env\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74c387b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Super_Agent(nn.Module):\n",
    "    #Common agent class for all hiders/seekers\n",
    "    \n",
    "    def __init__(self, num_actions, num_agents):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN architecture inspired by DQN for Atari\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(NUM_THINGS, 32, kernel_size=3, stride=1, padding=1),  # Output: 32 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Output: 64 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Output: 64 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),  # Output: 64 * 7 * 7 = 3136\n",
    "        )\n",
    "        self.actor = self._layer_init(nn.Linear(64*GRID_SIZE**2, num_actions), std=0.01) #TODO depends on GRID_SIZE\n",
    "        self.critic = self._layer_init(nn.Linear(64*GRID_SIZE**2, 1))\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.network(x / 1.0))  # Normalize input to [0, 1]\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x / 1.0)  # Normalize input to [0, 1]\n",
    "        \n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
    "\n",
    "\n",
    "def batchify_obs(obs, device):\n",
    "    \"\"\"Converts PZ style observations to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    obs = np.stack([obs[a] for a in obs], axis=0)\n",
    "    # convert to torch\n",
    "    obs = torch.tensor(obs).to(device)\n",
    "\n",
    "    return obs\n",
    "\n",
    "\n",
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unbatchify(x, env):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {a: x[i] for i, a in enumerate(env.possible_agents)}\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def reshape_obs(observations, env):\n",
    "    modified_observations = {}\n",
    "    for self_name, obs in observations.items():\n",
    "        self_layer = env.agent_layers[self_name]\n",
    "        enemy_layers = []\n",
    "        for name, layer_idx in env.agent_layers.items():\n",
    "            \n",
    "            if name == self_name:\n",
    "                self_layer = obs[layer_idx]\n",
    "            elif name.startswith(self_name[:4]): #starts with the same 4 letters: pred or hide\n",
    "                friend_layer = obs[layer_idx]\n",
    "            else:\n",
    "                enemy_layers.append(obs[layer_idx])\n",
    "         \n",
    "        new_obs = [obs[0]] #walls\n",
    "        new_obs.append(self_layer) #self\n",
    "        new_obs.append(friend_layer) #friend\n",
    "        new_obs.append(sum(enemy_layers)) #enemies\n",
    "        new_obs.append(obs[-1]) #movable walls\n",
    "\n",
    "        modified_observations[self_name] = np.stack(new_obs, axis = 0)\n",
    "        \n",
    "    return modified_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b238b9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/1028984/ipykernel_848649/2038389096.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent_pred1.load_state_dict(torch.load(policies[0]))\n",
      "/scratch/1028984/ipykernel_848649/2038389096.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent_pred2.load_state_dict(torch.load(policies[1]))\n",
      "/scratch/1028984/ipykernel_848649/2038389096.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent_flee1.load_state_dict(torch.load(policies[2]))\n",
      "/scratch/1028984/ipykernel_848649/2038389096.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent_flee2.load_state_dict(torch.load(policies[3]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from ./models/PRED_GROUP_POLICY_BOTH_INDIV_RWD_ROUND2_1900.ckpt\n",
      "loaded from ./models/PRED_GROUP_POLICY_BOTH_INDIV_RWD_ROUND2_1900.ckpt\n",
      "loaded from ./models/HIDER_GROUP_POLICY_BOTH_INDIV_RWD_1800.ckpt\n",
      "loaded from ./models/HIDER_GROUP_POLICY_BOTH_INDIV_RWD_1800.ckpt\n",
      "saved GIF at path ./random.gif\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m         actions[idx] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, num_actions, (\u001b[38;5;241m1\u001b[39m,))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#print(actions)\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m obs, rewards, terms, truncs, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43munbatchify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#obs = batchify_obs(obs, device)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m terms \u001b[38;5;241m=\u001b[39m [terms[a] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m terms]\n",
      "File \u001b[0;32m~/2d_RL_hide_seek/PARALLEL/movable_wall_parallel.py:480\u001b[0m, in \u001b[0;36mparallel_env.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m,actions):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 480\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m actions:\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/2d_RL_hide_seek/PARALLEL/movable_wall_parallel.py:222\u001b[0m, in \u001b[0;36mparallel_env.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m imageio\u001b[38;5;241m.\u001b[39mmimsave(path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgif_frames, fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved GIF at path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 222\u001b[0m \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "    \"\"\" ENV SETUP \"\"\"\n",
    "    \n",
    "\n",
    "    num_agents = len(env.possible_agents)\n",
    "    num_actions = env.action_space(env.possible_agents[0]).n\n",
    "    observation_size = env.observation_space(env.possible_agents[0]).shape\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \"\"\" RENDER THE POLICY \"\"\"\n",
    "    \n",
    "    agent_pred1 = Super_Agent(num_actions=num_actions, num_agents=num_agents).to(device)\n",
    "    if policies[0] is not None:\n",
    "        agent_pred1.load_state_dict(torch.load(policies[0]))\n",
    "        print(f'loaded from {policies[0]}')\n",
    "\n",
    "    agent_pred2 = Super_Agent(num_actions=num_actions, num_agents=num_agents).to(device)\n",
    "    if policies[1] is not None:\n",
    "        agent_pred2.load_state_dict(torch.load(policies[1]))\n",
    "        print(f'loaded from {policies[1]}')\n",
    "\n",
    "    agent_flee1 = Super_Agent(num_actions=num_actions, num_agents=num_agents).to(device)\n",
    "    if policies[2] is not None:\n",
    "        agent_flee1.load_state_dict(torch.load(policies[2]))\n",
    "        print(f'loaded from {policies[2]}')\n",
    "\n",
    "    agent_flee2 = Super_Agent(num_actions=num_actions, num_agents=num_agents).to(device)\n",
    "    if policies[3] is not None:\n",
    "        agent_flee2.load_state_dict(torch.load(policies[3]))\n",
    "        print(f'loaded from {policies[3]}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # render 5 episodes out\n",
    "        for episode in range(5):\n",
    "            obs, infos = env.reset(seed=None)\n",
    "            #obs = batchify_obs(obs, device)\n",
    "            terms = [False]\n",
    "            truncs = [False]\n",
    "            total_ep_rew = {'pred_1':0, 'pred_2':0, 'hider_1':0, 'hider_2':0}\n",
    "            while not any(terms) and not any(truncs):\n",
    "                obs = reshape_obs(obs,env)\n",
    "                action_p1, logprob_p1, _, value_p1 = agent_pred1.get_action_and_value(torch.tensor(obs['pred_1']).unsqueeze(0).to(device))\n",
    "                action_p2, logprob_p2, _, value_p2 = agent_pred2.get_action_and_value(torch.tensor(obs['pred_2']).unsqueeze(0).to(device))\n",
    "                action_h1, logprob_h1, _, value_h1 = agent_flee1.get_action_and_value(torch.tensor(obs['hider_1']).unsqueeze(0).to(device))\n",
    "                action_h2, logprob_h2, _, value_h2 = agent_flee2.get_action_and_value(torch.tensor(obs['hider_2']).unsqueeze(0).to(device))\n",
    "\n",
    "                actions = torch.cat([action_p1, action_p2, action_h1, action_h2])\n",
    "                #print(actions)  \n",
    "                \n",
    "                for idx,p in enumerate(policies):\n",
    "                    if p == None:\n",
    "                        actions[idx] = torch.randint(0, num_actions, (1,)).to(device)\n",
    "\n",
    "                #print(actions)\n",
    "                obs, rewards, terms, truncs, infos = env.step(unbatchify(actions, env))\n",
    "                #obs = batchify_obs(obs, device)\n",
    "                terms = [terms[a] for a in terms]\n",
    "                truncs = [truncs[a] for a in truncs]\n",
    "\n",
    "                total_ep_rew['pred_1'] += rewards['pred_1']\n",
    "                total_ep_rew['pred_2'] += rewards['pred_2']\n",
    "                total_ep_rew['hider_1'] += rewards['hider_1']\n",
    "                total_ep_rew['hider_2'] += rewards['hider_2']\n",
    "\n",
    "\n",
    "            print(f\"Episode {episode} rewards: {total_ep_rew}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b3734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7649d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL project env",
   "language": "python",
   "name": "drl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
